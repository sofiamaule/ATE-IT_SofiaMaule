{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5216164",
   "metadata": {},
   "source": [
    "### Data Loading and Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "228c2354",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "import spacy\n",
    "from spacy.tokens import DocBin, Doc\n",
    "from spacy.training import Example\n",
    "from spacy.util import minibatch, compounding\n",
    "from spacy.pipeline import EntityRuler\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bf5872a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Data loading functions work correctly\n"
     ]
    }
   ],
   "source": [
    "def load_jsonl(path: str) -> List[Dict]:\n",
    "    \"\"\"Load a JSON lines file or JSON array file.\"\"\"\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        text = f.read().strip()\n",
    "    if not text:\n",
    "        return []\n",
    "    try:\n",
    "        # Try parsing as single JSON object/array\n",
    "        data = json.loads(text)\n",
    "    except json.JSONDecodeError:\n",
    "        # Fall back to JSONL (one JSON per line)\n",
    "        data = []\n",
    "        for line in text.splitlines():\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                data.append(json.loads(line))\n",
    "    return data\n",
    "\n",
    "\n",
    "def build_sentence_gold_map(records: List[Dict]) -> List[Dict]:\n",
    "    \"\"\"Convert dataset rows into list of sentences with aggregated terms.\n",
    "    \n",
    "    Handles both formats:\n",
    "    - Records with 'term_list' field (list of terms) for input files in json format\n",
    "    - Records with individual 'term' field (one term per row) for input files in csv format\n",
    "    \"\"\"\n",
    "    out = {}\n",
    "    \n",
    "    # Support both dict with 'data' key and plain list\n",
    "    if isinstance(records, dict) and 'data' in records:\n",
    "        rows = records['data']\n",
    "    else:\n",
    "        rows = records\n",
    "    \n",
    "    for r in rows:\n",
    "        key = (r.get('document_id'), r.get('paragraph_id'), r.get('sentence_id'))\n",
    "        if key not in out:\n",
    "            out[key] = {\n",
    "                'document_id': r.get('document_id'),\n",
    "                'paragraph_id': r.get('paragraph_id'),\n",
    "                'sentence_id': r.get('sentence_id'),\n",
    "                'sentence_text': r.get('sentence_text', ''),\n",
    "                'terms': []\n",
    "            }\n",
    "        \n",
    "        # Support both 'term_list' (list) and 'term' (single value)\n",
    "        if isinstance(r.get('term_list'), list):\n",
    "            for t in r.get('term_list'):\n",
    "                if t and t not in out[key]['terms']:\n",
    "                    out[key]['terms'].append(t)\n",
    "        else:\n",
    "            term = r.get('term')\n",
    "            if term and term not in out[key]['terms']:\n",
    "                out[key]['terms'].append(term)\n",
    "    \n",
    "    return list(out.values())\n",
    "\n",
    "\n",
    "# Test: Load a small sample\n",
    "test_data = {\n",
    "    'data': [\n",
    "        {\n",
    "            'document_id': 'doc1',\n",
    "            'paragraph_id': 'p1',\n",
    "            'sentence_id': 's1',\n",
    "            'sentence_text': 'La tassa di successione è un tributo.',\n",
    "            'term_list': ['tassa di successione', 'tributo']\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "test_sentences = build_sentence_gold_map(test_data)\n",
    "assert len(test_sentences) == 1\n",
    "assert test_sentences[0]['terms'] == ['tassa di successione', 'tributo']\n",
    "print(\"✓ Data loading functions work correctly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "33073e32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training sentences: 2308\n",
      "Dev sentences: 577\n",
      "\n",
      "Example sentence:\n",
      "  Text: AFFIDAMENTO DEL “SERVIZIO DI SPAZZAMENTO, RACCOLTA, TRASPORTO E SMALTIMENTO/RECUPERO DEI RIFIUTI URBANI ED ASSIMILATI E SERVIZI COMPLEMENTARI DELLA CITTA' DI AGROPOLI” VALEVOLE PER UN QUINQUENNIO\n",
      "  Terms: ['raccolta', 'recupero', 'servizio di raccolta', 'servizio di spazzamento', 'smaltimento', 'trasporto']\n"
     ]
    }
   ],
   "source": [
    "# Load actual training and dev data\n",
    "train_data = load_jsonl('../../data/subtask_a_train.json')\n",
    "dev_data = load_jsonl('../../data/subtask_a_dev.json')\n",
    "\n",
    "train_sentences = build_sentence_gold_map(train_data)\n",
    "dev_sentences = build_sentence_gold_map(dev_data)\n",
    "\n",
    "print(f\"Training sentences: {len(train_sentences)}\")\n",
    "print(f\"Dev sentences: {len(dev_sentences)}\")\n",
    "print(f\"\\nExample sentence:\")\n",
    "print(f\"  Text: {train_sentences[6]['sentence_text']}\")\n",
    "print(f\"  Terms: {train_sentences[6]['terms']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4181bd14",
   "metadata": {},
   "source": [
    "### Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2854f180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Evaluation functions work correctly\n",
      "  Test metrics: P=0.67, R=0.67, F1=0.67\n",
      "  Type metrics: P=0.67, R=0.67, F1=0.67\n"
     ]
    }
   ],
   "source": [
    "def micro_f1_score(gold_standard, system_output):\n",
    "    \"\"\"\n",
    "    Evaluates performance using Precision, Recall, and F1 score \n",
    "    based on individual term matching (micro-average).\n",
    "    \n",
    "    Args:\n",
    "        gold_standard: List of lists, where each inner list contains gold standard terms\n",
    "        system_output: List of lists, where each inner list contains extracted terms\n",
    "    \n",
    "    Returns:\n",
    "        Tuple containing (precision, recall, f1, tp, fp, fn)\n",
    "    \"\"\"\n",
    "    total_true_positives = 0\n",
    "    total_false_positives = 0\n",
    "    total_false_negatives = 0\n",
    "    \n",
    "    # Iterate through each item's gold standard and system output terms\n",
    "    for gold, system in zip(gold_standard, system_output):\n",
    "        # Convert to sets for efficient comparison\n",
    "        gold_set = set(gold)\n",
    "        system_set = set(system)\n",
    "        \n",
    "        # Calculate TP, FP, FN for the current item\n",
    "        true_positives = len(gold_set.intersection(system_set))\n",
    "        false_positives = len(system_set - gold_set)\n",
    "        false_negatives = len(gold_set - system_set)\n",
    "        \n",
    "        # Accumulate totals across all items\n",
    "        total_true_positives += true_positives\n",
    "        total_false_positives += false_positives\n",
    "        total_false_negatives += false_negatives\n",
    "    \n",
    "    # Calculate Precision, Recall, and F1 score (micro-average)\n",
    "    precision = total_true_positives / (total_true_positives + total_false_positives) if (total_true_positives + total_false_positives) > 0 else 0\n",
    "    recall = total_true_positives / (total_true_positives + total_false_negatives) if (total_true_positives + total_false_negatives) > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    return precision, recall, f1, total_true_positives, total_false_positives, total_false_negatives\n",
    "\n",
    "\n",
    "def type_f1_score(gold_standard, system_output):\n",
    "    \"\"\"\n",
    "    Evaluates performance using Type Precision, Type Recall, and Type F1 score\n",
    "    based on the set of unique terms extracted at least once across the entire dataset.\n",
    "    \n",
    "    Args:\n",
    "        gold_standard: List of lists, where each inner list contains gold standard terms\n",
    "        system_output: List of lists, where each inner list contains extracted terms\n",
    "    \n",
    "    Returns:\n",
    "        Tuple containing (type_precision, type_recall, type_f1)\n",
    "    \"\"\"\n",
    "    # Get the set of all unique gold standard terms across the dataset\n",
    "    all_gold_terms = set()\n",
    "    for item_terms in gold_standard:\n",
    "        all_gold_terms.update(item_terms)\n",
    "    \n",
    "    # Get the set of all unique system extracted terms across the dataset\n",
    "    all_system_terms = set()\n",
    "    for item_terms in system_output:\n",
    "        all_system_terms.update(item_terms)\n",
    "    \n",
    "    # Calculate True Positives (terms present in both sets)\n",
    "    type_true_positives = len(all_gold_terms.intersection(all_system_terms))\n",
    "    \n",
    "    # Calculate False Positives (terms in system output but not in gold standard)\n",
    "    type_false_positives = len(all_system_terms - all_gold_terms)\n",
    "    \n",
    "    # Calculate False Negatives (terms in gold standard but not in system output)\n",
    "    type_false_negatives = len(all_gold_terms - all_system_terms)\n",
    "    \n",
    "    # Calculate Type Precision, Type Recall, and Type F1 score\n",
    "    type_precision = type_true_positives / (type_true_positives + type_false_positives) if (type_true_positives + type_false_positives) > 0 else 0\n",
    "    type_recall = type_true_positives / (type_true_positives + type_false_negatives) if (type_true_positives + type_false_negatives) > 0 else 0\n",
    "    type_f1 = 2 * (type_precision * type_recall) / (type_precision + type_recall) if (type_precision + type_recall) > 0 else 0\n",
    "    \n",
    "    return type_precision, type_recall, type_f1\n",
    "\n",
    "\n",
    "# Test: Simple case\n",
    "gold_test = [['term1', 'term2'], ['term3']]\n",
    "pred_test = [['term1', 'term4'], ['term3']]\n",
    "precision, recall, f1, tp, fp, fn = micro_f1_score(gold_test, pred_test)\n",
    "assert tp == 2  # term1 and term3\n",
    "assert fp == 1  # term4\n",
    "assert fn == 1  # term2\n",
    "print(\"✓ Evaluation functions work correctly\")\n",
    "print(f\"  Test metrics: P={precision:.2f}, R={recall:.2f}, F1={f1:.2f}\")\n",
    "\n",
    "# Test type-level metrics\n",
    "type_p, type_r, type_f1 = type_f1_score(gold_test, pred_test)\n",
    "print(f\"  Type metrics: P={type_p:.2f}, R={type_r:.2f}, F1={type_f1:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066abcb5",
   "metadata": {},
   "source": [
    "## SpaCy import and Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557044cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#python -m spacy download it_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2c708a74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Italian model loaded successfully\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "# Load Italian model\n",
    "try:\n",
    "    nlp = spacy.load('it_core_news_sm')\n",
    "    print(\"✓ Italian model loaded successfully\")\n",
    "except:\n",
    "    print(\"Model not found. Install with: python -m spacy download it_core_news_sm\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2bc363",
   "metadata": {},
   "source": [
    "### Text Normalization Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8c2ea78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text_spacy(t: str) -> str:\n",
    "    \"\"\"\n",
    "    Normalize raw sentence text before sending it to spaCy:\n",
    "      - replace non-breaking spaces\n",
    "      - collapse multiple spaces\n",
    "      - unify curly quotes to plain apostrophes\n",
    "    \"\"\"\n",
    "    t = t.replace(\"\\u00a0\", \" \")\n",
    "    t = \" \".join(t.split())\n",
    "    t = t.replace(\"’\", \"'\").replace(\"`\", \"'\")\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b640a9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_candidate_spacy(term: str) -> str:\n",
    "    \"\"\"\n",
    "    Normalize a candidate term for comparison/merging:\n",
    "      - strip whitespace\n",
    "      - lowercase\n",
    "      - collapse multiple spaces\n",
    "      - strip leading/trailing punctuation and quotes\n",
    "    \"\"\"\n",
    "    import string\n",
    "\n",
    "    t = term.strip()\n",
    "    if not t:\n",
    "        return \"\"\n",
    "\n",
    "    t = t.lower()\n",
    "    t = \" \".join(t.split())\n",
    "    # remove punctuation only at boundaries\n",
    "    t = t.strip(string.punctuation + \"«»“”'\\\"\")\n",
    "\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7910cb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_annotate_sentence(text: str) -> Dict:\n",
    "    \"\"\"\n",
    "    Run spaCy on a sentence and return:\n",
    "      - doc: the spaCy Doc object\n",
    "      - tokens: list of token texts\n",
    "      - lemmas: list of lemmas\n",
    "      - pos: list of POS tags\n",
    "      - noun_chunks: list of noun chunk texts\n",
    "    \"\"\"\n",
    "    clean_text = normalize_text_spacy(text)\n",
    "    doc = nlp(clean_text)\n",
    "\n",
    "    tokens = [token.text for token in doc]\n",
    "    lemmas = [token.lemma_ for token in doc]\n",
    "    pos_tags = [token.pos_ for token in doc]\n",
    "    noun_chunks = [chunk.text for chunk in doc.noun_chunks]\n",
    "\n",
    "    return {\n",
    "        \"doc\": doc,\n",
    "        \"tokens\": tokens,\n",
    "        \"lemmas\": lemmas,\n",
    "        \"pos\": pos_tags,\n",
    "        \"noun_chunks\": noun_chunks,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66fe5c69",
   "metadata": {},
   "source": [
    "### Extraction opf the sPaCy candidates (pattern + noun chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a0931fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_spacy_candidates(doc) -> List[str]:\n",
    "    \"\"\"\n",
    "    Extract term candidates from a spaCy Doc using:\n",
    "      - noun chunks\n",
    "      - simple POS patterns:\n",
    "          * ADJ + NOUN\n",
    "          * NOUN + NOUN\n",
    "          * NOUN + ADP + NOUN   (e.g., \"centro di raccolta\")\n",
    "\n",
    "    Returns:\n",
    "        A list of *normalized* candidate strings (deduplicated, order-preserving).\n",
    "    \"\"\"\n",
    "    candidates = []\n",
    "\n",
    "    # 1) Direct noun chunks\n",
    "    for chunk in doc.noun_chunks:\n",
    "        candidates.append(chunk.text)\n",
    "\n",
    "    # 2) POS-based patterns\n",
    "    tokens = list(doc)\n",
    "    n = len(tokens)\n",
    "\n",
    "    i = 0\n",
    "    while i < n:\n",
    "        tok = tokens[i]\n",
    "\n",
    "        # ADJ + NOUN\n",
    "        if tok.pos_ == \"ADJ\" and i + 1 < n and tokens[i + 1].pos_ == \"NOUN\":\n",
    "            span = doc[tok.i : tokens[i + 1].i + 1]\n",
    "            candidates.append(span.text)\n",
    "\n",
    "        # NOUN + NOUN\n",
    "        if tok.pos_ == \"NOUN\" and i + 1 < n and tokens[i + 1].pos_ == \"NOUN\":\n",
    "            span = doc[tok.i : tokens[i + 1].i + 1]\n",
    "            candidates.append(span.text)\n",
    "\n",
    "        # NOUN + ADP + NOUN  (e.g., \"centro di raccolta\")\n",
    "        if (\n",
    "            tok.pos_ == \"NOUN\"\n",
    "            and i + 2 < n\n",
    "            and tokens[i + 1].pos_ == \"ADP\"\n",
    "            and tokens[i + 2].pos_ == \"NOUN\"\n",
    "        ):\n",
    "            span = doc[tok.i : tokens[i + 2].i + 1]\n",
    "            candidates.append(span.text)\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    # Normalize and deduplicate while preserving order\n",
    "    norm_seen = set()\n",
    "    final = []\n",
    "    for c in candidates:\n",
    "        norm = normalize_candidate_spacy(c)\n",
    "        if norm and norm not in norm_seen:\n",
    "            norm_seen.add(norm)\n",
    "            final.append(norm)\n",
    "\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "20dc93e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_spacy_annotations(sentences: List[Dict]) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    For each sentence entry, add:\n",
    "      - entry[\"spacy_tokens\"]\n",
    "      - entry[\"spacy_lemmas\"]\n",
    "      - entry[\"spacy_pos\"]\n",
    "      - entry[\"spacy_noun_chunks\"]\n",
    "      - entry[\"spacy_candidates\"]  (list of normalized candidate strings)\n",
    "\n",
    "    NOTE: this function mutates the input list in place and returns it.\n",
    "    \"\"\"\n",
    "    total = len(sentences)\n",
    "    print(f\"→ Starting spaCy annotation on {total} sentences\")\n",
    "\n",
    "    for i, entry in enumerate(sentences):\n",
    "        if i % 500 == 0:\n",
    "            print(f\"  [spaCy] processing sentence {i}/{total}\")\n",
    "\n",
    "        sent_text = entry[\"sentence_text\"]\n",
    "        ann = spacy_annotate_sentence(sent_text)\n",
    "\n",
    "        entry[\"spacy_tokens\"] = ann[\"tokens\"]\n",
    "        entry[\"spacy_lemmas\"] = ann[\"lemmas\"]\n",
    "        entry[\"spacy_pos\"] = ann[\"pos\"]\n",
    "        entry[\"spacy_noun_chunks\"] = ann[\"noun_chunks\"]\n",
    "\n",
    "        # linguistic candidates\n",
    "        spacy_cands = extract_spacy_candidates(ann[\"doc\"])\n",
    "        entry[\"spacy_candidates\"] = spacy_cands\n",
    "\n",
    "    print(\"→ spaCy annotation completed\")\n",
    "    return sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e042b8d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Annotating TRAIN with spaCy...\n",
      "→ Starting spaCy annotation on 2308 sentences\n",
      "  [spaCy] processing sentence 0/2308\n",
      "  [spaCy] processing sentence 500/2308\n",
      "  [spaCy] processing sentence 1000/2308\n",
      "  [spaCy] processing sentence 1500/2308\n",
      "  [spaCy] processing sentence 2000/2308\n",
      "→ spaCy annotation completed\n",
      "\n",
      "Annotating DEV with spaCy...\n",
      "→ Starting spaCy annotation on 577 sentences\n",
      "  [spaCy] processing sentence 0/577\n",
      "  [spaCy] processing sentence 500/577\n",
      "→ spaCy annotation completed\n",
      "\n",
      "✓ spaCy annotation completed on TRAIN (2308) and DEV (577) sentences\n",
      "\n",
      "[DEBUG] Example DEV sentences with gold terms and spaCy candidates:\n",
      "\n",
      "--- DEV sentence #0 ---\n",
      "Text: Non Domestica; CAMPEGGI, DISTRIBUTORI CARBURANTI, PARCHEGGI; 1,22; 4,73 \n",
      "Gold terms: []\n",
      "spaCy noun_chunks: ['Non Domestica', 'CAMPEGGI', 'DISTRIBUTORI CARBURANTI, PARCHEGGI']\n",
      "spaCy candidates: ['non domestica', 'campeggi', 'distributori carburanti, parcheggi']\n",
      "\n",
      "--- DEV sentence #1 ---\n",
      "Text: Il presente disciplinare per la gestione dei centri di raccolta comunali è stato redatto ai sensi e per effetto del DM 13/05/2009, pubblicato sulla G.U. n. 165 del 18/07/2009, con il quale sono state apportate le modifiche sostanziali al DM 08/04/2008, Disciplina dei centri di raccolta dei rifiuti urbani raccolti in modo differenziato, come previsto dall'art. 183, comma 7, lettera cc) del Dlgs 3 aprile 2006, n. 152, e ss.mm.ii.\n",
      "Gold terms: ['disciplina dei centri di raccolta dei rifiuti urbani raccolti in modo differenziato', 'disciplinare per la gestione dei centri di raccolta comunali']\n",
      "spaCy noun_chunks: ['la gestione', 'centri', 'raccolta comunali', 'sensi', 'per effetto', 'DM 13/05/2009', 'G.U.', 'n.', '18/07/2009', 'il quale', 'le modifiche sostanziali al DM 08/04/2008', 'Disciplina', 'centri', 'raccolta', 'rifiuti urbani', 'modo differenziato', 'art.', 'comma', 'lettera cc) del Dlgs 3 aprile 2006, n. 152, e ss.mm.ii']\n",
      "spaCy candidates: ['la gestione', 'centri', 'raccolta comunali', 'sensi', 'per effetto', 'dm 13/05/2009', 'g.u', 'n', '18/07/2009', 'il quale', 'le modifiche sostanziali al dm 08/04/2008', 'disciplina', 'raccolta', 'rifiuti urbani', 'modo differenziato', 'art', 'comma', 'lettera cc) del dlgs 3 aprile 2006, n. 152, e ss.mm.ii', 'gestione dei centri', 'centri di raccolta', 'effetto del dm', 'disciplina dei centri', 'raccolta dei rifiuti', 'lettera cc']\n",
      "\n",
      "--- DEV sentence #2 ---\n",
      "Text: Voti astenuti: 2 (Acampora Alessandro, Gargiulo Mario)\n",
      "Gold terms: []\n",
      "spaCy noun_chunks: ['Voti', '(Acampora Alessandro, Gargiulo Mario)']\n",
      "spaCy candidates: ['voti', 'acampora alessandro, gargiulo mario']\n",
      "\n",
      "[DEBUG] spaCy candidates statistics on DEV:\n",
      "  Min #candidates: 0\n",
      "  Max #candidates: 62\n",
      "  Avg #candidates: 5.72\n",
      "\n",
      "[DEBUG] Sentence with the highest number of candidates (index 284):\n",
      "  Text: VISTI e RICHIAMATI il D.ligs.3 aprile 2006 n.152 e smi ed in particolare l'articolo 191 del che stabilisce, qualora si verifichino situazioni di eccezionale ed urgente necessità di tutela della salute pubblica e dell'ambiente, che il Sindaco può emettere ordinanze contingibili e urgenti per consentire il ricorso temporaneo a speciali forme di gestione dei rifiuti, anche in deroga alle disposizioni vigenti, garantendo un elevato livello di tutela della salute e dell'ambiente; dette ordinanze sono comunicate al Presidente del Consiglio dei Ministri, al Ministro dell'ambiente e della tutela del territorio e del mare, al Ministro della salute, al Ministro delle attività produttive, al Presidente della Regione e all'Autorità d'ambito di cui all'articolo 201 entro 3 giorni dall'emissione ed hanno efficacia per un periodo non superiore ai 6 mesi; il D.lgs. 18 agosto 2000 n. 267 e smi ed in particolare:\n",
      "  #Candidates: 62\n",
      "  Candidates: ['visti', 'richiamati', 'il d.ligs.3 aprile 2006', 'n.152', 'smi', 'particolare', 'che', 'situazioni', 'necessità', 'tutela', 'salute pubblica', \"dell'ambiente\", 'il sindaco', 'ordinanze contingibili e urgenti', 'il ricorso temporaneo', 'speciali forme', 'gestione', 'rifiuti', 'anche in deroga', 'disposizioni vigenti', 'un elevato livello', 'salute', 'dette ordinanze', 'presidente', 'consiglio', 'ministri', 'ministro', 'ambiente', 'della tutela', 'territorio', 'del mare', ' al ministro', 'attività produttive', 'regione', \"all'autorità\", 'ambito', 'cui', 'articolo', '3 giorni', 'emissione', 'efficacia', 'un periodo non superiore ai 6 mesi', 'il d.lgs', '18 agosto 2000 n', 'in particolare', 'urgente necessità', 'necessità di tutela', 'tutela della salute', 'forme di gestione', 'gestione dei rifiuti', 'deroga alle disposizioni', 'elevato livello', 'livello di tutela', 'presidente del consiglio', 'consiglio dei ministri', \"ministro dell'ambiente\", 'tutela del territorio', 'ministro della salute', 'ministro delle attività', 'presidente della regione', \"autorità d'ambito\", \"giorni dall'emissione\"]\n",
      "\n",
      "Built spaCy predictions for DEV: 577 sentences\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------------\n",
    "# Run spaCy on TRAIN and DEV and inspect a few debug examples\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "print(\"\\nAnnotating TRAIN with spaCy...\")\n",
    "train_sentences = build_spacy_annotations(train_sentences)\n",
    "\n",
    "print(\"\\nAnnotating DEV with spaCy...\")\n",
    "dev_sentences = build_spacy_annotations(dev_sentences)\n",
    "\n",
    "# Extract raw text and gold terms for DEV\n",
    "dev_texts = [s[\"sentence_text\"] for s in dev_sentences]\n",
    "dev_gold = [s[\"terms\"] for s in dev_sentences]\n",
    "\n",
    "print(f\"\\n✓ spaCy annotation completed on TRAIN ({len(train_sentences)}) \"\n",
    "      f\"and DEV ({len(dev_sentences)}) sentences\")\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# DEBUG: inspect the first few DEV sentences and their candidates\n",
    "# -------------------------------------------------------------------\n",
    "print(\"\\n[DEBUG] Example DEV sentences with gold terms and spaCy candidates:\\n\")\n",
    "\n",
    "num_debug_examples = 3\n",
    "for idx in range(min(num_debug_examples, len(dev_sentences))):\n",
    "    sent = dev_sentences[idx]\n",
    "    print(f\"--- DEV sentence #{idx} ---\")\n",
    "    print(\"Text:\", sent[\"sentence_text\"])\n",
    "    print(\"Gold terms:\", sent.get(\"terms\", []))\n",
    "    print(\"spaCy noun_chunks:\", sent[\"spacy_noun_chunks\"])\n",
    "    print(\"spaCy candidates:\", sent[\"spacy_candidates\"])\n",
    "    print()\n",
    "\n",
    "# More global debug: basic stats about number of candidates per sentence\n",
    "candidates_lengths = [len(s[\"spacy_candidates\"]) for s in dev_sentences]\n",
    "if candidates_lengths:\n",
    "    avg_cands = sum(candidates_lengths) / len(candidates_lengths)\n",
    "    max_cands = max(candidates_lengths)\n",
    "    min_cands = min(candidates_lengths)\n",
    "    print(\"[DEBUG] spaCy candidates statistics on DEV:\")\n",
    "    print(f\"  Min #candidates: {min_cands}\")\n",
    "    print(f\"  Max #candidates: {max_cands}\")\n",
    "    print(f\"  Avg #candidates: {avg_cands:.2f}\")\n",
    "\n",
    "    # Show a sentence with many candidates as a sanity check\n",
    "    max_idx = candidates_lengths.index(max_cands)\n",
    "    print(f\"\\n[DEBUG] Sentence with the highest number of candidates (index {max_idx}):\")\n",
    "    print(\"  Text:\", dev_sentences[max_idx][\"sentence_text\"])\n",
    "    print(\"  #Candidates:\", max_cands)\n",
    "    print(\"  Candidates:\", dev_sentences[max_idx][\"spacy_candidates\"])\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Build predictions for DEV using spaCy candidates\n",
    "# -------------------------------------------------------------------\n",
    "spacy_dev_preds: List[List[str]] = [s[\"spacy_candidates\"] for s in dev_sentences]\n",
    "\n",
    "print(f\"\\nBuilt spaCy predictions for DEV: {len(spacy_dev_preds)} sentences\")\n",
    "assert len(spacy_dev_preds) == len(dev_sentences), \"Mismatch between preds and dev sentences!\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4744606c",
   "metadata": {},
   "source": [
    "### Save Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b5ad3580",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_predictions(predictions: List[List[str]], \n",
    "                     sentences: List[Dict], \n",
    "                     output_path: str):\n",
    "    \"\"\"Save predictions in competition format.\"\"\"\n",
    "    output = {'data': []}\n",
    "    for pred, sent in zip(predictions, sentences):\n",
    "        output['data'].append({\n",
    "            'document_id': sent['document_id'],\n",
    "            'paragraph_id': sent['paragraph_id'],\n",
    "            'sentence_id': sent['sentence_id'],\n",
    "            'term_list': pred\n",
    "        })\n",
    "    \n",
    "    os.makedirs(os.path.dirname(output_path) or '.', exist_ok=True)\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(output, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"Saved {len(predictions)} predictions to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "61142d01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "spaCy linguistic pipeline Results:\n",
      "Micro-averaged metrics:\n",
      "  Precision: 0.0509\n",
      "  Recall:    0.3725\n",
      "  F1 Score:  0.0896\n",
      "  TP=168, FP=3131, FN=283\n",
      "\n",
      "Type-level metrics:\n",
      "  Type Precision: 0.0408\n",
      "  Type Recall:    0.4174\n",
      "  Type F1 Score:  0.0743\n",
      "Saved 577 predictions to results/spacy_pipeline_dev_predictions.json\n",
      "✓ Predictions saved to: results/spacy_pipeline_dev_predictions.json\n"
     ]
    }
   ],
   "source": [
    "# Valutazione (se le metriche normalizzano già, usa dev_gold e spacy_dev_preds)\n",
    "precision, recall, f1, tp, fp, fn = micro_f1_score(dev_gold, spacy_dev_preds)\n",
    "type_precision, type_recall, type_f1 = type_f1_score(dev_gold, spacy_dev_preds)\n",
    "\n",
    "print(\"\\nspaCy linguistic pipeline Results:\")\n",
    "print(\"Micro-averaged metrics:\")\n",
    "print(f\"  Precision: {precision:.4f}\")\n",
    "print(f\"  Recall:    {recall:.4f}\")\n",
    "print(f\"  F1 Score:  {f1:.4f}\")\n",
    "print(f\"  TP={tp}, FP={fp}, FN={fn}\")\n",
    "print(\"\\nType-level metrics:\")\n",
    "print(f\"  Type Precision: {type_precision:.4f}\")\n",
    "print(f\"  Type Recall:    {type_recall:.4f}\")\n",
    "print(f\"  Type F1 Score:  {type_f1:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "# Salva predizioni in formato competizione (se hai già save_predictions definita)\n",
    "output_path = \"results/spacy_pipeline_dev_predictions.json\"\n",
    "save_predictions(spacy_dev_preds, dev_sentences, output_path)\n",
    "print(f\"✓ Predictions saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8203782c",
   "metadata": {},
   "source": [
    "### Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8cd07963",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Building detailed prediction table for DEBUG...\n",
      "✓ Detailed prediction table created.\n",
      "  Number of predicted term occurrences: 3299\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document_id</th>\n",
       "      <th>paragraph_id</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>sentence_text</th>\n",
       "      <th>term</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>doc_praiano_07</td>\n",
       "      <td>32</td>\n",
       "      <td>7</td>\n",
       "      <td>Non Domestica; CAMPEGGI, DISTRIBUTORI CARBURAN...</td>\n",
       "      <td>non domestica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>doc_praiano_07</td>\n",
       "      <td>32</td>\n",
       "      <td>7</td>\n",
       "      <td>Non Domestica; CAMPEGGI, DISTRIBUTORI CARBURAN...</td>\n",
       "      <td>campeggi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>doc_praiano_07</td>\n",
       "      <td>32</td>\n",
       "      <td>7</td>\n",
       "      <td>Non Domestica; CAMPEGGI, DISTRIBUTORI CARBURAN...</td>\n",
       "      <td>distributori carburanti, parcheggi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>doc_caserta_06</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Il presente disciplinare per la gestione dei c...</td>\n",
       "      <td>la gestione</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>doc_caserta_06</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Il presente disciplinare per la gestione dei c...</td>\n",
       "      <td>centri</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      document_id  paragraph_id  sentence_id  \\\n",
       "0  doc_praiano_07            32            7   \n",
       "1  doc_praiano_07            32            7   \n",
       "2  doc_praiano_07            32            7   \n",
       "3  doc_caserta_06             3            1   \n",
       "4  doc_caserta_06             3            1   \n",
       "\n",
       "                                       sentence_text  \\\n",
       "0  Non Domestica; CAMPEGGI, DISTRIBUTORI CARBURAN...   \n",
       "1  Non Domestica; CAMPEGGI, DISTRIBUTORI CARBURAN...   \n",
       "2  Non Domestica; CAMPEGGI, DISTRIBUTORI CARBURAN...   \n",
       "3  Il presente disciplinare per la gestione dei c...   \n",
       "4  Il presente disciplinare per la gestione dei c...   \n",
       "\n",
       "                                 term  \n",
       "0                       non domestica  \n",
       "1                            campeggi  \n",
       "2  distributori carburanti, parcheggi  \n",
       "3                         la gestione  \n",
       "4                              centri  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "rows = []\n",
    "\n",
    "print(\"\\nBuilding detailed prediction table for DEBUG...\")\n",
    "\n",
    "for sent in dev_sentences:\n",
    "    doc_id    = sent[\"document_id\"]\n",
    "    par_id    = sent[\"paragraph_id\"]\n",
    "    sent_id   = sent[\"sentence_id\"]\n",
    "    sent_text = sent[\"sentence_text\"]\n",
    "\n",
    "    preds = sent[\"spacy_candidates\"]  # this is a list of strings\n",
    "\n",
    "    for term in preds:\n",
    "        rows.append({\n",
    "            \"document_id\": doc_id,\n",
    "            \"paragraph_id\": par_id,\n",
    "            \"sentence_id\": sent_id,\n",
    "            \"sentence_text\": sent_text,\n",
    "            \"term\": term\n",
    "        })\n",
    "\n",
    "dev_df_spacy = pd.DataFrame(rows)\n",
    "\n",
    "print(\"✓ Detailed prediction table created.\")\n",
    "print(\"  Number of predicted term occurrences:\", len(dev_df_spacy))\n",
    "dev_df_spacy.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9c798e85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Gold table created.\n",
      "  Number of gold term occurrences: 451\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document_id</th>\n",
       "      <th>paragraph_id</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>sentence_text</th>\n",
       "      <th>term</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>doc_caserta_06</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Il presente disciplinare per la gestione dei c...</td>\n",
       "      <td>disciplina dei centri di raccolta dei rifiuti ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>doc_caserta_06</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Il presente disciplinare per la gestione dei c...</td>\n",
       "      <td>disciplinare per la gestione dei centri di rac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>doc_poggiomarino_01</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>È un Servizio Supplementare di raccolta, rivol...</td>\n",
       "      <td>raccolta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>doc_nola_05</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>ll servizio di raccolta dei rifiuti derivanti ...</td>\n",
       "      <td>servizio di raccolta dei rifiuti</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>doc_nola_05</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>ll servizio di raccolta dei rifiuti derivanti ...</td>\n",
       "      <td>sfalci e potature</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           document_id  paragraph_id  sentence_id  \\\n",
       "0       doc_caserta_06             3            1   \n",
       "1       doc_caserta_06             3            1   \n",
       "2  doc_poggiomarino_01             6            1   \n",
       "3          doc_nola_05             2            2   \n",
       "4          doc_nola_05             2            2   \n",
       "\n",
       "                                       sentence_text  \\\n",
       "0  Il presente disciplinare per la gestione dei c...   \n",
       "1  Il presente disciplinare per la gestione dei c...   \n",
       "2  È un Servizio Supplementare di raccolta, rivol...   \n",
       "3  ll servizio di raccolta dei rifiuti derivanti ...   \n",
       "4  ll servizio di raccolta dei rifiuti derivanti ...   \n",
       "\n",
       "                                                term  \n",
       "0  disciplina dei centri di raccolta dei rifiuti ...  \n",
       "1  disciplinare per la gestione dei centri di rac...  \n",
       "2                                           raccolta  \n",
       "3                   servizio di raccolta dei rifiuti  \n",
       "4                                  sfalci e potature  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows_gold = []\n",
    "\n",
    "for sent in dev_sentences:\n",
    "    doc_id    = sent[\"document_id\"]\n",
    "    par_id    = sent[\"paragraph_id\"]\n",
    "    sent_id   = sent[\"sentence_id\"]\n",
    "    sent_text = sent[\"sentence_text\"]\n",
    "\n",
    "    gold_terms = sent[\"terms\"]  # lista dei gold\n",
    "\n",
    "    for term in gold_terms:\n",
    "        rows_gold.append({\n",
    "            \"document_id\": doc_id,\n",
    "            \"paragraph_id\": par_id,\n",
    "            \"sentence_id\": sent_id,\n",
    "            \"sentence_text\": sent_text,\n",
    "            \"term\": term\n",
    "        })\n",
    "\n",
    "dev_df_gold = pd.DataFrame(rows_gold)\n",
    "print(\"✓ Gold table created.\")\n",
    "print(\"  Number of gold term occurrences:\", len(dev_df_gold))\n",
    "dev_df_gold.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "52d179f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fp_fn(gold_df, pred_df):\n",
    "    \"\"\"Return DataFrames for false positives and false negatives.\"\"\"\n",
    "    def normalize(df):\n",
    "        df = df.copy()\n",
    "        df[\"term\"] = df[\"term\"].str.lower().str.strip()\n",
    "        return df[[\"document_id\", \"paragraph_id\", \"sentence_id\", \"term\"]]\n",
    "\n",
    "    gold = normalize(gold_df)\n",
    "    pred = normalize(pred_df)\n",
    "\n",
    "    gold_set = set(gold.itertuples(index=False, name=None))\n",
    "    pred_set = set(pred.itertuples(index=False, name=None))\n",
    "\n",
    "    tp = gold_set & pred_set\n",
    "    fp = pred_set - gold_set\n",
    "    fn = gold_set - pred_set\n",
    "\n",
    "    fp_df = pd.DataFrame(list(fp), columns=[\"document_id\", \"paragraph_id\", \"sentence_id\", \"term\"])\n",
    "    fn_df = pd.DataFrame(list(fn), columns=[\"document_id\", \"paragraph_id\", \"sentence_id\", \"term\"])\n",
    "\n",
    "    return fp_df, fn_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "85609798",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== COUNTS ===\n",
      "False Positives: 3116\n",
      "False Negatives: 273\n",
      "True Positives: 168\n"
     ]
    }
   ],
   "source": [
    "fp_df, fn_df = get_fp_fn(dev_df_gold, dev_df_spacy)\n",
    "\n",
    "print(\"\\n=== COUNTS ===\")\n",
    "print(\"False Positives:\", len(fp_df))\n",
    "print(\"False Negatives:\", len(fn_df))\n",
    "print(\"True Positives:\", len(set(dev_df_gold.apply(tuple, axis=1)) \n",
    "                              & set(dev_df_spacy.apply(tuple, axis=1))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "58bbf823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== FALSE POSITIVES (examples) ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document_id</th>\n",
       "      <th>paragraph_id</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>term</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>doc_agropoli_09</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>durata del contratto</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>doc_agropoli_09</td>\n",
       "      <td>17</td>\n",
       "      <td>2</td>\n",
       "      <td>spesa a carico</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>doc_sorrento_10</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>1° aprile</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>doc_salerno_05</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>igiene della casa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>doc_santagnello_19</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>planimetrie degli immobili</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>doc_battipaglia_02</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>sindaco del comune</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>doc_capaccio_10</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>2° giovedì</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>doc_sarno_12</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>uranio presente nelle rocce</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>doc_nocerainferiore_06</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>torquato</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>doc_auletta_01</td>\n",
       "      <td>5</td>\n",
       "      <td>25</td>\n",
       "      <td>laboratori</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>doc_salerno_06</td>\n",
       "      <td>12</td>\n",
       "      <td>6</td>\n",
       "      <td>rifiuti</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>doc_caserta_06</td>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>rifiuti</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>doc_agropoli_09</td>\n",
       "      <td>17</td>\n",
       "      <td>2</td>\n",
       "      <td>tipo di pretesa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>doc_sorrento_10</td>\n",
       "      <td>54</td>\n",
       "      <td>2</td>\n",
       "      <td>componenti essenziali</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>doc_agropoli_09</td>\n",
       "      <td>29</td>\n",
       "      <td>3</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>doc_pellezzano_01</td>\n",
       "      <td>29</td>\n",
       "      <td>6</td>\n",
       "      <td>importo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>doc_sorrento_10</td>\n",
       "      <td>56</td>\n",
       "      <td>2</td>\n",
       "      <td>cv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>doc_auletta_01</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>piscine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>doc_sorrento_10</td>\n",
       "      <td>28</td>\n",
       "      <td>2</td>\n",
       "      <td>parte</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>doc_capaccio_06</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>chi</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               document_id  paragraph_id  sentence_id  \\\n",
       "0          doc_agropoli_09            17            0   \n",
       "1          doc_agropoli_09            17            2   \n",
       "2          doc_sorrento_10            18            0   \n",
       "3           doc_salerno_05             7            2   \n",
       "4       doc_santagnello_19             3            6   \n",
       "5       doc_battipaglia_02            28            0   \n",
       "6          doc_capaccio_10             8            3   \n",
       "7             doc_sarno_12             3            4   \n",
       "8   doc_nocerainferiore_06             3            0   \n",
       "9           doc_auletta_01             5           25   \n",
       "10          doc_salerno_06            12            6   \n",
       "11          doc_caserta_06            13            4   \n",
       "12         doc_agropoli_09            17            2   \n",
       "13         doc_sorrento_10            54            2   \n",
       "14         doc_agropoli_09            29            3   \n",
       "15       doc_pellezzano_01            29            6   \n",
       "16         doc_sorrento_10            56            2   \n",
       "17          doc_auletta_01             5            3   \n",
       "18         doc_sorrento_10            28            2   \n",
       "19         doc_capaccio_06             5            0   \n",
       "\n",
       "                           term  \n",
       "0          durata del contratto  \n",
       "1                spesa a carico  \n",
       "2                     1° aprile  \n",
       "3             igiene della casa  \n",
       "4    planimetrie degli immobili  \n",
       "5            sindaco del comune  \n",
       "6                    2° giovedì  \n",
       "7   uranio presente nelle rocce  \n",
       "8                      torquato  \n",
       "9                    laboratori  \n",
       "10                      rifiuti  \n",
       "11                      rifiuti  \n",
       "12              tipo di pretesa  \n",
       "13        componenti essenziali  \n",
       "14                           no  \n",
       "15                      importo  \n",
       "16                           cv  \n",
       "17                      piscine  \n",
       "18                        parte  \n",
       "19                          chi  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== FALSE NEGATIVES (examples) ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document_id</th>\n",
       "      <th>paragraph_id</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>term</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>doc_praiano_07</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>coefficiente proporzionale di produttività kb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>doc_nola_02</td>\n",
       "      <td>16</td>\n",
       "      <td>8</td>\n",
       "      <td>frazione secca</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>doc_salerno_05</td>\n",
       "      <td>7</td>\n",
       "      <td>16</td>\n",
       "      <td>conferire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>doc_caserta_02</td>\n",
       "      <td>64</td>\n",
       "      <td>9</td>\n",
       "      <td>isole ecologiche</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>doc_salerno_05</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>conferire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>doc_nocerainferiore_06</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>servizio ecologia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>doc_salerno_06</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>utenti</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>doc_caserta_02</td>\n",
       "      <td>66</td>\n",
       "      <td>2</td>\n",
       "      <td>tubi fluorescenti ed altri tubi contenenti mer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>doc_auletta_13</td>\n",
       "      <td>36</td>\n",
       "      <td>1</td>\n",
       "      <td>gestore dello spazzamento e lavaggio delle strade</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>doc_caserta_02</td>\n",
       "      <td>59</td>\n",
       "      <td>3</td>\n",
       "      <td>multimateriale</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>doc_salerno_03</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>raccolta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>doc_capaccio_10</td>\n",
       "      <td>14</td>\n",
       "      <td>6</td>\n",
       "      <td>secchiello</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>doc_caserta_02</td>\n",
       "      <td>68</td>\n",
       "      <td>8</td>\n",
       "      <td>indifferenziato</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>doc_nocerainferiore_06</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>pannolini</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>doc_auletta_13</td>\n",
       "      <td>36</td>\n",
       "      <td>1</td>\n",
       "      <td>gestore dello spazzamento e lavaggio</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>doc_capaccio_10</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>sacchetto trasparente</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>doc_salerno_06</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>utenze domestiche</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>doc_nocerainferiore_06</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>calendario utenze domestiche per la raccolta d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>doc_nocerainferiore_06</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>plastica e metalli</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>doc_poggiomarino_12</td>\n",
       "      <td>17</td>\n",
       "      <td>55</td>\n",
       "      <td>materiali non conferibili</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               document_id  paragraph_id  sentence_id  \\\n",
       "0           doc_praiano_07            21            1   \n",
       "1              doc_nola_02            16            8   \n",
       "2           doc_salerno_05             7           16   \n",
       "3           doc_caserta_02            64            9   \n",
       "4           doc_salerno_05            11            6   \n",
       "5   doc_nocerainferiore_06             6            0   \n",
       "6           doc_salerno_06            12            5   \n",
       "7           doc_caserta_02            66            2   \n",
       "8           doc_auletta_13            36            1   \n",
       "9           doc_caserta_02            59            3   \n",
       "10          doc_salerno_03             1            0   \n",
       "11         doc_capaccio_10            14            6   \n",
       "12          doc_caserta_02            68            8   \n",
       "13  doc_nocerainferiore_06            11            6   \n",
       "14          doc_auletta_13            36            1   \n",
       "15         doc_capaccio_10             6            6   \n",
       "16          doc_salerno_06            27            1   \n",
       "17  doc_nocerainferiore_06            10            0   \n",
       "18  doc_nocerainferiore_06             4            0   \n",
       "19     doc_poggiomarino_12            17           55   \n",
       "\n",
       "                                                 term  \n",
       "0       coefficiente proporzionale di produttività kb  \n",
       "1                                      frazione secca  \n",
       "2                                           conferire  \n",
       "3                                    isole ecologiche  \n",
       "4                                           conferire  \n",
       "5                                   servizio ecologia  \n",
       "6                                              utenti  \n",
       "7   tubi fluorescenti ed altri tubi contenenti mer...  \n",
       "8   gestore dello spazzamento e lavaggio delle strade  \n",
       "9                                      multimateriale  \n",
       "10                                           raccolta  \n",
       "11                                         secchiello  \n",
       "12                                    indifferenziato  \n",
       "13                                          pannolini  \n",
       "14               gestore dello spazzamento e lavaggio  \n",
       "15                              sacchetto trasparente  \n",
       "16                                  utenze domestiche  \n",
       "17  calendario utenze domestiche per la raccolta d...  \n",
       "18                                 plastica e metalli  \n",
       "19                          materiali non conferibili  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"\\n=== FALSE POSITIVES (examples) ===\")\n",
    "display(fp_df.head(20))\n",
    "\n",
    "print(\"\\n=== FALSE NEGATIVES (examples) ===\")\n",
    "display(fn_df.head(20))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
