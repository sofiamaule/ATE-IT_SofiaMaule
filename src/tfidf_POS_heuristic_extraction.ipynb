{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "235c2ab2",
      "metadata": {},
      "source": [
        "### Setup and dataset loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "67cedb0c",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import spacy\n",
        "import math\n",
        "from collections import Counter\n",
        "from typing import List, Dict\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "1b336c8d-a2af-408d-aaed-97e040d55077",
      "metadata": {
        "id": "1b336c8d-a2af-408d-aaed-97e040d55077",
        "outputId": "162e6127-2f81-4753-80ab-7fb00cb48ad8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train shape: (3423, 5)\n",
            "Dev shape  : (779, 5)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "pd.set_option(\"display.max_colwidth\", 200)\n",
        "\n",
        "def load_csv(path):\n",
        "    df = pd.read_csv(path)\n",
        "    if \"term\" in df.columns:\n",
        "        df[\"term\"] = df[\"term\"].apply(\n",
        "            lambda x: x.strip() if isinstance(x, str) else \"\"\n",
        "        )\n",
        "    return df\n",
        "\n",
        "train_path = \"https://raw.githubusercontent.com/nicolaCirillo/ate-it/main/data/subtask_a_train.csv\"\n",
        "dev_path   = \"https://raw.githubusercontent.com/nicolaCirillo/ate-it/main/data/subtask_a_dev.csv\"\n",
        "\n",
        "train_df = load_csv(train_path)\n",
        "dev_df   = load_csv(dev_path)\n",
        "\n",
        "print(\"Train shape:\", train_df.shape)\n",
        "print(\"Dev shape  :\", dev_df.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7b975b38",
      "metadata": {},
      "source": [
        "### Evaluation metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "331c5db7-c660-4438-9592-b8ad35c6a6b7",
      "metadata": {
        "id": "331c5db7-c660-4438-9592-b8ad35c6a6b7"
      },
      "outputs": [],
      "source": [
        "def micro_f1(gold_df, pred_df):\n",
        "    # normalize columns\n",
        "    def normalize(df):\n",
        "        df = df.copy()\n",
        "        df[\"term\"] = df[\"term\"].str.lower().str.strip()\n",
        "        return df[[\"document_id\", \"paragraph_id\", \"sentence_id\", \"term\"]]\n",
        "\n",
        "    gold = normalize(gold_df)\n",
        "    pred = normalize(pred_df)\n",
        "\n",
        "    gold_set = set(gold.itertuples(index=False, name=None))\n",
        "    pred_set = set(pred.itertuples(index=False, name=None))\n",
        "\n",
        "    tp = len(gold_set & pred_set)\n",
        "    fp = len(pred_set - gold_set)\n",
        "    fn = len(gold_set - pred_set)\n",
        "\n",
        "    precision = tp / (tp + fp) if tp + fp else 0.0\n",
        "    recall    = tp / (tp + fn) if tp + fn else 0.0\n",
        "    f1        = 2 * precision * recall / (precision + recall) if precision + recall else 0.0\n",
        "\n",
        "    return precision, recall, f1\n",
        "\n",
        "\n",
        "def type_f1(gold_df, pred_df):\n",
        "    gold_types = set(gold_df[\"term\"].str.lower().str.strip())\n",
        "    pred_types = set(pred_df[\"term\"].str.lower().str.strip())\n",
        "\n",
        "    gold_types.discard(\"\")\n",
        "    pred_types.discard(\"\")\n",
        "\n",
        "    tp = len(gold_types & pred_types)\n",
        "    fp = len(pred_types - gold_types)\n",
        "    fn = len(gold_types - pred_types)\n",
        "\n",
        "    precision = tp / (tp + fp) if tp + fp else 0.0\n",
        "    recall    = tp / (tp + fn) if tp + fn else 0.0\n",
        "    f1        = 2 * precision * recall / (precision + recall) if precision + recall else 0.0\n",
        "\n",
        "    return precision, recall, f1\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "22ab0ea6",
      "metadata": {},
      "source": [
        "### Load spaCy and stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "591f3f2f-360a-4789-81cf-d077c602c367",
      "metadata": {
        "id": "591f3f2f-360a-4789-81cf-d077c602c367"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Make sure stopwords are available\n",
        "try:\n",
        "    it_stopwords = set(stopwords.words(\"italian\"))\n",
        "except:\n",
        "    nltk.download(\"stopwords\")\n",
        "    it_stopwords = set(stopwords.words(\"italian\"))\n",
        "\n",
        "nlp = spacy.load(\"it_core_news_sm\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9cd98cf9",
      "metadata": {},
      "source": [
        "### 1. **Linguistic Filtering of Candidate Terms**\n",
        "\n",
        "Before we extract statistical n-grams, we apply linguistic rules to discard\n",
        "non-term-like spans.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "c3340450-ea6c-45c9-be78-c202a18a9488",
      "metadata": {
        "id": "c3340450-ea6c-45c9-be78-c202a18a9488"
      },
      "outputs": [],
      "source": [
        "# Unigram filter\n",
        "def is_valid_unigram_token(tok):\n",
        "    \"\"\"\n",
        "    Check if a single spaCy token is a valid unigram candidate.\n",
        "    Valid = alphabetic, non-stopword, sufficiently long, noun/proper noun.\n",
        "    \"\"\"\n",
        "    text = tok.text.lower().strip()\n",
        "\n",
        "    if len(text) < 3:\n",
        "        return False\n",
        "    if text in it_stopwords:\n",
        "        return False\n",
        "    if not text.isalpha():\n",
        "        return False\n",
        "    if tok.pos_ not in {\"NOUN\", \"PROPN\"}:\n",
        "        return False\n",
        "    \n",
        "    return True\n",
        "\n",
        "# N-gram (n>=2) span filter\n",
        "def is_valid_ngram_span_old(span_tokens):\n",
        "    \"\"\"\n",
        "    Rules for a valid multi-word candidate:\n",
        "    - No punctuation\n",
        "    - First token: NOUN / PROPN / ADJ\n",
        "    - Last token:  NOUN / PROPN / ADJ\n",
        "    - Must contain at least one noun/proper noun inside\n",
        "    - All alphabetic\n",
        "    \"\"\"\n",
        "    # no punctuation\n",
        "    for t in span_tokens:\n",
        "        if t.is_punct:\n",
        "            return False\n",
        "\n",
        "    first = span_tokens[0]\n",
        "    last  = span_tokens[-1]\n",
        "\n",
        "    # start: allow NOUN / PROPN / ADJ as first token\n",
        "    if first.pos_ not in {\"NOUN\", \"PROPN\", \"ADJ\"}:\n",
        "        return False\n",
        "\n",
        "    # end: must be noun or proper noun (your new constraint) or adjective \"raccolta differenziata\"\n",
        "    if last.pos_ not in {\"NOUN\", \"PROPN\", \"ADJ\"}:\n",
        "        return False\n",
        "\n",
        "    # must contain at least one noun/proper noun somewhere\n",
        "    if not any(t.pos_ in {\"NOUN\", \"PROPN\"} for t in span_tokens):\n",
        "        return False\n",
        "\n",
        "    # optional: all alphabetic\n",
        "    if not all(t.text.isalpha() for t in span_tokens):\n",
        "        return False\n",
        "\n",
        "    return True\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "8317d181",
      "metadata": {},
      "outputs": [],
      "source": [
        "def is_valid_ngram_span(span_tokens):\n",
        "    \"\"\"\n",
        "    Enhanced POS-based n-gram filter.\n",
        "    Accepts:\n",
        "      - base rules (start-end POS)\n",
        "      - new syntactic patterns:\n",
        "          * NOUN + ADP + DET + NOUN\n",
        "          * NOUN + CCONJ + NOUN\n",
        "          * ADJ  + NOUN + ADP + NOUN\n",
        "    \"\"\"\n",
        "\n",
        "    # Reject punctuation inside span\n",
        "    for t in span_tokens:\n",
        "        if t.is_punct:\n",
        "            return False\n",
        "\n",
        "    first = span_tokens[0]\n",
        "    last  = span_tokens[-1]\n",
        "\n",
        "    # Base rule: valid start\n",
        "    if first.pos_ not in {\"NOUN\", \"PROPN\", \"ADJ\"}:\n",
        "        return False\n",
        "\n",
        "    # Base rule: valid end\n",
        "    if last.pos_ not in {\"NOUN\", \"PROPN\", \"ADJ\"}:\n",
        "        return False\n",
        "\n",
        "    # Must contain at least one noun or proper noun\n",
        "    if not any(t.pos_ in {\"NOUN\", \"PROPN\"} for t in span_tokens):\n",
        "        return False\n",
        "\n",
        "    # All alphabetic tokens (optional, but keeps noise low)\n",
        "    if not all(t.text.isalpha() for t in span_tokens):\n",
        "        return False\n",
        "\n",
        "    # NEW ADVANCED SYNTAX PATTERNS\n",
        "    # Pattern: NOUN + ADP + DET + NOUN\n",
        "    if len(span_tokens) == 4:\n",
        "        if (span_tokens[0].pos_ == \"NOUN\" and\n",
        "            span_tokens[1].pos_ == \"ADP\"  and\n",
        "            span_tokens[2].pos_ == \"DET\"  and\n",
        "            span_tokens[3].pos_ in {\"NOUN\", \"PROPN\"}):\n",
        "            return True\n",
        "\n",
        "    # Pattern: NOUN + CCONJ + NOUN\n",
        "    if len(span_tokens) == 3:\n",
        "        if (span_tokens[0].pos_ == \"NOUN\"  and\n",
        "            span_tokens[1].pos_ == \"CCONJ\" and\n",
        "            span_tokens[2].pos_ == \"NOUN\"):\n",
        "            return True\n",
        "\n",
        "    # Pattern: ADJ + NOUN + ADP + NOUN\n",
        "    if len(span_tokens) == 4:\n",
        "        if (span_tokens[0].pos_ == \"ADJ\"  and\n",
        "            span_tokens[1].pos_ == \"NOUN\" and\n",
        "            span_tokens[2].pos_ == \"ADP\"  and\n",
        "            span_tokens[3].pos_ in {\"NOUN\", \"PROPN\"}):\n",
        "            return True\n",
        "\n",
        "    # If not matched custom patterns, return base accept\n",
        "    return True\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "741abf61",
      "metadata": {},
      "outputs": [],
      "source": [
        "def spacy_dependency_subtrees(doc):\n",
        "    \"\"\"\n",
        "    Extract candidate multiword terms from dependency subtrees\n",
        "    rooted at NOUN/PROPN heads.\n",
        "    This captures things like:\n",
        "        - \"raccolta differenziata\"\n",
        "        - \"gestione dei rifiuti urbani\"\n",
        "        - \"svuotamento dei carrellati condominiali\"\n",
        "    \"\"\"\n",
        "\n",
        "    candidates = []\n",
        "\n",
        "    for token in doc:\n",
        "        if token.pos_ in {\"NOUN\", \"PROPN\"}:\n",
        "            subtree = list(token.subtree)\n",
        "            # Avoid huge subtrees (>6 words)\n",
        "            if 1 < len(subtree) <= 6:\n",
        "                # Filter punctuation and spaces\n",
        "                words = [t.text for t in subtree if not t.is_space and not t.is_punct]\n",
        "                if len(words) >= 2:\n",
        "                    candidates.append(\" \".join(words).lower())\n",
        "\n",
        "    return candidates\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "d59168c7",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of unique gold terms: 713\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "term\n",
              "vetro                    69\n",
              "porta a porta            66\n",
              "rifiuti                  57\n",
              "conferire                57\n",
              "multimateriale           56\n",
              "conferimento             55\n",
              "indifferenziato          40\n",
              "carta e cartone          36\n",
              "plastica                 32\n",
              "rifiuti organici         30\n",
              "isole ecologiche         29\n",
              "raccolta                 27\n",
              "utenze domestiche        27\n",
              "utente                   23\n",
              "utenza                   23\n",
              "utenti                   22\n",
              "utenze non domestiche    20\n",
              "umido                    19\n",
              "centro di raccolta       19\n",
              "isola ecologica          18\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Gold terms from the training set (from the `term` column)\n",
        "gold_terms = train_df[\"term\"].str.lower().str.strip()\n",
        "gold_terms = gold_terms[gold_terms != \"\"]  # drop empty\n",
        "\n",
        "# How often each string is annotated as a term\n",
        "gold_counts = gold_terms.value_counts()\n",
        "\n",
        "print(\"Number of unique gold terms:\", len(gold_counts))\n",
        "gold_counts.head(20)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ljJvVUrwwb7",
      "metadata": {
        "id": "9ljJvVUrwwb7"
      },
      "source": [
        "### 2. Statistical N-gram Extraction (TF, DF, TF-IDF)\n",
        "\n",
        "Once we have linguistic filters, we extract valid n-grams from each document\n",
        "and measure their importance statistically.\n",
        "\n",
        "For each n-gram:\n",
        "- **TF** (Term Frequency): how often it appears in the corpus  \n",
        "- **DF** (Document Frequency): in how many documents it appears  \n",
        "- **TF-IDF**: penalizes generic expressions and boosts informative ones  \n",
        "\n",
        "This gives a first ranking of “term-like” units based solely on statistics.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "7ca1072c-991f-4dbb-95ef-5e71bb8680d1",
      "metadata": {
        "id": "7ca1072c-991f-4dbb-95ef-5e71bb8680d1"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "from collections import Counter\n",
        "\n",
        "def build_ngram_tfidf_from_documents_pos(train_df, max_n=3):\n",
        "    tf_counter = Counter()\n",
        "    df_counter = Counter()\n",
        "\n",
        "    doc_ids = train_df[\"document_id\"].unique()\n",
        "    N_docs = len(doc_ids)\n",
        "\n",
        "    for doc_id in doc_ids:\n",
        "        # concatenate all sentences of the document\n",
        "        doc_texts = train_df[train_df[\"document_id\"] == doc_id][\"sentence_text\"]\n",
        "        full_text = \" \".join(doc_texts)\n",
        "\n",
        "        spacy_doc = nlp(full_text)\n",
        "        tokens = [t for t in spacy_doc if not t.is_space]\n",
        "        L = len(tokens)\n",
        "\n",
        "        doc_ngrams = set()\n",
        "\n",
        "        for n in range(1, max_n + 1):\n",
        "            for i in range(L - n + 1):\n",
        "                span_tokens = tokens[i:i+n]\n",
        "\n",
        "                # unigram case\n",
        "                if n == 1:\n",
        "                    if not is_valid_unigram_token(span_tokens[0]):\n",
        "                        continue\n",
        "                else:\n",
        "                    if not is_valid_ngram_span(span_tokens):\n",
        "                        continue\n",
        "\n",
        "                ngram = \" \".join(t.text.lower() for t in span_tokens)\n",
        "\n",
        "                tf_counter[ngram] += 1\n",
        "                doc_ngrams.add(ngram)\n",
        "\n",
        "        # DF increment\n",
        "        for ng in doc_ngrams:\n",
        "            df_counter[ng] += 1\n",
        "\n",
        "    # compute TF-IDF\n",
        "    tfidf = {}\n",
        "    for ng, tf in tf_counter.items():\n",
        "        df_val = df_counter.get(ng, 0)\n",
        "        idf = math.log((1 + N_docs) / (1 + df_val)) + 1\n",
        "        tfidf[ng] = tf * idf\n",
        "\n",
        "    return (\n",
        "        pd.Series(tf_counter),\n",
        "        pd.Series(df_counter),\n",
        "        pd.Series(tfidf).sort_values(ascending=False),\n",
        "        N_docs,\n",
        "    )\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "0b31e717",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "porta               937.244905\n",
              "carta               851.663186\n",
              "rifiuti             789.963230\n",
              "raccolta            782.200573\n",
              "vetro               597.029624\n",
              "cartone             593.471020\n",
              "conferimento        526.726652\n",
              "contenitori         523.482496\n",
              "imballaggi          500.018994\n",
              "servizio            495.492322\n",
              "porta a porta       453.938402\n",
              "multimateriale      435.929855\n",
              "plastica            435.028606\n",
              "organici            401.966809\n",
              "rifiuti organici    370.383598\n",
              "frazione            357.815257\n",
              "indifferenziato     355.399021\n",
              "ore                 348.895607\n",
              "carta e cartone     328.744094\n",
              "utenze              322.041205\n",
              "dtype: float64"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tf_series_pos, df_series_pos, tfidf_series_pos, N_docs = \\\n",
        "    build_ngram_tfidf_from_documents_pos(train_df, max_n=5)\n",
        "\n",
        "tfidf_series_pos.head(20)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "PrH0C_Omw9O4",
      "metadata": {
        "id": "PrH0C_Omw9O4"
      },
      "source": [
        "## 3. **Supervised boosting using TRAIN gold**\n",
        "\n",
        "For each n-gram:\n",
        "- If it appears in the training gold annotations, we **boost its score**\n",
        "- The boosting factor depends on how frequently it appears as a gold term\n",
        "- Controlled by parameter **alpha**\n",
        "\n",
        "This integrates weak supervision:\n",
        "- gold terms become more prominent in the final ranking  \n",
        "- irrelevant but frequent n-grams get lower priority\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "5af7d478-658a-4230-b48f-84720422b777",
      "metadata": {
        "id": "5af7d478-658a-4230-b48f-84720422b777"
      },
      "outputs": [],
      "source": [
        "def build_supervised_tfidf(tfidf_series, gold_counts, alpha=1.0):\n",
        "    \"\"\"\n",
        "    Boost TF-IDF scores for n-grams that appear more often as gold terms.\n",
        "\n",
        "    tfidf_series: pandas Series, index = n-gram string, value = base TF-IDF\n",
        "    gold_counts:  pandas Series, index = term string, value = gold frequency\n",
        "    alpha:        strength of boosting (0 = no supervision effect)\n",
        "    \"\"\"\n",
        "    if len(gold_counts) > 0:\n",
        "        max_gold = gold_counts.max()\n",
        "    else:\n",
        "        max_gold = 1\n",
        "\n",
        "    scores = {}\n",
        "\n",
        "    for ng, base_tfidf in tfidf_series.items():\n",
        "        # how many times this n-gram is annotated as a term\n",
        "        gold_freq = gold_counts.get(ng, 0)\n",
        "\n",
        "        if gold_freq > 0:\n",
        "            norm = gold_freq / max_gold        # in (0,1]\n",
        "            boost = 1.0 + alpha * norm         # >= 1.0\n",
        "        else:\n",
        "            boost = 1.0                        # no boost for non-gold\n",
        "\n",
        "        scores[ng] = base_tfidf * boost #boosting frequency\n",
        "\n",
        "    supervised_series = pd.Series(scores).sort_values(ascending=False)\n",
        "    return supervised_series"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "d6496bd0-5159-4a18-a6e6-1ad8f28a0cb0",
      "metadata": {
        "id": "d6496bd0-5159-4a18-a6e6-1ad8f28a0cb0",
        "outputId": "f20d2379-2f40-4a43-9d83-e36c576750e6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "rifiuti             1442.541551\n",
              "vetro               1194.059247\n",
              "raccolta            1088.279058\n",
              "carta               1061.493247\n",
              "conferimento         946.581231\n",
              "porta                937.244905\n",
              "porta a porta        888.140352\n",
              "multimateriale       789.727999\n",
              "plastica             636.781004\n",
              "cartone              636.476166\n",
              "indifferenziato      561.427439\n",
              "rifiuti organici     531.419944\n",
              "contenitori          523.482496\n",
              "imballaggi           521.758950\n",
              "carta e cartone      500.262752\n",
              "servizio             495.492322\n",
              "organici             401.966809\n",
              "frazione             378.558170\n",
              "utenze               354.712052\n",
              "ore                  348.895607\n",
              "dtype: float64"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "alpha = 1.0  # try 1.0, 2.0, etc.\n",
        "tfidf_supervised = build_supervised_tfidf(tfidf_series_pos, gold_counts, alpha=alpha)\n",
        "\n",
        "tfidf_supervised.head(20)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d5a62b7d",
      "metadata": {},
      "source": [
        "### **4. Build memory (vocabulary of statistical candidates)**\n",
        "\n",
        "select the highest-scoring n-grams as our \"memory\":\n",
        "a dictionary mapping n-gram length → set of n-gram tuples.\n",
        "Only n-grams with supervised TF-IDF above a threshold are kept."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "33ea163b-195b-403b-a1f8-aa4e486b4144",
      "metadata": {
        "id": "33ea163b-195b-403b-a1f8-aa4e486b4144",
        "outputId": "bdec76b3-ba0f-4be1-cd3c-1e1830afa02f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Threshold: 100.0\n",
            "Max n-gram length in memory: 5\n",
            "Number of n-grams selected: 186\n"
          ]
        }
      ],
      "source": [
        "   \n",
        "def build_ngram_memory_tfidf(tfidf_series, threshold):\n",
        "    \"\"\"\n",
        "    Select n-grams with TF-IDF above threshold and build\n",
        "    a dictionary memory_by_len[length] = set of tuples.\n",
        "    \"\"\"\n",
        "    selected = tfidf_series[tfidf_series >= threshold]\n",
        "    termlist = list(selected.index)\n",
        "\n",
        "    memory_by_len = {}\n",
        "    for phrase in termlist:\n",
        "        tokens = phrase.split(\" \")\n",
        "        L = len(tokens)\n",
        "        if L not in memory_by_len:\n",
        "            memory_by_len[L] = set()\n",
        "        memory_by_len[L].add(tuple(tokens))\n",
        "\n",
        "    max_len = max(memory_by_len.keys()) if memory_by_len else 0\n",
        "    return memory_by_len, max_len\n",
        "\n",
        "threshold = 100.0  # TF-IDF threshold on the *boosted* scores\n",
        "memory_by_len_sup, max_ngram_len_sup = build_ngram_memory_tfidf(tfidf_supervised, threshold)\n",
        "\n",
        "print(\"Threshold:\", threshold)\n",
        "print(\"Max n-gram length in memory:\", max_ngram_len_sup)\n",
        "total_terms = sum(len(s) for s in memory_by_len_sup.values())\n",
        "print(\"Number of n-grams selected:\", total_terms)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4779f897",
      "metadata": {},
      "source": [
        "### **5. Extract candidates from each DEV sentence**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "4c6344a0-3ff4-4a67-b244-26d5361aa48e",
      "metadata": {
        "id": "4c6344a0-3ff4-4a67-b244-26d5361aa48e"
      },
      "outputs": [],
      "source": [
        "def tokenize(text: str):\n",
        "    \"\"\"Simple tokenizer compatible with our memory matcher.\"\"\"\n",
        "    doc = nlp(text)\n",
        "    return [t.text.lower() for t in doc if t.is_alpha]\n",
        "\n",
        "def extract_terms_from_sentence_ngram(sentence, memory_by_len, max_ngram_len):\n",
        "    doc = nlp(sentence)\n",
        "\n",
        "    # --- 1) Dependency-based subtree extraction\n",
        "    dep_candidates = spacy_dependency_subtrees(doc)\n",
        "\n",
        "    # --- 2) Standard n-gram scanning\n",
        "    tokens = [t.text.lower() for t in doc if t.is_alpha]\n",
        "    L = len(tokens)\n",
        "    i = 0\n",
        "    found_terms = []\n",
        "\n",
        "    while i < L:\n",
        "        matched = None\n",
        "        matched_len = 0\n",
        "\n",
        "        for span_len in range(max_ngram_len, 0, -1):\n",
        "            if i + span_len > L:\n",
        "                continue\n",
        "            span = tuple(tokens[i:i+span_len])\n",
        "            if span_len in memory_by_len and span in memory_by_len[span_len]:\n",
        "                matched = \" \".join(span)\n",
        "                matched_len = span_len\n",
        "                break\n",
        "\n",
        "        if matched:\n",
        "            found_terms.append(matched)\n",
        "            i += matched_len\n",
        "        else:\n",
        "            i += 1\n",
        "\n",
        "    # --- 3) Combine & deduplicate\n",
        "    all_candidates = dep_candidates + found_terms\n",
        "    seen = set()\n",
        "    unique = []\n",
        "    for t in all_candidates:\n",
        "        if t not in seen:\n",
        "            seen.add(t)\n",
        "            unique.append(t)\n",
        "\n",
        "    return unique\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "d4a3c34b",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build prediction dataframe\n",
        "rows = []\n",
        "\n",
        "for _, row in dev_df.iterrows():\n",
        "    doc_id    = row[\"document_id\"]\n",
        "    par_id    = row[\"paragraph_id\"]\n",
        "    sent_id   = row[\"sentence_id\"]\n",
        "    sent_text = row[\"sentence_text\"]\n",
        "\n",
        "    preds = extract_terms_from_sentence_ngram(\n",
        "        sent_text,\n",
        "        memory_by_len_sup,\n",
        "        max_ngram_len_sup\n",
        "    )\n",
        "\n",
        "    for term in preds:\n",
        "        rows.append({\n",
        "            \"document_id\": doc_id,\n",
        "            \"paragraph_id\": par_id,\n",
        "            \"sentence_id\": sent_id,\n",
        "            \"sentence_text\": sent_text,\n",
        "            \"term\": term\n",
        "        })\n",
        "\n",
        "dev_df_tfidf_sup = pd.DataFrame(rows)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eae7ea50",
      "metadata": {},
      "source": [
        "### Save predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "id": "2d78627b-53f9-4e6e-a41a-f0c994b96d78",
      "metadata": {
        "id": "2d78627b-53f9-4e6e-a41a-f0c994b96d78"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "from typing import Dict, List\n",
        "import pandas as pd\n",
        "\n",
        "def save_predictions_stats(pred_df: pd.DataFrame,\n",
        "                           dev_df: pd.DataFrame,\n",
        "                           output_path: str):\n",
        "\n",
        "    output = {\"data\": []}\n",
        "\n",
        "    # Group predicted terms by (doc, par, sentence)\n",
        "    grouped = pred_df.groupby(\n",
        "        [\"document_id\", \"paragraph_id\", \"sentence_id\"]\n",
        "    )[\"term\"].apply(list).to_dict()\n",
        "\n",
        "    for _, row in dev_df.iterrows():\n",
        "        key = (\n",
        "            row[\"document_id\"],\n",
        "            row[\"paragraph_id\"],\n",
        "            row[\"sentence_id\"],\n",
        "        )\n",
        "\n",
        "        term_list = grouped.get(key, [])\n",
        "\n",
        "        output[\"data\"].append({\n",
        "            \"document_id\": row[\"document_id\"],\n",
        "            \"paragraph_id\": row[\"paragraph_id\"],\n",
        "            \"sentence_id\": row[\"sentence_id\"],\n",
        "            \"term_list\": term_list\n",
        "        })\n",
        "\n",
        "    os.makedirs(os.path.dirname(output_path) or \".\", exist_ok=True)\n",
        "\n",
        "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(output, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    print(f\"✓ Saved predictions to {output_path}\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "id": "c100de32",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Saved predictions to ../predictions/subtask_a_statistics_extraction_improved.json\n"
          ]
        }
      ],
      "source": [
        "output_path = \"../predictions/subtask_a_statistics_extraction_improved.json\"\n",
        "\n",
        "\n",
        "save_predictions_stats(\n",
        "    pred_df=dev_df_tfidf_sup,\n",
        "    dev_df=dev_df,\n",
        "    output_path=output_path\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "98601598",
      "metadata": {},
      "source": [
        "### Evaluation and debug"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "a36c304c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== SUPERVISED (GOLD-BOOSTED - IMPROVED) TF-IDF MODEL ===\n",
            "alpha      : 1.0\n",
            "threshold  : 100.0\n",
            "Micro P/R/F: 0.083 / 0.271 / 0.127\n",
            "Type  P/R/F: 0.047 / 0.322 / 0.082\n"
          ]
        }
      ],
      "source": [
        "p_micro, r_micro, f1_micro = micro_f1(dev_df, dev_df_tfidf_sup)\n",
        "p_type,  r_type,  f1_type  = type_f1(dev_df, dev_df_tfidf_sup)\n",
        "\n",
        "print(\"=== SUPERVISED (GOLD-BOOSTED - IMPROVED) TF-IDF MODEL ===\")\n",
        "print(\"alpha      :\", alpha)\n",
        "print(\"threshold  :\", threshold)\n",
        "print(f\"Micro P/R/F: {p_micro:.3f} / {r_micro:.3f} / {f1_micro:.3f}\")\n",
        "print(f\"Type  P/R/F: {p_type:.3f} / {r_type:.3f} / {f1_type:.3f}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "56b6d2cb",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "False Positives: 2133\n",
            "False Negatives: 574\n"
          ]
        }
      ],
      "source": [
        "def get_fp_fn(gold_df, pred_df):\n",
        "    # normalize and reduce to comparable columns\n",
        "    def normalize(df):\n",
        "        df = df.copy()\n",
        "        df[\"term\"] = df[\"term\"].str.lower().str.strip()\n",
        "        return df[[\"document_id\", \"paragraph_id\", \"sentence_id\", \"term\"]]\n",
        "\n",
        "    gold = normalize(gold_df)\n",
        "    pred = normalize(pred_df)\n",
        "\n",
        "    gold_set = set(gold.itertuples(index=False, name=None))\n",
        "    pred_set = set(pred.itertuples(index=False, name=None))\n",
        "\n",
        "    tp = gold_set & pred_set\n",
        "    fp = pred_set - gold_set\n",
        "    fn = gold_set - pred_set\n",
        "\n",
        "    # convert back to dataframes for readability\n",
        "    fp_df = pd.DataFrame(list(fp), columns=[\"document_id\", \"paragraph_id\", \"sentence_id\", \"term\"])\n",
        "    fn_df = pd.DataFrame(list(fn), columns=[\"document_id\", \"paragraph_id\", \"sentence_id\", \"term\"])\n",
        "\n",
        "    return fp_df, fn_df\n",
        "\n",
        "fp_df, fn_df = get_fp_fn(dev_df, dev_df_tfidf_sup)\n",
        "\n",
        "print(\"False Positives:\", len(fp_df))\n",
        "print(\"False Negatives:\", len(fn_df))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "id": "9d7053ab-e28f-472f-aa6b-49d7c054e2e0",
      "metadata": {
        "id": "9d7053ab-e28f-472f-aa6b-49d7c054e2e0",
        "outputId": "ece4219e-4161-4fe5-e701-41727df5bf85"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== FALSE POSITIVES (predicted but not gold) ===\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>document_id</th>\n",
              "      <th>paragraph_id</th>\n",
              "      <th>sentence_id</th>\n",
              "      <th>term</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>doc_praiano_07</td>\n",
              "      <td>28</td>\n",
              "      <td>0</td>\n",
              "      <td>l' immediata eseguibilità del presente atto</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>doc_sarno_12</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>in alcuni casi</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>doc_praiano_05</td>\n",
              "      <td>13</td>\n",
              "      <td>2</td>\n",
              "      <td>gestione</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>doc_agropoli_09</td>\n",
              "      <td>17</td>\n",
              "      <td>2</td>\n",
              "      <td>dei rifiuti</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>doc_sorrento_15</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>di bonifica</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>doc_capaccio_21</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>in carta e cartone</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>doc_nola_02</td>\n",
              "      <td>8</td>\n",
              "      <td>23</td>\n",
              "      <td>frazione</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>doc_battipaglia_02</td>\n",
              "      <td>20</td>\n",
              "      <td>2</td>\n",
              "      <td>dell' ordinamento</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>doc_francavillais_02</td>\n",
              "      <td>12</td>\n",
              "      <td>17</td>\n",
              "      <td>tale organo</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>doc_nola_02</td>\n",
              "      <td>8</td>\n",
              "      <td>23</td>\n",
              "      <td>con il relativo codice cer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>doc_poggiomarino_01</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>deposito dalle</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>doc_battipaglia_02</td>\n",
              "      <td>20</td>\n",
              "      <td>0</td>\n",
              "      <td>gestione</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>doc_caserta_06</td>\n",
              "      <td>10</td>\n",
              "      <td>4</td>\n",
              "      <td>rifiuti</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>doc_agropoli_09</td>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "      <td>di gara</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>doc_praiano_05</td>\n",
              "      <td>13</td>\n",
              "      <td>2</td>\n",
              "      <td>nel rispetto</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>doc_praiano_04</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>ordinanza nr 50 |</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>doc_capaccio_28</td>\n",
              "      <td>26</td>\n",
              "      <td>2</td>\n",
              "      <td>del suo contenuto</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>doc_fisciano_02</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>della scrivente società</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>doc_capaccio_28</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>disciplinare di gara</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>doc_sorrento_10</td>\n",
              "      <td>30</td>\n",
              "      <td>0</td>\n",
              "      <td>con il ministero dell' interno</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             document_id  paragraph_id  sentence_id  \\\n",
              "0         doc_praiano_07            28            0   \n",
              "1           doc_sarno_12             3            4   \n",
              "2         doc_praiano_05            13            2   \n",
              "3        doc_agropoli_09            17            2   \n",
              "4        doc_sorrento_15             2            0   \n",
              "5        doc_capaccio_21             1            7   \n",
              "6            doc_nola_02             8           23   \n",
              "7     doc_battipaglia_02            20            2   \n",
              "8   doc_francavillais_02            12           17   \n",
              "9            doc_nola_02             8           23   \n",
              "10   doc_poggiomarino_01             3            1   \n",
              "11    doc_battipaglia_02            20            0   \n",
              "12        doc_caserta_06            10            4   \n",
              "13       doc_agropoli_09             7            0   \n",
              "14        doc_praiano_05            13            2   \n",
              "15        doc_praiano_04             2            0   \n",
              "16       doc_capaccio_28            26            2   \n",
              "17       doc_fisciano_02             5            4   \n",
              "18       doc_capaccio_28             3            1   \n",
              "19       doc_sorrento_10            30            0   \n",
              "\n",
              "                                           term  \n",
              "0   l' immediata eseguibilità del presente atto  \n",
              "1                                in alcuni casi  \n",
              "2                                      gestione  \n",
              "3                                   dei rifiuti  \n",
              "4                                   di bonifica  \n",
              "5                            in carta e cartone  \n",
              "6                                      frazione  \n",
              "7                             dell' ordinamento  \n",
              "8                                   tale organo  \n",
              "9                    con il relativo codice cer  \n",
              "10                               deposito dalle  \n",
              "11                                     gestione  \n",
              "12                                      rifiuti  \n",
              "13                                      di gara  \n",
              "14                                 nel rispetto  \n",
              "15                            ordinanza nr 50 |  \n",
              "16                            del suo contenuto  \n",
              "17                      della scrivente società  \n",
              "18                         disciplinare di gara  \n",
              "19               con il ministero dell' interno  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== FALSE NEGATIVES (gold but missed) ===\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>document_id</th>\n",
              "      <th>paragraph_id</th>\n",
              "      <th>sentence_id</th>\n",
              "      <th>term</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>doc_agropoli_09</td>\n",
              "      <td>29</td>\n",
              "      <td>1</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>doc_caserta_02</td>\n",
              "      <td>66</td>\n",
              "      <td>1</td>\n",
              "      <td>porta a porta</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>doc_salerno_03</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>differenziata</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>doc_caserta_02</td>\n",
              "      <td>64</td>\n",
              "      <td>12</td>\n",
              "      <td>r.a.e.e.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>doc_sorrento_22</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>pneumatici</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>doc_salerno_05</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>sacchetti di carta</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>doc_auletta_04</td>\n",
              "      <td>9</td>\n",
              "      <td>4</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>doc_salerno_06</td>\n",
              "      <td>27</td>\n",
              "      <td>1</td>\n",
              "      <td>svuotamento dei carrellati condominiali</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>doc_agropoli_13</td>\n",
              "      <td>1</td>\n",
              "      <td>14</td>\n",
              "      <td>conferire</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>doc_auletta_13</td>\n",
              "      <td>36</td>\n",
              "      <td>1</td>\n",
              "      <td>gestore dello spazzamento e lavaggio</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>doc_sorrento_05</td>\n",
              "      <td>9</td>\n",
              "      <td>4</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>doc_praiano_07</td>\n",
              "      <td>32</td>\n",
              "      <td>12</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>doc_capaccio_21</td>\n",
              "      <td>13</td>\n",
              "      <td>3</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>doc_sorrento_05</td>\n",
              "      <td>28</td>\n",
              "      <td>0</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>doc_nola_02</td>\n",
              "      <td>12</td>\n",
              "      <td>3</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>doc_praiano_05</td>\n",
              "      <td>13</td>\n",
              "      <td>2</td>\n",
              "      <td>gestione dei rifiuti</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>doc_capaccio_10</td>\n",
              "      <td>9</td>\n",
              "      <td>4</td>\n",
              "      <td>busta con legaccio</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>doc_francavillais_09</td>\n",
              "      <td>19</td>\n",
              "      <td>1</td>\n",
              "      <td>ruote gommate</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>doc_poggiomarino_01</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>olii esausti</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>doc_capaccio_15</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>differenziati</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             document_id  paragraph_id  sentence_id  \\\n",
              "0        doc_agropoli_09            29            1   \n",
              "1         doc_caserta_02            66            1   \n",
              "2         doc_salerno_03             1            0   \n",
              "3         doc_caserta_02            64           12   \n",
              "4        doc_sorrento_22             2            0   \n",
              "5         doc_salerno_05             7            2   \n",
              "6         doc_auletta_04             9            4   \n",
              "7         doc_salerno_06            27            1   \n",
              "8        doc_agropoli_13             1           14   \n",
              "9         doc_auletta_13            36            1   \n",
              "10       doc_sorrento_05             9            4   \n",
              "11        doc_praiano_07            32           12   \n",
              "12       doc_capaccio_21            13            3   \n",
              "13       doc_sorrento_05            28            0   \n",
              "14           doc_nola_02            12            3   \n",
              "15        doc_praiano_05            13            2   \n",
              "16       doc_capaccio_10             9            4   \n",
              "17  doc_francavillais_09            19            1   \n",
              "18   doc_poggiomarino_01             5            1   \n",
              "19       doc_capaccio_15             2            2   \n",
              "\n",
              "                                       term  \n",
              "0                                            \n",
              "1                             porta a porta  \n",
              "2                             differenziata  \n",
              "3                                  r.a.e.e.  \n",
              "4                                pneumatici  \n",
              "5                        sacchetti di carta  \n",
              "6                                            \n",
              "7   svuotamento dei carrellati condominiali  \n",
              "8                                 conferire  \n",
              "9      gestore dello spazzamento e lavaggio  \n",
              "10                                           \n",
              "11                                           \n",
              "12                                           \n",
              "13                                           \n",
              "14                                           \n",
              "15                     gestione dei rifiuti  \n",
              "16                       busta con legaccio  \n",
              "17                            ruote gommate  \n",
              "18                             olii esausti  \n",
              "19                            differenziati  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "print(\"\\n=== FALSE POSITIVES (predicted but not gold) ===\")\n",
        "display(fp_df.head(20))\n",
        "\n",
        "print(\"\\n=== FALSE NEGATIVES (gold but missed) ===\")\n",
        "display(fn_df.head(20))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b9a0e8dc",
      "metadata": {},
      "source": [
        "## Precision-Prior Filtering for Statistical Term Extraction\n",
        "This section implements a refinement over pure TF-IDF statistical extraction.\n",
        "\n",
        "We add:\n",
        "- **Gold term normalization**\n",
        "- **A prior precision estimate for each n-gram**\n",
        "- **A dual-threshold filter** combining TF-IDF + precision-prior\n",
        "- **Prediction extraction**\n",
        "- **Evaluation**\n",
        "- **Grid search hyperparameter tuning**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "76ffe62f-3eb1-4bc5-b0c8-e34bcbb069d8",
      "metadata": {
        "id": "76ffe62f-3eb1-4bc5-b0c8-e34bcbb069d8"
      },
      "outputs": [],
      "source": [
        "def normalize_gold_terms(gold_series):\n",
        "    valid_terms = []\n",
        "\n",
        "    for term in gold_series.dropna():\n",
        "        term = term.lower().strip()\n",
        "        if not term:\n",
        "            continue\n",
        "        doc = nlp(term)\n",
        "        tokens = [t for t in doc if not t.is_space]\n",
        "\n",
        "        if len(tokens) == 1:\n",
        "            if is_valid_unigram_token(tokens[0]):\n",
        "                valid_terms.append(term)\n",
        "        else:\n",
        "            if is_valid_ngram_span(tokens):\n",
        "                valid_terms.append(term)\n",
        "\n",
        "    return pd.Series(valid_terms)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "id": "60022de7",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original gold count: 3423\n",
            "Filtered gold count: 1487\n"
          ]
        }
      ],
      "source": [
        "gold_terms_clean = normalize_gold_terms(train_df[\"term\"])\n",
        "gold_counts = gold_terms_clean.value_counts()\n",
        "\n",
        "print(\"Original gold count:\", len(train_df[\"term\"]))\n",
        "print(\"Filtered gold count:\", len(gold_terms_clean))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f6cb4f3",
      "metadata": {},
      "source": [
        "#### Building the precision-prior table\n",
        "\n",
        "`prior_df` merges:\n",
        "- `tf`: how often each n-gram appears in the whole corpus\n",
        "- `gold`: how often it appears as a gold term\n",
        "- `prec_prior`: an empirical estimate of the n-gram's \"term-likeness\"\n",
        "\n",
        "### Interpretation:\n",
        "- A high `prec_prior` means:\n",
        "  - the n-gram appears frequently **as a gold term**\n",
        "  - relative to how often it appears overall\n",
        "- A low `prec_prior` means:\n",
        "  - the n-gram is frequent but **rarely annotated** → likely NOT a term\n",
        "\n",
        "This acts as a **weakly supervised precision signal**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "id": "98d07cef",
      "metadata": {},
      "outputs": [],
      "source": [
        "prior_df = pd.DataFrame({\n",
        "    \"tf\": tf_series_pos,\n",
        "    \"gold\": gold_counts\n",
        "}).fillna(0)\n",
        "\n",
        "prior_df[\"prec_prior\"] = prior_df[\"gold\"] / prior_df[\"tf\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8f3732ef",
      "metadata": {},
      "source": [
        "#### Building the precision-aware vocabulary (statistical memory)\n",
        "\n",
        "This function combines:\n",
        "- **TF-IDF score** (importance in corpus)\n",
        "- **Precision-prior** (how often it's a gold term)\n",
        "to filter n-grams.\n",
        "\n",
        "We keep only the n-grams such as:\n",
        "- \"rifiuti urbani\"\n",
        "- \"centro di raccolta\"\n",
        "- \"plastica e metalli\"\n",
        "\n",
        "and discard irrelevant ones:\n",
        "- \"sindaco del comune\"\n",
        "- \"1° aprile\"\n",
        "- \"componenti essenziali\"\n",
        "\n",
        "Output:\n",
        "- `memory_by_len[L]` = set of valid n-grams of length L\n",
        "- `max_ngram_len_hp` = maximum length of any stored n-gram\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b79fcf0-e21a-4242-a17e-63ac1d7e511d",
      "metadata": {
        "id": "7b79fcf0-e21a-4242-a17e-63ac1d7e511d"
      },
      "outputs": [],
      "source": [
        "prec_threshold = 0.2  # broader 0.1\n",
        "\n",
        "def build_memory_with_prec_filter(tfidf_series, prior_df, tfidf_threshold, prec_threshold):\n",
        "    # join tfidf with prior\n",
        "    df = pd.DataFrame({\"tfidf\": tfidf_series}).join(prior_df[[\"prec_prior\"]], how=\"left\")\n",
        "    df[\"prec_prior\"] = df[\"prec_prior\"].fillna(0.0)\n",
        "\n",
        "    # keep only n-grams that pass both thresholds\n",
        "    selected = df[(df[\"tfidf\"] >= tfidf_threshold) & (df[\"prec_prior\"] >= prec_threshold)]\n",
        "\n",
        "    memory_by_len = {}\n",
        "    for phrase in selected.index:\n",
        "        tokens = phrase.split(\" \")\n",
        "        L = len(tokens)\n",
        "        if L not in memory_by_len:\n",
        "            memory_by_len[L] = set()\n",
        "        memory_by_len[L].add(tuple(tokens))\n",
        "\n",
        "    max_len = max(memory_by_len.keys()) if memory_by_len else 0\n",
        "    return memory_by_len, max_len\n",
        "\n",
        "tfidf_threshold = 20  #broad 2\n",
        "memory_by_len_hp, max_ngram_len_hp = build_memory_with_prec_filter(\n",
        "    tfidf_supervised,  # your gold-boosted tfidf\n",
        "    prior_df,\n",
        "    tfidf_threshold,\n",
        "    prec_threshold,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7008a97d",
      "metadata": {},
      "source": [
        "#### Extracting candidate terms from DEV using the precision-prior memory\n",
        "\n",
        "For each sentence:\n",
        "1. It is tokenized.\n",
        "2. We scan through all tokens and attempt to match:\n",
        "   - longest possible n-grams first,\n",
        "   - checking membership in `memory_by_len_hp`\n",
        "\n",
        "This gives a *precision-filtered statistical baseline*.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "id": "4a2bd632-f4da-45b5-91e6-3e7e62411a56",
      "metadata": {
        "id": "4a2bd632-f4da-45b5-91e6-3e7e62411a56",
        "outputId": "48683acd-079e-4e96-a5af-710e273f5093"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of predicted term occurrences (precision-prior model): 345\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>document_id</th>\n",
              "      <th>paragraph_id</th>\n",
              "      <th>sentence_id</th>\n",
              "      <th>sentence_text</th>\n",
              "      <th>term</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>doc_caserta_06</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>Il presente disciplinare per la gestione dei centri di raccolta comunali è stato redatto ai sensi e per effetto del DM 13/05/2009, pubblicato sulla G.U. n. 165 del 18/07/2009, con il quale sono st...</td>\n",
              "      <td>centri di raccolta</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>doc_caserta_06</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>Il presente disciplinare per la gestione dei centri di raccolta comunali è stato redatto ai sensi e per effetto del DM 13/05/2009, pubblicato sulla G.U. n. 165 del 18/07/2009, con il quale sono st...</td>\n",
              "      <td>centri di raccolta</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>doc_salerno_05</td>\n",
              "      <td>24</td>\n",
              "      <td>17</td>\n",
              "      <td>Triciclo CCR/Servizio Ingombranti</td>\n",
              "      <td>ccr</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>doc_salerno_05</td>\n",
              "      <td>24</td>\n",
              "      <td>17</td>\n",
              "      <td>Triciclo CCR/Servizio Ingombranti</td>\n",
              "      <td>servizio ingombranti</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>doc_salerno_05</td>\n",
              "      <td>24</td>\n",
              "      <td>17</td>\n",
              "      <td>Triciclo CCR/Servizio Ingombranti</td>\n",
              "      <td>ccr</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>doc_salerno_05</td>\n",
              "      <td>24</td>\n",
              "      <td>17</td>\n",
              "      <td>Triciclo CCR/Servizio Ingombranti</td>\n",
              "      <td>servizio ingombranti</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>doc_caserta_06</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>- alla vigilanza nel rispetto delle norme del C.S.A. e sulla corretta gestione del centro di raccolta;</td>\n",
              "      <td>centro di raccolta</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>doc_praiano_05</td>\n",
              "      <td>15</td>\n",
              "      <td>1</td>\n",
              "      <td>Lunedì; Rifiuti Organici</td>\n",
              "      <td>rifiuti organici</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>doc_caserta_06</td>\n",
              "      <td>9</td>\n",
              "      <td>2</td>\n",
              "      <td>Qualora l'utente fosse impossibilitato per forza maggiore ad effettuare il conferimento del rifiuto, può richiedere l'intervento dell'operatore ecologico presente.</td>\n",
              "      <td>rifiuto</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>doc_caserta_06</td>\n",
              "      <td>9</td>\n",
              "      <td>2</td>\n",
              "      <td>Qualora l'utente fosse impossibilitato per forza maggiore ad effettuare il conferimento del rifiuto, può richiedere l'intervento dell'operatore ecologico presente.</td>\n",
              "      <td>rifiuto</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      document_id  paragraph_id  sentence_id  \\\n",
              "0  doc_caserta_06             3            1   \n",
              "1  doc_caserta_06             3            1   \n",
              "2  doc_salerno_05            24           17   \n",
              "3  doc_salerno_05            24           17   \n",
              "4  doc_salerno_05            24           17   \n",
              "5  doc_salerno_05            24           17   \n",
              "6  doc_caserta_06             6            2   \n",
              "7  doc_praiano_05            15            1   \n",
              "8  doc_caserta_06             9            2   \n",
              "9  doc_caserta_06             9            2   \n",
              "\n",
              "                                                                                                                                                                                             sentence_text  \\\n",
              "0  Il presente disciplinare per la gestione dei centri di raccolta comunali è stato redatto ai sensi e per effetto del DM 13/05/2009, pubblicato sulla G.U. n. 165 del 18/07/2009, con il quale sono st...   \n",
              "1  Il presente disciplinare per la gestione dei centri di raccolta comunali è stato redatto ai sensi e per effetto del DM 13/05/2009, pubblicato sulla G.U. n. 165 del 18/07/2009, con il quale sono st...   \n",
              "2                                                                                                                                                                        Triciclo CCR/Servizio Ingombranti   \n",
              "3                                                                                                                                                                        Triciclo CCR/Servizio Ingombranti   \n",
              "4                                                                                                                                                                        Triciclo CCR/Servizio Ingombranti   \n",
              "5                                                                                                                                                                        Triciclo CCR/Servizio Ingombranti   \n",
              "6                                                                                                   - alla vigilanza nel rispetto delle norme del C.S.A. e sulla corretta gestione del centro di raccolta;   \n",
              "7                                                                                                                                                                                 Lunedì; Rifiuti Organici   \n",
              "8                                      Qualora l'utente fosse impossibilitato per forza maggiore ad effettuare il conferimento del rifiuto, può richiedere l'intervento dell'operatore ecologico presente.   \n",
              "9                                      Qualora l'utente fosse impossibilitato per forza maggiore ad effettuare il conferimento del rifiuto, può richiedere l'intervento dell'operatore ecologico presente.   \n",
              "\n",
              "                   term  \n",
              "0    centri di raccolta  \n",
              "1    centri di raccolta  \n",
              "2                   ccr  \n",
              "3  servizio ingombranti  \n",
              "4                   ccr  \n",
              "5  servizio ingombranti  \n",
              "6    centro di raccolta  \n",
              "7      rifiuti organici  \n",
              "8               rifiuto  \n",
              "9               rifiuto  "
            ]
          },
          "execution_count": 61,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rows = []\n",
        "\n",
        "for _, row in dev_df.iterrows():\n",
        "    doc_id    = row[\"document_id\"]\n",
        "    par_id    = row[\"paragraph_id\"]\n",
        "    sent_id   = row[\"sentence_id\"]\n",
        "    sent_text = row[\"sentence_text\"]\n",
        "\n",
        "    preds = extract_terms_from_sentence_ngram(\n",
        "        sent_text,\n",
        "        memory_by_len_hp,\n",
        "        max_ngram_len_hp\n",
        "    )\n",
        "\n",
        "    for term in preds:\n",
        "        rows.append({\n",
        "            \"document_id\": doc_id,\n",
        "            \"paragraph_id\": par_id,\n",
        "            \"sentence_id\": sent_id,\n",
        "            \"sentence_text\": sent_text,\n",
        "            \"term\": term\n",
        "        })\n",
        "\n",
        "dev_pred_hp = pd.DataFrame(rows)\n",
        "print(\"Number of predicted term occurrences (precision-prior model):\", len(dev_pred_hp))\n",
        "dev_pred_hp.head(10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "id": "13642286-be9b-4e9b-8154-7ce0593b13f5",
      "metadata": {
        "id": "13642286-be9b-4e9b-8154-7ce0593b13f5",
        "outputId": "cf996e65-05da-4127-c157-0328ee6c2dab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== PRECISION-PRIOR FILTERED TF-IDF MODEL ===\n",
            "tfidf_threshold : 20\n",
            "prec_threshold  : 0.2\n",
            "Micro P/R/F     : 0.8276 / 0.1540 / 0.2597\n",
            "Type  P/R/F     : 0.9375 / 0.1860 / 0.3103\n"
          ]
        }
      ],
      "source": [
        "p_micro, r_micro, f1_micro = micro_f1(dev_df, dev_pred_hp)\n",
        "p_type,  r_type,  f1_type  = type_f1(dev_df, dev_pred_hp)\n",
        "\n",
        "print(\"=== PRECISION-PRIOR FILTERED TF-IDF MODEL ===\")\n",
        "print(f\"tfidf_threshold : {tfidf_threshold}\")\n",
        "print(f\"prec_threshold  : {prec_threshold}\")\n",
        "print(f\"Micro P/R/F     : {p_micro:.4f} / {r_micro:.4f} / {f1_micro:.4f}\")\n",
        "print(f\"Type  P/R/F     : {p_type:.4f} / {r_type:.4f} / {f1_type:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "id": "0a8c2955",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Saved predictions to ../predictions/subtask_a_statistics_precision_strong.json\n"
          ]
        }
      ],
      "source": [
        "output_path = \"../predictions/subtask_a_statistics_precision_strong.json\"\n",
        "\n",
        "save_predictions_stats(\n",
        "    pred_df=dev_pred_hp,\n",
        "    dev_df=dev_df,\n",
        "    output_path=output_path,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0bff8dc9",
      "metadata": {},
      "source": [
        "## Grid Search Hyperparameter Tuning\n",
        "\n",
        "We sweep:\n",
        "- several **TF-IDF thresholds**\n",
        "- several **precision-prior thresholds**\n",
        "\n",
        "For each combination:\n",
        "- build memory\n",
        "- extract predictions\n",
        "- compute metrics\n",
        "- store results\n",
        "\n",
        "This allows us to find the best trade-off between:\n",
        "- precision\n",
        "- recall\n",
        "- vocabulary size\n",
        "- number of predictions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "73307055-c95a-4905-a7d4-6ad9d67e820a",
      "metadata": {
        "id": "73307055-c95a-4905-a7d4-6ad9d67e820a",
        "outputId": "2f8daf78-88c9-4e12-8010-4e0be55f0e74"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tfidf_thr</th>\n",
              "      <th>prec_thr</th>\n",
              "      <th>micro_p</th>\n",
              "      <th>micro_r</th>\n",
              "      <th>micro_f1</th>\n",
              "      <th>type_p</th>\n",
              "      <th>type_r</th>\n",
              "      <th>type_f1</th>\n",
              "      <th>num_terms_in_memory</th>\n",
              "      <th>num_preds</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.092</td>\n",
              "      <td>0.294</td>\n",
              "      <td>0.141</td>\n",
              "      <td>0.076</td>\n",
              "      <td>0.446</td>\n",
              "      <td>0.130</td>\n",
              "      <td>9813</td>\n",
              "      <td>4045</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2.0</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.684</td>\n",
              "      <td>0.208</td>\n",
              "      <td>0.319</td>\n",
              "      <td>0.714</td>\n",
              "      <td>0.289</td>\n",
              "      <td>0.412</td>\n",
              "      <td>269</td>\n",
              "      <td>548</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2.0</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.800</td>\n",
              "      <td>0.169</td>\n",
              "      <td>0.280</td>\n",
              "      <td>0.821</td>\n",
              "      <td>0.227</td>\n",
              "      <td>0.356</td>\n",
              "      <td>212</td>\n",
              "      <td>386</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2.0</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.857</td>\n",
              "      <td>0.092</td>\n",
              "      <td>0.167</td>\n",
              "      <td>0.842</td>\n",
              "      <td>0.132</td>\n",
              "      <td>0.229</td>\n",
              "      <td>140</td>\n",
              "      <td>196</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.105</td>\n",
              "      <td>0.303</td>\n",
              "      <td>0.156</td>\n",
              "      <td>0.091</td>\n",
              "      <td>0.446</td>\n",
              "      <td>0.151</td>\n",
              "      <td>5249</td>\n",
              "      <td>3716</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>5.0</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.685</td>\n",
              "      <td>0.207</td>\n",
              "      <td>0.318</td>\n",
              "      <td>0.719</td>\n",
              "      <td>0.285</td>\n",
              "      <td>0.408</td>\n",
              "      <td>242</td>\n",
              "      <td>539</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>5.0</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.804</td>\n",
              "      <td>0.168</td>\n",
              "      <td>0.278</td>\n",
              "      <td>0.831</td>\n",
              "      <td>0.223</td>\n",
              "      <td>0.352</td>\n",
              "      <td>185</td>\n",
              "      <td>377</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>5.0</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.866</td>\n",
              "      <td>0.091</td>\n",
              "      <td>0.165</td>\n",
              "      <td>0.861</td>\n",
              "      <td>0.128</td>\n",
              "      <td>0.223</td>\n",
              "      <td>113</td>\n",
              "      <td>187</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>10.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.113</td>\n",
              "      <td>0.308</td>\n",
              "      <td>0.166</td>\n",
              "      <td>0.100</td>\n",
              "      <td>0.426</td>\n",
              "      <td>0.161</td>\n",
              "      <td>3547</td>\n",
              "      <td>3503</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>10.0</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.687</td>\n",
              "      <td>0.203</td>\n",
              "      <td>0.313</td>\n",
              "      <td>0.733</td>\n",
              "      <td>0.273</td>\n",
              "      <td>0.398</td>\n",
              "      <td>203</td>\n",
              "      <td>530</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>10.0</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.810</td>\n",
              "      <td>0.164</td>\n",
              "      <td>0.273</td>\n",
              "      <td>0.864</td>\n",
              "      <td>0.211</td>\n",
              "      <td>0.339</td>\n",
              "      <td>146</td>\n",
              "      <td>368</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>10.0</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.895</td>\n",
              "      <td>0.087</td>\n",
              "      <td>0.159</td>\n",
              "      <td>0.933</td>\n",
              "      <td>0.116</td>\n",
              "      <td>0.206</td>\n",
              "      <td>74</td>\n",
              "      <td>177</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>20.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.124</td>\n",
              "      <td>0.303</td>\n",
              "      <td>0.176</td>\n",
              "      <td>0.117</td>\n",
              "      <td>0.393</td>\n",
              "      <td>0.180</td>\n",
              "      <td>1853</td>\n",
              "      <td>3171</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>20.0</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.685</td>\n",
              "      <td>0.193</td>\n",
              "      <td>0.301</td>\n",
              "      <td>0.759</td>\n",
              "      <td>0.248</td>\n",
              "      <td>0.374</td>\n",
              "      <td>151</td>\n",
              "      <td>514</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>20.0</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.828</td>\n",
              "      <td>0.154</td>\n",
              "      <td>0.260</td>\n",
              "      <td>0.938</td>\n",
              "      <td>0.186</td>\n",
              "      <td>0.310</td>\n",
              "      <td>94</td>\n",
              "      <td>345</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>20.0</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.913</td>\n",
              "      <td>0.081</td>\n",
              "      <td>0.149</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.103</td>\n",
              "      <td>0.187</td>\n",
              "      <td>43</td>\n",
              "      <td>166</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>30.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.144</td>\n",
              "      <td>0.313</td>\n",
              "      <td>0.198</td>\n",
              "      <td>0.153</td>\n",
              "      <td>0.376</td>\n",
              "      <td>0.217</td>\n",
              "      <td>1040</td>\n",
              "      <td>2884</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>30.0</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.705</td>\n",
              "      <td>0.190</td>\n",
              "      <td>0.299</td>\n",
              "      <td>0.806</td>\n",
              "      <td>0.240</td>\n",
              "      <td>0.369</td>\n",
              "      <td>110</td>\n",
              "      <td>504</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>30.0</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.844</td>\n",
              "      <td>0.153</td>\n",
              "      <td>0.259</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.182</td>\n",
              "      <td>0.308</td>\n",
              "      <td>66</td>\n",
              "      <td>337</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>30.0</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.912</td>\n",
              "      <td>0.080</td>\n",
              "      <td>0.146</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.099</td>\n",
              "      <td>0.180</td>\n",
              "      <td>32</td>\n",
              "      <td>163</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>40.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.159</td>\n",
              "      <td>0.300</td>\n",
              "      <td>0.208</td>\n",
              "      <td>0.180</td>\n",
              "      <td>0.343</td>\n",
              "      <td>0.236</td>\n",
              "      <td>707</td>\n",
              "      <td>2550</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>40.0</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.693</td>\n",
              "      <td>0.177</td>\n",
              "      <td>0.282</td>\n",
              "      <td>0.810</td>\n",
              "      <td>0.211</td>\n",
              "      <td>0.334</td>\n",
              "      <td>83</td>\n",
              "      <td>467</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>40.0</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.833</td>\n",
              "      <td>0.141</td>\n",
              "      <td>0.241</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.157</td>\n",
              "      <td>0.271</td>\n",
              "      <td>49</td>\n",
              "      <td>320</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>40.0</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.903</td>\n",
              "      <td>0.072</td>\n",
              "      <td>0.133</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.083</td>\n",
              "      <td>0.153</td>\n",
              "      <td>28</td>\n",
              "      <td>152</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    tfidf_thr  prec_thr  micro_p  micro_r  micro_f1  type_p  type_r  type_f1  \\\n",
              "0         2.0       0.0    0.092    0.294     0.141   0.076   0.446    0.130   \n",
              "1         2.0       0.1    0.684    0.208     0.319   0.714   0.289    0.412   \n",
              "2         2.0       0.2    0.800    0.169     0.280   0.821   0.227    0.356   \n",
              "3         2.0       0.3    0.857    0.092     0.167   0.842   0.132    0.229   \n",
              "4         5.0       0.0    0.105    0.303     0.156   0.091   0.446    0.151   \n",
              "5         5.0       0.1    0.685    0.207     0.318   0.719   0.285    0.408   \n",
              "6         5.0       0.2    0.804    0.168     0.278   0.831   0.223    0.352   \n",
              "7         5.0       0.3    0.866    0.091     0.165   0.861   0.128    0.223   \n",
              "8        10.0       0.0    0.113    0.308     0.166   0.100   0.426    0.161   \n",
              "9        10.0       0.1    0.687    0.203     0.313   0.733   0.273    0.398   \n",
              "10       10.0       0.2    0.810    0.164     0.273   0.864   0.211    0.339   \n",
              "11       10.0       0.3    0.895    0.087     0.159   0.933   0.116    0.206   \n",
              "12       20.0       0.0    0.124    0.303     0.176   0.117   0.393    0.180   \n",
              "13       20.0       0.1    0.685    0.193     0.301   0.759   0.248    0.374   \n",
              "14       20.0       0.2    0.828    0.154     0.260   0.938   0.186    0.310   \n",
              "15       20.0       0.3    0.913    0.081     0.149   1.000   0.103    0.187   \n",
              "16       30.0       0.0    0.144    0.313     0.198   0.153   0.376    0.217   \n",
              "17       30.0       0.1    0.705    0.190     0.299   0.806   0.240    0.369   \n",
              "18       30.0       0.2    0.844    0.153     0.259   1.000   0.182    0.308   \n",
              "19       30.0       0.3    0.912    0.080     0.146   1.000   0.099    0.180   \n",
              "20       40.0       0.0    0.159    0.300     0.208   0.180   0.343    0.236   \n",
              "21       40.0       0.1    0.693    0.177     0.282   0.810   0.211    0.334   \n",
              "22       40.0       0.2    0.833    0.141     0.241   1.000   0.157    0.271   \n",
              "23       40.0       0.3    0.903    0.072     0.133   1.000   0.083    0.153   \n",
              "\n",
              "    num_terms_in_memory  num_preds  \n",
              "0                  9813       4045  \n",
              "1                   269        548  \n",
              "2                   212        386  \n",
              "3                   140        196  \n",
              "4                  5249       3716  \n",
              "5                   242        539  \n",
              "6                   185        377  \n",
              "7                   113        187  \n",
              "8                  3547       3503  \n",
              "9                   203        530  \n",
              "10                  146        368  \n",
              "11                   74        177  \n",
              "12                 1853       3171  \n",
              "13                  151        514  \n",
              "14                   94        345  \n",
              "15                   43        166  \n",
              "16                 1040       2884  \n",
              "17                  110        504  \n",
              "18                   66        337  \n",
              "19                   32        163  \n",
              "20                  707       2550  \n",
              "21                   83        467  \n",
              "22                   49        320  \n",
              "23                   28        152  "
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# define grids – adjust as you like\n",
        "tfidf_thresholds = [2.0, 5.0, 10.0, 20.0, 30.0, 40.0]\n",
        "prec_thresholds  = [0.0, 0.1, 0.2, 0.3]\n",
        "\n",
        "results = []\n",
        "\n",
        "for tfidf_thr in tfidf_thresholds:\n",
        "    for prec_thr in prec_thresholds:\n",
        "        # 1) Build memory with both thresholds\n",
        "        memory_by_len_hp, max_ngram_len_hp = build_memory_with_prec_filter(\n",
        "            tfidf_supervised,\n",
        "            prior_df,\n",
        "            tfidf_threshold=tfidf_thr,\n",
        "            prec_threshold=prec_thr,\n",
        "        )\n",
        "\n",
        "        # if memory is empty, skip\n",
        "        if max_ngram_len_hp == 0:\n",
        "            results.append({\n",
        "                \"tfidf_thr\": tfidf_thr,\n",
        "                \"prec_thr\": prec_thr,\n",
        "                \"micro_p\": 0.0,\n",
        "                \"micro_r\": 0.0,\n",
        "                \"micro_f1\": 0.0,\n",
        "                \"type_p\": 0.0,\n",
        "                \"type_r\": 0.0,\n",
        "                \"type_f1\": 0.0,\n",
        "                \"num_terms_in_memory\": 0,\n",
        "                \"num_preds\": 0,\n",
        "            })\n",
        "            continue\n",
        "\n",
        "        # 2) Predict on dev\n",
        "        rows = []\n",
        "        for _, row in dev_df.iterrows():\n",
        "            doc_id    = row[\"document_id\"]\n",
        "            par_id    = row[\"paragraph_id\"]\n",
        "            sent_id   = row[\"sentence_id\"]\n",
        "            sent_text = row[\"sentence_text\"]\n",
        "\n",
        "            preds = extract_terms_from_sentence_ngram(\n",
        "                sent_text,\n",
        "                memory_by_len_hp,\n",
        "                max_ngram_len_hp\n",
        "            )\n",
        "\n",
        "            for term in preds:\n",
        "                rows.append({\n",
        "                    \"document_id\": doc_id,\n",
        "                    \"paragraph_id\": par_id,\n",
        "                    \"sentence_id\": sent_id,\n",
        "                    \"sentence_text\": sent_text,\n",
        "                    \"term\": term\n",
        "                })\n",
        "\n",
        "        dev_pred_hp = pd.DataFrame(rows)\n",
        "\n",
        "        # 3) Evaluate\n",
        "        p_micro, r_micro, f1_micro = micro_f1(dev_df, dev_pred_hp)\n",
        "        p_type,  r_type,  f1_type  = type_f1(dev_df, dev_pred_hp)\n",
        "\n",
        "        # 4) Store results\n",
        "        num_terms_in_memory = sum(len(s) for s in memory_by_len_hp.values())\n",
        "        num_preds = len(dev_pred_hp)\n",
        "\n",
        "        results.append({\n",
        "            \"tfidf_thr\": tfidf_thr,\n",
        "            \"prec_thr\": prec_thr,\n",
        "            \"micro_p\": p_micro,\n",
        "            \"micro_r\": r_micro,\n",
        "            \"micro_f1\": f1_micro,\n",
        "            \"type_p\": p_type,\n",
        "            \"type_r\": r_type,\n",
        "            \"type_f1\": f1_type,\n",
        "            \"num_terms_in_memory\": num_terms_in_memory,\n",
        "            \"num_preds\": num_preds,\n",
        "        })\n",
        "\n",
        "# Collect into a DataFrame\n",
        "grid_df = pd.DataFrame(results)\n",
        "\n",
        "# Round for readability\n",
        "for col in [\"micro_p\", \"micro_r\", \"micro_f1\", \"type_p\", \"type_r\", \"type_f1\"]:\n",
        "    grid_df[col] = grid_df[col].apply(lambda x: float(f\"{x:.3f}\"))\n",
        "\n",
        "grid_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "29e53b3c",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tfidf_thr</th>\n",
              "      <th>prec_thr</th>\n",
              "      <th>micro_p</th>\n",
              "      <th>micro_r</th>\n",
              "      <th>micro_f1</th>\n",
              "      <th>type_p</th>\n",
              "      <th>type_r</th>\n",
              "      <th>type_f1</th>\n",
              "      <th>num_terms_in_memory</th>\n",
              "      <th>num_preds</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2.0</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.684</td>\n",
              "      <td>0.208</td>\n",
              "      <td>0.319</td>\n",
              "      <td>0.714</td>\n",
              "      <td>0.289</td>\n",
              "      <td>0.412</td>\n",
              "      <td>269</td>\n",
              "      <td>548</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>5.0</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.685</td>\n",
              "      <td>0.207</td>\n",
              "      <td>0.318</td>\n",
              "      <td>0.719</td>\n",
              "      <td>0.285</td>\n",
              "      <td>0.408</td>\n",
              "      <td>242</td>\n",
              "      <td>539</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>10.0</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.687</td>\n",
              "      <td>0.203</td>\n",
              "      <td>0.313</td>\n",
              "      <td>0.733</td>\n",
              "      <td>0.273</td>\n",
              "      <td>0.398</td>\n",
              "      <td>203</td>\n",
              "      <td>530</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>20.0</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.685</td>\n",
              "      <td>0.193</td>\n",
              "      <td>0.301</td>\n",
              "      <td>0.759</td>\n",
              "      <td>0.248</td>\n",
              "      <td>0.374</td>\n",
              "      <td>151</td>\n",
              "      <td>514</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>30.0</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.705</td>\n",
              "      <td>0.190</td>\n",
              "      <td>0.299</td>\n",
              "      <td>0.806</td>\n",
              "      <td>0.240</td>\n",
              "      <td>0.369</td>\n",
              "      <td>110</td>\n",
              "      <td>504</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>40.0</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.693</td>\n",
              "      <td>0.177</td>\n",
              "      <td>0.282</td>\n",
              "      <td>0.810</td>\n",
              "      <td>0.211</td>\n",
              "      <td>0.334</td>\n",
              "      <td>83</td>\n",
              "      <td>467</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2.0</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.800</td>\n",
              "      <td>0.169</td>\n",
              "      <td>0.280</td>\n",
              "      <td>0.821</td>\n",
              "      <td>0.227</td>\n",
              "      <td>0.356</td>\n",
              "      <td>212</td>\n",
              "      <td>386</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>5.0</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.804</td>\n",
              "      <td>0.168</td>\n",
              "      <td>0.278</td>\n",
              "      <td>0.831</td>\n",
              "      <td>0.223</td>\n",
              "      <td>0.352</td>\n",
              "      <td>185</td>\n",
              "      <td>377</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>10.0</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.810</td>\n",
              "      <td>0.164</td>\n",
              "      <td>0.273</td>\n",
              "      <td>0.864</td>\n",
              "      <td>0.211</td>\n",
              "      <td>0.339</td>\n",
              "      <td>146</td>\n",
              "      <td>368</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>20.0</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.828</td>\n",
              "      <td>0.154</td>\n",
              "      <td>0.260</td>\n",
              "      <td>0.938</td>\n",
              "      <td>0.186</td>\n",
              "      <td>0.310</td>\n",
              "      <td>94</td>\n",
              "      <td>345</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    tfidf_thr  prec_thr  micro_p  micro_r  micro_f1  type_p  type_r  type_f1  \\\n",
              "1         2.0       0.1    0.684    0.208     0.319   0.714   0.289    0.412   \n",
              "5         5.0       0.1    0.685    0.207     0.318   0.719   0.285    0.408   \n",
              "9        10.0       0.1    0.687    0.203     0.313   0.733   0.273    0.398   \n",
              "13       20.0       0.1    0.685    0.193     0.301   0.759   0.248    0.374   \n",
              "17       30.0       0.1    0.705    0.190     0.299   0.806   0.240    0.369   \n",
              "21       40.0       0.1    0.693    0.177     0.282   0.810   0.211    0.334   \n",
              "2         2.0       0.2    0.800    0.169     0.280   0.821   0.227    0.356   \n",
              "6         5.0       0.2    0.804    0.168     0.278   0.831   0.223    0.352   \n",
              "10       10.0       0.2    0.810    0.164     0.273   0.864   0.211    0.339   \n",
              "14       20.0       0.2    0.828    0.154     0.260   0.938   0.186    0.310   \n",
              "\n",
              "    num_terms_in_memory  num_preds  \n",
              "1                   269        548  \n",
              "5                   242        539  \n",
              "9                   203        530  \n",
              "13                  151        514  \n",
              "17                  110        504  \n",
              "21                   83        467  \n",
              "2                   212        386  \n",
              "6                   185        377  \n",
              "10                  146        368  \n",
              "14                   94        345  "
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Sort by micro F1 (descending)\n",
        "grid_df.sort_values(\"micro_f1\", ascending=False).head(10)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
