{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52af72d2",
   "metadata": {},
   "source": [
    "# **Term Extraction Ensemble (BERT + spaCy + Dictionary)**\n",
    "\n",
    "This notebook implements a complete post-processing pipeline for the ATE-IT Subtask A (Automatic Term Extraction).  \n",
    "It takes the raw predictions from a fine-tuned **BERT token classification model** and combines them with **spaCy noun-chunk spans** and a **gold-derived domain vocabulary** to produce a higher-quality list of domain terms for each sentence.\n",
    "\n",
    "### Pipeline Summary\n",
    "1. **Load BERT and spaCy predictions**  \n",
    "   - Import model outputs in ATE-IT JSON format.  \n",
    "   - Map predictions to sentence identifiers for easy lookup.\n",
    "\n",
    "2. **Normalize and clean BERT terms**  \n",
    "   - Remove punctuation, unify quotes, lowercase, collapse whitespace.  \n",
    "   - Filter out spurious or generic one-word candidates.\n",
    "\n",
    "3. **Build a domain vocabulary from the gold training set**  \n",
    "   - Normalize gold terms.  \n",
    "   - Track frequencies to identify strong (repeated) vs. weak (rare) terms.\n",
    "\n",
    "4. **Merge BERT + spaCy + Dictionary knowledge**  \n",
    "   - **Upgrade** short BERT terms to longer spaCy spans when they form a valid multi-word expression present in the gold vocabulary.  \n",
    "   - **Add** additional spaCy multi-word spans only if they appear in the gold vocabulary.  \n",
    "   - **Filter out** generic, meaningless, or uninformative unigrams.  \n",
    "   - **Normalize and deduplicate** final terms.\n",
    "\n",
    "5. **Generate final ensemble predictions**  \n",
    "   - For each sentence, produce an improved term list combining all signals.  \n",
    "   - Output saved in ATE-IT JSON format.\n",
    "\n",
    "### Goal\n",
    "The notebook improves recall and precision of automatic term extraction by combining:\n",
    "- contextual predictions (BERT),\n",
    "- linguistic structure (spaCy),\n",
    "- and domain consistency (gold vocabulary).\n",
    "\n",
    "This hybrid ensemble typically outperforms each component alone.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919c9b83",
   "metadata": {},
   "source": [
    "#### import and file paths\n",
    "We load:\n",
    "- the **train** file to extract the gold vocabulary,\n",
    "- the **dev** file (gold) for evaluation and text,\n",
    "- BERT and spaCy predictions on the dev set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "35713571",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, os\n",
    "def load_json(path: str):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "    \n",
    "\n",
    "def save_json(obj, path: str):\n",
    "    os.makedirs(os.path.dirname(path) or \".\", exist_ok=True)\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(obj, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"✓ Saved cleaned predictions to {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc351393",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"../../data/\"\n",
    "PRED_DIR = \"../../src/final_train_dev_training/predictions/\"\n",
    "\n",
    "TRAIN_FILE = os.path.join(DATA_DIR, \"subtask_a_train.json\")\n",
    "DEV_FILE = os.path.join(DATA_DIR, \"subtask_a_dev.json\")\n",
    "TEST_FILE  = os.path.join(DATA_DIR, \"test.json\")\n",
    "\n",
    "BERT_TEST_PRED_FILE = os.path.join(\n",
    "    PRED_DIR, \"subtask_a_test_bert_preds_train_dev_cleaned.json\"\n",
    ")\n",
    "SPACY_TEST_PRED_FILE = os.path.join( \n",
    "    PRED_DIR, \"subtask_a_test_spacy_trained_preds_train_dev.json\" \n",
    ")\n",
    "\n",
    "ENSEMBLE_OUT_FILE = os.path.join(\n",
    "    PRED_DIR, \"subtask_a_test_ensemble_bert_spacy_dictfilter.json\"\n",
    ")\n",
    "\n",
    "os.makedirs(PRED_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5bbaf257",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = load_json(TRAIN_FILE)\n",
    "dev_data   = load_json(DEV_FILE)\n",
    "\n",
    "# unisci i due insiemi\n",
    "full_train_dev = {\n",
    "    \"data\": train_data[\"data\"] + dev_data[\"data\"]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114d1230",
   "metadata": {},
   "source": [
    "### 1. Normalization and training vocabulary\n",
    "\n",
    "We first define a canonical normalization function `norm()` and build:\n",
    "- a **normalized vocabulary** of gold terms from the training set,\n",
    "- a helper map from prediction JSONs to `(doc, par, sent) → term_list`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fb6b47ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "\n",
    "def norm(t: str) -> str:\n",
    "    if not t:\n",
    "        return \"\"\n",
    "    t = t.lower()\n",
    "    t = unicodedata.normalize(\"NFKC\", t)\n",
    "    t = t.replace(\"’\", \"'\").replace(\"`\", \"'\")\n",
    "    t = t.replace(\"“\", '\"').replace(\"”\", '\"')\n",
    "    t = \" \".join(t.split())\n",
    "    # strip punteggiatura ai bordi\n",
    "    t = t.strip(\".,;:-'\\\"()[]{}\")\n",
    "    return t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e070cd7",
   "metadata": {},
   "source": [
    "## 2. Build a gold-derived vocabulary\n",
    "\n",
    "We now construct a **normalized vocabulary** of domain terms from the gold training set.  \n",
    "This vocabulary is later used to:\n",
    "- validate candidate terms,\n",
    "- decide which spaCy spans are trustworthy,\n",
    "- avoid keeping generic words that never appear as gold terms.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3beb4aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_train_vocab(*datasets) -> set:\n",
    "    vocab = set()\n",
    "    for data in datasets:\n",
    "        rows = data[\"data\"] if isinstance(data, dict) and \"data\" in data else data\n",
    "        for entry in rows:\n",
    "            for term in entry.get(\"term_list\", []):\n",
    "                n = norm(term)\n",
    "                if n:\n",
    "                    vocab.add(n)\n",
    "    return vocab\n",
    "\n",
    "\n",
    "\n",
    "def build_term_map(pred_json: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Build a mapping:\n",
    "        (document_id, paragraph_id, sentence_id) -> list of predicted terms\n",
    "    from a prediction JSON in the ATE-IT format.\n",
    "    \"\"\"\n",
    "    m = {}\n",
    "    for e in pred_json[\"data\"]:\n",
    "        key = (e[\"document_id\"], e[\"paragraph_id\"], e[\"sentence_id\"])\n",
    "        m[key] = e.get(\"term_list\", []) or []\n",
    "    return m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49fcf46",
   "metadata": {},
   "source": [
    "#### 3. Helper: map predictions to sentence IDs\n",
    "\n",
    "We define a helper function that converts a prediction JSON in ATE-IT format into a dictionary:\n",
    "`(document_id, paragraph_id, sentence_id) -> list of predicted terms`.\n",
    "\n",
    "This makes it easy to align BERT and spaCy predictions for the same sentence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4014db25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def build_train_vocab_with_freq(train_data):\n",
    "    freq = Counter()\n",
    "    for e in train_data[\"data\"]:\n",
    "        for term in e.get(\"term_list\", []):\n",
    "            n = norm(term)\n",
    "            if n:\n",
    "                freq[n] += 1\n",
    "    strong = {t for t, c in freq.items() if c >= 3}\n",
    "    weak   = {t for t, c in freq.items() if c == 1}\n",
    "    return freq, strong, weak\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68cac7ae",
   "metadata": {},
   "source": [
    "### 3. Generic heads and acronym heuristics\n",
    "\n",
    "We now define:\n",
    "- a small list of **generic heads** (e.g. *rifiuti, materiali, servizio*),\n",
    "- a heuristic to detect **acronyms** (e.g. *RAEE, R1, TARI*),\n",
    "- a filter for **generic unigrams** that never appear as gold terms.\n",
    "\n",
    "The goal is to:\n",
    "- keep important single-word terms if they are in the gold vocabulary,\n",
    "- discard only very generic heads that never occur as true domain terms.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1090afc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "GENERIC_HEADS = {\n",
    "    \"rifiuti\", \"materiali\", \"utenti\", \"plastica\", \"carta\",\n",
    "    \"residui\", \"tariffe\", \"gestore\", \"servizio\", \"modalità\",\n",
    "    \"conferimento\", \"costi\", \"parte\", \"quota\", \"impianto\"\n",
    "}\n",
    "def looks_like_acronym(n: str) -> bool:\n",
    "    # es: \"tmb\", \"raee\", \"r.a.e.e.\"\n",
    "    n_clean = n.replace(\".\", \"\")\n",
    "    return (len(n_clean) >= 2 and len(n_clean) <= 6 and n_clean.isalpha())\n",
    "\n",
    "def filter_generic_unigrams(terms, train_vocab_norm):\n",
    "    filtered = []\n",
    "    for t in terms:\n",
    "        n = norm(t)\n",
    "        tokens = n.split()\n",
    "        if len(tokens) == 1:\n",
    "            if n in GENERIC_HEADS and n not in train_vocab_norm and not looks_like_acronym(n):\n",
    "                continue\n",
    "            if n in GENERIC_HEADS and n not in train_vocab_norm:\n",
    "                # scarta \"quota\", \"parte\", ecc. se non compaiono mai come termini gold\n",
    "                continue\n",
    "        filtered.append(t)\n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de4ca50",
   "metadata": {},
   "source": [
    "## 4. Multiword upgrade: BERT → spaCy spans\n",
    "\n",
    "BERT sometimes predicts short fragments (e.g. *ferro*) where the gold term is a\n",
    "longer span (e.g. *materiali ferrosi*).\n",
    "\n",
    "We therefore:\n",
    "1. Look for **spaCy multiword spans** that:\n",
    "   - are present in the gold vocabulary,\n",
    "   - contain the BERT term as a contiguous token subsequence.\n",
    "2. If such a span exists, we **upgrade** the BERT term to the longer spaCy span.\n",
    "3. We also maintain a small list of **GENERIC_BAD** terms that we never keep.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "64f161d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "GENERIC_BAD = {\n",
    "    \"parte\", \"gestione\", \"città\", \"territorio\", \"comune\",\n",
    "    \"ore\", \"no\", \"si\", \"anno\", \"mese\", \"giorno\"\n",
    "} \n",
    "def contains_as_subspan(longer: str, shorter: str) -> bool:\n",
    "    long_tokens = longer.split()\n",
    "    short_tokens = shorter.split()\n",
    "    L, S = len(long_tokens), len(short_tokens)\n",
    "    if S > L:\n",
    "        return False\n",
    "    for i in range(L - S + 1):\n",
    "        if long_tokens[i:i+S] == short_tokens:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def upgrade_with_longer_spacy(bert_terms, spacy_terms, train_vocab_norm):\n",
    "    \"\"\"\n",
    "    Upgrade BERT terms to longer spaCy spans ONLY WHEN BENEFICIAL.\n",
    "    \"\"\"\n",
    "    final = []\n",
    "    seen = set()\n",
    "    \n",
    "    spacy_norm_map = {norm(t): t for t in spacy_terms or []}\n",
    "\n",
    "    for b in bert_terms or []:\n",
    "        b_norm = norm(b)\n",
    "        if not b_norm or b_norm in GENERIC_BAD:\n",
    "            continue\n",
    "\n",
    "        best = None\n",
    "\n",
    "        # search longest valid spaCy span containing the BERT term\n",
    "        for s_norm, s in spacy_norm_map.items():\n",
    "            if len(s_norm.split()) < 2:\n",
    "                continue\n",
    "            if s_norm not in train_vocab_norm:\n",
    "                continue\n",
    "            if contains_as_subspan(s_norm, b_norm):\n",
    "                if best is None or len(s_norm.split()) > len(norm(best).split()):\n",
    "                    best = s\n",
    "\n",
    "\n",
    "        chosen = best if best else b\n",
    "        c_norm = norm(chosen)\n",
    "\n",
    "        if c_norm not in seen and c_norm not in GENERIC_BAD:\n",
    "            final.append(chosen)\n",
    "            seen.add(c_norm)\n",
    "\n",
    "    return final\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79f9a59",
   "metadata": {},
   "source": [
    "### 5. BERT + spaCy + vocabulary merge\n",
    "now define the main merge function that combines:\n",
    "- cleaned **BERT terms**,\n",
    "- **spaCy noun-chunk spans**,\n",
    "- and the **gold-derived vocabulary**.\n",
    "\n",
    "The strategy is:\n",
    "1. First, **upgrade** BERT terms to longer spaCy spans when they match a gold term.\n",
    "2. Then, **add extra spaCy multiword spans** that:\n",
    "   - are in the gold vocabulary,\n",
    "   - are not already covered,\n",
    "   - are not clearly generic or meaningless.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "32bb4936",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_bert_spacy_with_dict(bert_terms, spacy_terms, train_vocab_norm):\n",
    "    \"\"\"\n",
    "    BEST ensemble so far:\n",
    "    1. upgrade BERT with spaCy\n",
    "    2. add dictionary-filtered spaCy spans\n",
    "    3. skip generic or meaningless words\n",
    "    \"\"\"\n",
    "    upgraded = upgrade_with_longer_spacy(\n",
    "        bert_terms=bert_terms,\n",
    "        spacy_terms=spacy_terms,\n",
    "        train_vocab_norm=train_vocab_norm,\n",
    "    )\n",
    "\n",
    "    final = upgraded[:]\n",
    "    seen = {norm(t) for t in upgraded}\n",
    "\n",
    "    for s in spacy_terms or []:\n",
    "        s_norm = norm(s)\n",
    "\n",
    "        if len(s.split()) < 2:\n",
    "            continue\n",
    "        if s_norm not in train_vocab_norm:\n",
    "            continue\n",
    "        if s_norm in seen:\n",
    "            continue\n",
    "        if s_norm in GENERIC_BAD:\n",
    "            continue\n",
    "\n",
    "        final.append(s)\n",
    "        seen.add(s_norm)\n",
    "\n",
    "    return final\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36fc5b01",
   "metadata": {},
   "source": [
    "### 6. Per-sentence merge helper\n",
    " helper `merge_sentence()` that:\n",
    "1. Applies the BERT+spaCy+vocabulary merge strategy.\n",
    "2. Removes only **truly generic unigrams** (using the gold vocabulary as a whitelist).\n",
    "3. Normalizes and deduplicates the final term list.\n",
    "\n",
    "This function is called once per sentence in the dev/test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "35d36815",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_sentence(bert_terms, spacy_terms, train_vocab_norm):\n",
    "    merged = merge_bert_spacy_with_dict(\n",
    "        bert_terms=bert_terms,\n",
    "        spacy_terms=spacy_terms,\n",
    "        train_vocab_norm=train_vocab_norm\n",
    "    )\n",
    "    merged = filter_generic_unigrams(merged, train_vocab_norm)\n",
    "\n",
    "    # dedupe and normalize\n",
    "    seen = set()\n",
    "    final = []\n",
    "    for t in merged:\n",
    "        n = norm(t)\n",
    "        if n not in seen:\n",
    "            final.append(n)\n",
    "            seen.add(n)\n",
    "    return final\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9d3d67",
   "metadata": {},
   "source": [
    "### 7. BUILD BERT + SPACY ENSEMBLE USING merge_sentence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "48532509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading TRAIN + DEV data to build vocabulary...\n",
      "# unique normalized terms from TRAIN+DEV gold: 812\n",
      "Loading TEST data...\n",
      "# test sentences: 1142\n",
      "Loading BERT predictions (TEST)...\n",
      "Loading SpaCy predictions (TEST)...\n",
      "\n",
      "Building BERT + SpaCy ensemble on TEST ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1142/1142 [00:00<00:00, 137565.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------------------------------\n",
      "Sentence 0\n",
      "TEXT: COMUNE DI AMATO\n",
      "  BERT      : []\n",
      "  SPACY     : []\n",
      "  MERGED OUT: []\n",
      "\n",
      "---------------------------------------\n",
      "Sentence 1\n",
      "TEXT: PROVINCIA DI CATANZARO\n",
      "  BERT      : []\n",
      "  SPACY     : []\n",
      "  MERGED OUT: []\n",
      "\n",
      "---------------------------------------\n",
      "Sentence 2\n",
      "TEXT: (UFFICIO DEL SINDACO)\n",
      "  BERT      : []\n",
      "  SPACY     : []\n",
      "  MERGED OUT: []\n",
      "✓ Saved cleaned predictions to ../../src/final_train_dev_training/predictions/subtask_a_test_ensemble_bert_spacy_dictfilter.json\n",
      "\n",
      "Ensemble predictions saved to: ../../src/final_train_dev_training/predictions/subtask_a_test_ensemble_bert_spacy_dictfilter.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm  # <-- non \"import tqdm\"\n",
    "\n",
    "# ---- Load TRAIN + DEV data and build vocabulary ----\n",
    "print(\"Loading TRAIN + DEV data to build vocabulary...\")\n",
    "train_data = load_json(TRAIN_FILE)\n",
    "dev_data   = load_json(DEV_FILE)\n",
    "\n",
    "train_vocab_norm = build_train_vocab(train_data, dev_data)\n",
    "print(f\"# unique normalized terms from TRAIN+DEV gold: {len(train_vocab_norm)}\")\n",
    "\n",
    "# ---- Load TEST data (per scorrere frasi nell'ordine corretto) ----\n",
    "print(\"Loading TEST data...\")\n",
    "test_data = load_json(TEST_FILE)\n",
    "test_rows = test_data[\"data\"] if isinstance(test_data, dict) and \"data\" in test_data else test_data\n",
    "print(f\"# test sentences: {len(test_rows)}\")\n",
    "\n",
    "# ---- Load BERT, SpaCy predictions ----\n",
    "print(\"Loading BERT predictions (TEST)...\")\n",
    "bert_pred = load_json(BERT_TEST_PRED_FILE)\n",
    "bert_map = build_term_map(bert_pred)\n",
    "\n",
    "print(\"Loading SpaCy predictions (TEST)...\")\n",
    "spacy_pred = load_json(SPACY_TEST_PRED_FILE)\n",
    "spacy_map = build_term_map(spacy_pred)\n",
    "\n",
    "# ---- Build ensemble predictions using merge_sentence ----\n",
    "ensemble_output = {\"data\": []}\n",
    "\n",
    "print(\"\\nBuilding BERT + SpaCy ensemble on TEST ...\")\n",
    "\n",
    "for idx, row in enumerate(tqdm(test_rows)):\n",
    "    key = (row[\"document_id\"], row[\"paragraph_id\"], row[\"sentence_id\"])\n",
    "\n",
    "    bert_terms  = bert_map.get(key, []) or []\n",
    "    spacy_terms = spacy_map.get(key, []) or []\n",
    "\n",
    "    merged_terms = merge_sentence(\n",
    "        bert_terms=bert_terms,\n",
    "        spacy_terms=spacy_terms,\n",
    "        train_vocab_norm=train_vocab_norm\n",
    "    )\n",
    "\n",
    "    # Debug sulle prime 3 frasi\n",
    "    if idx < 3:\n",
    "        print(\"\\n---------------------------------------\")\n",
    "        print(\"Sentence\", idx)\n",
    "        print(\"TEXT:\", row[\"sentence_text\"])\n",
    "        print(\"  BERT      :\", bert_terms)\n",
    "        print(\"  SPACY     :\", spacy_terms)\n",
    "        print(\"  MERGED OUT:\", merged_terms)\n",
    "\n",
    "    ensemble_output[\"data\"].append({\n",
    "        \"document_id\": row[\"document_id\"],\n",
    "        \"paragraph_id\": row[\"paragraph_id\"],\n",
    "        \"sentence_id\": row[\"sentence_id\"],\n",
    "        \"term_list\": merged_terms,\n",
    "    })\n",
    "\n",
    "# ---- Save final merged predictions (SUBMISSION) ----\n",
    "save_json(ensemble_output, ENSEMBLE_OUT_FILE)\n",
    "print(f\"\\nEnsemble predictions saved to: {ENSEMBLE_OUT_FILE}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5f2d4e",
   "metadata": {},
   "source": [
    "#### Save predictions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ATE-IT venv)",
   "language": "python",
   "name": "ate-it-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
