{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f192efa",
   "metadata": {},
   "source": [
    "# SpaCy Term Extraction for Italian Text\n",
    " **Trained**: Custom NER model fine-tuned for term extraction\n",
    "\n",
    "Dataset: EvalITA 2025 ATE-IT (Automatic Term Extraction - Italian Testbed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5800b0f8",
   "metadata": {},
   "source": [
    "## Setup and Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "82b6c4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m spacy download it_core_news_sm\n",
    "#!python -m spacy download it_core_news_md\n",
    "#!python -m spacy download it_core_news_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cfe3a8ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Italian model loaded successfully\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "import spacy\n",
    "from spacy.tokens import DocBin, Doc\n",
    "from spacy.training import Example\n",
    "from spacy.util import minibatch, compounding\n",
    "from spacy.pipeline import EntityRuler\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load Italian model\n",
    "try:\n",
    "    nlp = spacy.load('it_core_news_md')\n",
    "    print(\"✓ Italian model loaded successfully\")\n",
    "except:\n",
    "    print(\"Model not found. Install with: python -m spacy download it_core_news_md\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff0de76",
   "metadata": {},
   "source": [
    "## Data Loading and Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f7d62edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_jsonl(path: str) -> List[Dict]:\n",
    "    \"\"\"Load a JSON lines file or JSON array file.\"\"\"\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        text = f.read().strip()\n",
    "    if not text:\n",
    "        return []\n",
    "    try:\n",
    "        # Try parsing as single JSON object/array\n",
    "        data = json.loads(text)\n",
    "    except json.JSONDecodeError:\n",
    "        # Fall back to JSONL (one JSON per line)\n",
    "        data = []\n",
    "        for line in text.splitlines():\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                data.append(json.loads(line))\n",
    "    return data\n",
    "\n",
    "\n",
    "def build_sentence_gold_map(records: List[Dict]) -> List[Dict]:\n",
    "    \"\"\"Convert dataset rows into list of sentences with aggregated terms.\n",
    "    \n",
    "    Handles both formats:\n",
    "    - Records with 'term_list' field (list of terms) for input files in json format\n",
    "    - Records with individual 'term' field (one term per row) for input files in csv format\n",
    "    \"\"\"\n",
    "    out = {}\n",
    "    \n",
    "    # Support both dict with 'data' key and plain list\n",
    "    if isinstance(records, dict) and 'data' in records:\n",
    "        rows = records['data']\n",
    "    else:\n",
    "        rows = records\n",
    "    \n",
    "    for r in rows:\n",
    "        key = (r.get('document_id'), r.get('paragraph_id'), r.get('sentence_id'))\n",
    "        if key not in out:\n",
    "            out[key] = {\n",
    "                'document_id': r.get('document_id'),\n",
    "                'paragraph_id': r.get('paragraph_id'),\n",
    "                'sentence_id': r.get('sentence_id'),\n",
    "                'sentence_text': r.get('sentence_text', ''),\n",
    "                'terms': []\n",
    "            }\n",
    "        \n",
    "        # Support both 'term_list' (list) and 'term' (single value)\n",
    "        if isinstance(r.get('term_list'), list):\n",
    "            for t in r.get('term_list'):\n",
    "                if t and t not in out[key]['terms']:\n",
    "                    out[key]['terms'].append(t)\n",
    "        else:\n",
    "            term = r.get('term')\n",
    "            if term and term not in out[key]['terms']:\n",
    "                out[key]['terms'].append(term)\n",
    "    \n",
    "    return list(out.values())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1e62011d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training sentences: 2308\n",
      "Dev sentences: 577\n",
      "\n",
      "Example sentence:\n",
      "  Text: AFFIDAMENTO DEL “SERVIZIO DI SPAZZAMENTO, RACCOLTA, TRASPORTO E SMALTIMENTO/RECUPERO DEI RIFIUTI URBANI ED ASSIMILATI E SERVIZI COMPLEMENTARI DELLA CITTA' DI AGROPOLI” VALEVOLE PER UN QUINQUENNIO\n",
      "  Terms: ['raccolta', 'recupero', 'servizio di raccolta', 'servizio di spazzamento', 'smaltimento', 'trasporto']\n",
      "TRAIN+DEV sentences: 2885\n"
     ]
    }
   ],
   "source": [
    "# Load actual training and dev data\n",
    "train_data = load_jsonl('../../data/subtask_a_train.json')\n",
    "dev_data = load_jsonl('../../data/subtask_a_dev.json')\n",
    "\n",
    "train_sentences = build_sentence_gold_map(train_data)\n",
    "dev_sentences = build_sentence_gold_map(dev_data)\n",
    "\n",
    "print(f\"Training sentences: {len(train_sentences)}\")\n",
    "print(f\"Dev sentences: {len(dev_sentences)}\")\n",
    "print(f\"\\nExample sentence:\")\n",
    "print(f\"  Text: {train_sentences[6]['sentence_text']}\")\n",
    "print(f\"  Terms: {train_sentences[6]['terms']}\")\n",
    "\n",
    "# Build TRAIN+DEV sentence list for final training\n",
    "train_dev_sentences = train_sentences + dev_sentences\n",
    "print(f\"TRAIN+DEV sentences: {len(train_dev_sentences)}\")\n",
    "\n",
    "# Texts and term lists for training\n",
    "train_dev_texts = [s[\"sentence_text\"] for s in train_dev_sentences]\n",
    "train_dev_term_lists = [s[\"terms\"] for s in train_dev_sentences]\n",
    "\n",
    "# For evaluation on DEV (sanity check only)\n",
    "dev_texts = [s[\"sentence_text\"] for s in dev_sentences]\n",
    "dev_gold = [s[\"terms\"] for s in dev_sentences]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11fcded5",
   "metadata": {},
   "source": [
    "## Evaluation Metrics\n",
    "\n",
    "Using the official evaluation metrics from the competition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9b1f2479",
   "metadata": {},
   "outputs": [],
   "source": [
    "def micro_f1_score(gold_standard, system_output):\n",
    "    \"\"\"\n",
    "    Evaluates performance using Precision, Recall, and F1 score \n",
    "    based on individual term matching (micro-average).\n",
    "    \n",
    "    Args:\n",
    "        gold_standard: List of lists, where each inner list contains gold standard terms\n",
    "        system_output: List of lists, where each inner list contains extracted terms\n",
    "    \n",
    "    Returns:\n",
    "        Tuple containing (precision, recall, f1, tp, fp, fn)\n",
    "    \"\"\"\n",
    "    total_true_positives = 0\n",
    "    total_false_positives = 0\n",
    "    total_false_negatives = 0\n",
    "    \n",
    "    # Iterate through each item's gold standard and system output terms\n",
    "    for gold, system in zip(gold_standard, system_output):\n",
    "        # Convert to sets for efficient comparison\n",
    "        gold_set = set(gold)\n",
    "        system_set = set(system)\n",
    "        \n",
    "        # Calculate TP, FP, FN for the current item\n",
    "        true_positives = len(gold_set.intersection(system_set))\n",
    "        false_positives = len(system_set - gold_set)\n",
    "        false_negatives = len(gold_set - system_set)\n",
    "        \n",
    "        # Accumulate totals across all items\n",
    "        total_true_positives += true_positives\n",
    "        total_false_positives += false_positives\n",
    "        total_false_negatives += false_negatives\n",
    "    \n",
    "    # Calculate Precision, Recall, and F1 score (micro-average)\n",
    "    precision = total_true_positives / (total_true_positives + total_false_positives) if (total_true_positives + total_false_positives) > 0 else 0\n",
    "    recall = total_true_positives / (total_true_positives + total_false_negatives) if (total_true_positives + total_false_negatives) > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    return precision, recall, f1, total_true_positives, total_false_positives, total_false_negatives\n",
    "\n",
    "\n",
    "def type_f1_score(gold_standard, system_output):\n",
    "    \"\"\"\n",
    "    Evaluates performance using Type Precision, Type Recall, and Type F1 score\n",
    "    based on the set of unique terms extracted at least once across the entire dataset.\n",
    "    \n",
    "    Args:\n",
    "        gold_standard: List of lists, where each inner list contains gold standard terms\n",
    "        system_output: List of lists, where each inner list contains extracted terms\n",
    "    \n",
    "    Returns:\n",
    "        Tuple containing (type_precision, type_recall, type_f1)\n",
    "    \"\"\"\n",
    "    # Get the set of all unique gold standard terms across the dataset\n",
    "    all_gold_terms = set()\n",
    "    for item_terms in gold_standard:\n",
    "        all_gold_terms.update(item_terms)\n",
    "    \n",
    "    # Get the set of all unique system extracted terms across the dataset\n",
    "    all_system_terms = set()\n",
    "    for item_terms in system_output:\n",
    "        all_system_terms.update(item_terms)\n",
    "    \n",
    "    # Calculate True Positives (terms present in both sets)\n",
    "    type_true_positives = len(all_gold_terms.intersection(all_system_terms))\n",
    "    \n",
    "    # Calculate False Positives (terms in system output but not in gold standard)\n",
    "    type_false_positives = len(all_system_terms - all_gold_terms)\n",
    "    \n",
    "    # Calculate False Negatives (terms in gold standard but not in system output)\n",
    "    type_false_negatives = len(all_gold_terms - all_system_terms)\n",
    "    \n",
    "    # Calculate Type Precision, Type Recall, and Type F1 score\n",
    "    type_precision = type_true_positives / (type_true_positives + type_false_positives) if (type_true_positives + type_false_positives) > 0 else 0\n",
    "    type_recall = type_true_positives / (type_true_positives + type_false_negatives) if (type_true_positives + type_false_negatives) > 0 else 0\n",
    "    type_f1 = 2 * (type_precision * type_recall) / (type_precision + type_recall) if (type_precision + type_recall) > 0 else 0\n",
    "    \n",
    "    return type_precision, type_recall, type_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019c25b2",
   "metadata": {},
   "source": [
    "## Trained Model: Custom NER\n",
    "\n",
    "Neural approach that learns from examples:\n",
    "- Fine-tunes SpaCy's NER model on term extraction task\n",
    "- Learns patterns and context from labeled data\n",
    "- Can generalize to similar terms not seen during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e8ec48c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpacyTrainedModel:\n",
    "    \"\"\"Trainable NER model for term extraction.\"\"\"\n",
    "    \n",
    "    def __init__(self, model: str = 'it_core_news_sm'):\n",
    "        self.model_name = model\n",
    "        self.nlp = None\n",
    "    \n",
    "    def _prepare_training_data(self, sentences: List[str], term_lists: List[List[str]]) -> List[Example]:\n",
    "        \"\"\"Convert to SpaCy training format with character-span annotations.\"\"\"\n",
    "        training_data = []\n",
    "        \n",
    "        for sent_text, terms in zip(sentences, term_lists):\n",
    "            doc = self.nlp.make_doc(sent_text)\n",
    "            entities = []\n",
    "            \n",
    "            # Find character spans for each term\n",
    "            for term in terms:\n",
    "                if not term:\n",
    "                    continue\n",
    "                \n",
    "                # Find all occurrences\n",
    "                start_idx = 0\n",
    "                while True:\n",
    "                    start_idx = sent_text.find(term, start_idx)\n",
    "                    if start_idx == -1:\n",
    "                        break\n",
    "                    \n",
    "                    end_idx = start_idx + len(term)\n",
    "                    span = doc.char_span(start_idx, end_idx, label='TERM', alignment_mode='expand')\n",
    "                    if span is not None:\n",
    "                        entities.append((start_idx, end_idx, 'TERM'))\n",
    "                    \n",
    "                    start_idx = end_idx\n",
    "            \n",
    "            # Remove overlapping entities\n",
    "            entities = self._remove_overlapping(entities)\n",
    "            example = Example.from_dict(doc, {'entities': entities})\n",
    "            training_data.append(example)\n",
    "        \n",
    "        return training_data\n",
    "    \n",
    "    def _remove_overlapping(self, entities: List[Tuple[int, int, str]]) -> List[Tuple[int, int, str]]:\n",
    "        \"\"\"Keep longer spans when entities overlap.\"\"\"\n",
    "        if not entities:\n",
    "            return []\n",
    "        \n",
    "        # Sort by start, then by length (descending)\n",
    "        entities = sorted(entities, key=lambda x: (x[0], -(x[1] - x[0])))\n",
    "        \n",
    "        non_overlapping = []\n",
    "        for start, end, label in entities:\n",
    "            overlaps = False\n",
    "            for prev_start, prev_end, _ in non_overlapping:\n",
    "                if not (end <= prev_start or start >= prev_end):\n",
    "                    overlaps = True\n",
    "                    break\n",
    "            if not overlaps:\n",
    "                non_overlapping.append((start, end, label))\n",
    "        \n",
    "        return non_overlapping\n",
    "    \n",
    "    def train(self, sentences: List[str], term_lists: List[List[str]], \n",
    "              n_iter: int = 30, dropout: float = 0.2, batch_size: int = 8):\n",
    "        \"\"\"Train NER model on labeled data.\"\"\"\n",
    "        print(f\"Initializing model: {self.model_name}\")\n",
    "        \n",
    "        # Load base model\n",
    "        try:\n",
    "            self.nlp = spacy.load(self.model_name)\n",
    "        except:\n",
    "            print(f\"Model not found, using blank Italian model\")\n",
    "            self.nlp = spacy.blank('it')\n",
    "        \n",
    "        # Setup NER\n",
    "        if 'ner' not in self.nlp.pipe_names:\n",
    "            ner = self.nlp.add_pipe('ner')\n",
    "        else:\n",
    "            ner = self.nlp.get_pipe('ner')\n",
    "        ner.add_label('TERM')\n",
    "        \n",
    "        # Prepare training data\n",
    "        print(\"Preparing training examples...\")\n",
    "        train_examples = self._prepare_training_data(sentences, term_lists)\n",
    "        train_examples = [ex for ex in train_examples if len(ex.reference.ents) > 0] # Keep only examples with entities\n",
    "        print(f\"Training on {len(train_examples)} examples\")\n",
    "        \n",
    "        # Train\n",
    "        other_pipes = [pipe for pipe in self.nlp.pipe_names if pipe != 'ner']\n",
    "        with self.nlp.disable_pipes(*other_pipes):\n",
    "            #optimizer = self.nlp.begin_training()\n",
    "            if self.model_name == 'it_core_news_sm':\n",
    "                optimizer = self.nlp.resume_training()\n",
    "            else:\n",
    "                optimizer = self.nlp.begin_training()\n",
    "\n",
    "            for iteration in tqdm(range(n_iter), desc=\"Training\", total=n_iter):\n",
    "                random.shuffle(train_examples)\n",
    "                losses = {}\n",
    "                batches = minibatch(train_examples, size=compounding(4.0, batch_size, 1.001))\n",
    "                \n",
    "                for batch in batches:\n",
    "                    self.nlp.update(batch, drop=dropout, losses=losses)\n",
    "                \n",
    "                if iteration % 5 == 0:\n",
    "                    print(f\"  Iteration {iteration}: Loss = {losses.get('ner', 0):.3f}\")\n",
    "        \n",
    "        print(\"Training complete!\")\n",
    "    \n",
    "    def predict(self, sentences: List[str]) -> List[List[str]]:\n",
    "        \"\"\"Extract terms from sentences.\"\"\"\n",
    "        if self.nlp is None:\n",
    "            raise RuntimeError(\"Model not trained. Call train() or load() first.\")\n",
    "        \n",
    "        results = []\n",
    "        for doc in self.nlp.pipe(sentences, batch_size=32):\n",
    "            terms = [ent.text for ent in doc.ents if ent.label_ == 'TERM']\n",
    "            results.append(terms)\n",
    "        return results\n",
    "    \n",
    "    def save(self, path: str):\n",
    "        \"\"\"Save trained model.\"\"\"\n",
    "        if self.nlp is None:\n",
    "            raise RuntimeError(\"No model to save\")\n",
    "        \n",
    "        output_dir = Path(path)\n",
    "        output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        self.nlp.to_disk(output_dir)\n",
    "        print(f\"Model saved to {output_dir}\")\n",
    "    \n",
    "    def load(self, path: str):\n",
    "        \"\"\"Load trained model.\"\"\"\n",
    "        self.nlp = spacy.load(path)\n",
    "        if 'ner' not in self.nlp.pipe_names:\n",
    "            raise ValueError(\"Loaded model doesn't have NER component\")\n",
    "        print(f\"Model loaded from {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c444e273",
   "metadata": {},
   "source": [
    "### Train on TRAIN+DEV\n",
    "\n",
    "Note: This cell might take several minutes to run.\n",
    "\n",
    "**Additional configurations to test**\n",
    "- Keep overlapping entities\n",
    "- Keep documents with 0 entities in the training set\n",
    "- Change hyperparameters (*n_iter*, *dropout*, *batch_size*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9cdbf93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing model: it_core_news_md\n",
      "Preparing training examples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\super\\Documents\\UniPd\\ATA\\ATE-IT_SofiaMaule\\.venv\\Lib\\site-packages\\spacy\\training\\iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"Valore econ unit frazione= valore economico unitar...\" with entities \"[(17, 25, 'TERM'), (68, 76, 'TERM'), (132, 158, 'T...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "c:\\Users\\super\\Documents\\UniPd\\ATA\\ATE-IT_SofiaMaule\\.venv\\Lib\\site-packages\\spacy\\training\\iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"Scarti di cucina, di frutta e di verdura, avanzi d...\" with entities \"[(72, 74, 'TERM'), (113, 115, 'TERM'), (164, 166, ...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 763 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   2%|▎         | 1/40 [00:07<04:51,  7.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Iteration 0: Loss = 3532.880\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  15%|█▌        | 6/40 [00:44<04:12,  7.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Iteration 5: Loss = 710.821\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  28%|██▊       | 11/40 [01:22<03:41,  7.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Iteration 10: Loss = 549.970\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  40%|████      | 16/40 [02:00<03:03,  7.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Iteration 15: Loss = 264.305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  52%|█████▎    | 21/40 [02:39<02:25,  7.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Iteration 20: Loss = 181.076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  65%|██████▌   | 26/40 [03:17<01:46,  7.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Iteration 25: Loss = 193.852\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  78%|███████▊  | 31/40 [03:56<01:10,  7.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Iteration 30: Loss = 114.109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  90%|█████████ | 36/40 [04:35<00:30,  7.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Iteration 35: Loss = 120.093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 40/40 [05:07<00:00,  7.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Prepare training data\n",
    "trained_model = SpacyTrainedModel(model=\"it_core_news_md\")\n",
    "\n",
    "trained_model.train(\n",
    "    train_dev_texts,\n",
    "    train_dev_term_lists,\n",
    "    n_iter=40,   \n",
    "    dropout=0.1,\n",
    "    batch_size=8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8fd629db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running trained model predictions...\n",
      "\n",
      "Trained Model Results:\n",
      "Micro-averaged metrics:\n",
      "  Precision: 0.7383\n",
      "  Recall:    0.4878\n",
      "  F1 Score:  0.5874\n",
      "  TP=220, FP=78, FN=231\n",
      "\n",
      "Type-level metrics:\n",
      "  Type Precision: 0.7136\n",
      "  Type Recall:    0.5868\n",
      "  Type F1 Score:  0.6440\n"
     ]
    }
   ],
   "source": [
    "# Predict on dev set\n",
    "print(\"Running trained model predictions...\")\n",
    "trained_preds = trained_model.predict(dev_texts)\n",
    "\n",
    "# Evaluate\n",
    "precision, recall, f1, tp, fp, fn = micro_f1_score(dev_gold, trained_preds)\n",
    "type_precision, type_recall, type_f1 = type_f1_score(dev_gold, trained_preds)\n",
    "\n",
    "print(\"\\nTrained Model Results:\")\n",
    "print(\"Micro-averaged metrics:\")\n",
    "print(f\"  Precision: {precision:.4f}\")\n",
    "print(f\"  Recall:    {recall:.4f}\")\n",
    "print(f\"  F1 Score:  {f1:.4f}\")\n",
    "print(f\"  TP={tp}, FP={fp}, FN={fn}\")\n",
    "print(\"\\nType-level metrics:\")\n",
    "print(f\"  Type Precision: {type_precision:.4f}\")\n",
    "print(f\"  Type Recall:    {type_recall:.4f}\")\n",
    "print(f\"  Type F1 Score:  {type_f1:.4f}\")\n",
    "\n",
    "# Store metrics for later comparison\n",
    "trained_metrics = {\n",
    "    'precision': precision,\n",
    "    'recall': recall,\n",
    "    'f1': f1,\n",
    "    'type_precision': type_precision,\n",
    "    'type_recall': type_recall,\n",
    "    'type_f1': type_f1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4dc5fb1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models\\spacy_trained_train_dev\n"
     ]
    }
   ],
   "source": [
    "# Save trained model\n",
    "trained_model.save('models/spacy_trained_train_dev')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5299f2c1",
   "metadata": {},
   "source": [
    "## Results Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95db8f7d",
   "metadata": {},
   "source": [
    "## Save Predictions to Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67116b0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 577 predictions to predictions/subtask_a_dev_spacy_trained_preds_refined.json\n"
     ]
    }
   ],
   "source": [
    "def save_predictions(predictions: List[List[str]], \n",
    "                     sentences: List[Dict], \n",
    "                     output_path: str):\n",
    "    \"\"\"Save predictions in competition format.\"\"\"\n",
    "    output = {'data': []}\n",
    "    for pred, sent in zip(predictions, sentences):\n",
    "        output['data'].append({\n",
    "            'document_id': sent['document_id'],\n",
    "            'paragraph_id': sent['paragraph_id'],\n",
    "            'sentence_id': sent['sentence_id'],\n",
    "            'term_list': pred\n",
    "        })\n",
    "    \n",
    "    os.makedirs(os.path.dirname(output_path) or '.', exist_ok=True)\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(output, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"Saved {len(predictions)} predictions to {output_path}\")\n",
    "\n",
    "\n",
    "# Save both sets of predictions\n",
    "save_predictions(trained_preds, dev_sentences, 'predictions/subtask_a_dev_spacy_trained_preds__train_dev.json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
