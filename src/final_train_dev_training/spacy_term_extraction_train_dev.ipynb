{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f192efa",
   "metadata": {},
   "source": [
    "# SpaCy Term Extraction for Italian Text\n",
    " **Trained**: Custom NER model fine-tuned for term extraction\n",
    "\n",
    "Dataset: EvalITA 2025 ATE-IT (Automatic Term Extraction - Italian Testbed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5800b0f8",
   "metadata": {},
   "source": [
    "## Setup and Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "82b6c4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m spacy download it_core_news_sm\n",
    "#!python -m spacy download it_core_news_md\n",
    "#!python -m spacy download it_core_news_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cfe3a8ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Italian model loaded successfully\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "import spacy\n",
    "from spacy.tokens import DocBin, Doc\n",
    "from spacy.training import Example\n",
    "from spacy.util import minibatch, compounding\n",
    "from spacy.pipeline import EntityRuler\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load Italian model\n",
    "try:\n",
    "    nlp = spacy.load('it_core_news_md')\n",
    "    print(\"✓ Italian model loaded successfully\")\n",
    "except:\n",
    "    print(\"Model not found. Install with: python -m spacy download it_core_news_md\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff0de76",
   "metadata": {},
   "source": [
    "## Data Loading and Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7d62edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_jsonl(path: str) -> List[Dict]:\n",
    "    \"\"\"Load a JSON lines file or JSON array file.\"\"\"\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        text = f.read().strip()\n",
    "    if not text:\n",
    "        return []\n",
    "    try:\n",
    "        # Try parsing as single JSON object/array\n",
    "        data = json.loads(text)\n",
    "    except json.JSONDecodeError:\n",
    "        # Fall back to JSONL (one JSON per line)\n",
    "        data = []\n",
    "        for line in text.splitlines():\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                data.append(json.loads(line))\n",
    "    return data\n",
    "\n",
    "\n",
    "def build_sentence_gold_map(records: List[Dict]) -> List[Dict]:\n",
    "    \"\"\"Convert dataset rows into list of sentences with aggregated terms.\n",
    "    \n",
    "    Handles both formats:\n",
    "    - Records with 'term_list' field (list of terms) for input files in json format\n",
    "    - Records with individual 'term' field (one term per row) for input files in csv format\n",
    "    \"\"\"\n",
    "    out = {}\n",
    "    \n",
    "    # Support both dict with 'data' key and plain list\n",
    "    if isinstance(records, dict) and 'data' in records:\n",
    "        rows = records['data']\n",
    "    else:\n",
    "        rows = records\n",
    "    \n",
    "    for r in rows:\n",
    "        key = (r.get('document_id'), r.get('paragraph_id'), r.get('sentence_id'))\n",
    "        if key not in out:\n",
    "            out[key] = {\n",
    "                'document_id': r.get('document_id'),\n",
    "                'paragraph_id': r.get('paragraph_id'),\n",
    "                'sentence_id': r.get('sentence_id'),\n",
    "                'sentence_text': r.get('sentence_text', ''),\n",
    "                'terms': []\n",
    "            }\n",
    "        \n",
    "        # Support both 'term_list' (list) and 'term' (single value)\n",
    "        if isinstance(r.get('term_list'), list):\n",
    "            for t in r.get('term_list'):\n",
    "                if t and t not in out[key]['terms']:\n",
    "                    out[key]['terms'].append(t)\n",
    "        else:\n",
    "            term = r.get('term')\n",
    "            if term and term not in out[key]['terms']:\n",
    "                out[key]['terms'].append(term)\n",
    "    \n",
    "    return list(out.values())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e62011d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training sentences: 2308\n",
      "Dev sentences: 577\n",
      "\n",
      "Example sentence:\n",
      "  Text: AFFIDAMENTO DEL “SERVIZIO DI SPAZZAMENTO, RACCOLTA, TRASPORTO E SMALTIMENTO/RECUPERO DEI RIFIUTI URBANI ED ASSIMILATI E SERVIZI COMPLEMENTARI DELLA CITTA' DI AGROPOLI” VALEVOLE PER UN QUINQUENNIO\n",
      "  Terms: ['raccolta', 'recupero', 'servizio di raccolta', 'servizio di spazzamento', 'smaltimento', 'trasporto']\n",
      "TRAIN+DEV sentences: 2885\n"
     ]
    }
   ],
   "source": [
    "# Load actual training and dev data\n",
    "train_data = load_jsonl('../../data/subtask_a_train.json')\n",
    "dev_data = load_jsonl('../../data/subtask_a_dev.json')\n",
    "\n",
    "train_sentences = build_sentence_gold_map(train_data)\n",
    "dev_sentences = build_sentence_gold_map(dev_data)\n",
    "\n",
    "print(f\"Training sentences: {len(train_sentences)}\")\n",
    "print(f\"Dev sentences: {len(dev_sentences)}\")\n",
    "print(f\"\\nExample sentence:\")\n",
    "print(f\"  Text: {train_sentences[6]['sentence_text']}\")\n",
    "print(f\"  Terms: {train_sentences[6]['terms']}\")\n",
    "\n",
    "# Build TRAIN+DEV sentence list for final training\n",
    "train_dev_sentences = train_sentences + dev_sentences\n",
    "print(f\"TRAIN+DEV sentences: {len(train_dev_sentences)}\")\n",
    "\n",
    "# Texts and term lists for training\n",
    "train_dev_texts = [s[\"sentence_text\"] for s in train_dev_sentences]\n",
    "train_dev_term_lists = [s[\"terms\"] for s in train_dev_sentences]\n",
    "\n",
    "# For evaluation on DEV (sanity check only)\n",
    "dev_texts = [s[\"sentence_text\"] for s in dev_sentences]\n",
    "dev_gold = [s[\"terms\"] for s in dev_sentences]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019c25b2",
   "metadata": {},
   "source": [
    "## Trained Model: Custom NER\n",
    "\n",
    "Neural approach that learns from examples:\n",
    "- Fine-tunes SpaCy's NER model on term extraction task\n",
    "- Learns patterns and context from labeled data\n",
    "- Can generalize to similar terms not seen during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e8ec48c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpacyTrainedModel:\n",
    "    \"\"\"Trainable NER model for term extraction.\"\"\"\n",
    "    \n",
    "    def __init__(self, model: str = 'it_core_news_sm'):\n",
    "        self.model_name = model\n",
    "        self.nlp = None\n",
    "    \n",
    "    def _prepare_training_data(self, sentences: List[str], term_lists: List[List[str]]) -> List[Example]:\n",
    "        \"\"\"Convert to SpaCy training format with character-span annotations.\"\"\"\n",
    "        training_data = []\n",
    "        \n",
    "        for sent_text, terms in zip(sentences, term_lists):\n",
    "            doc = self.nlp.make_doc(sent_text)\n",
    "            entities = []\n",
    "            \n",
    "            # Find character spans for each term\n",
    "            for term in terms:\n",
    "                if not term:\n",
    "                    continue\n",
    "                \n",
    "                # Find all occurrences\n",
    "                start_idx = 0\n",
    "                while True:\n",
    "                    start_idx = sent_text.find(term, start_idx)\n",
    "                    if start_idx == -1:\n",
    "                        break\n",
    "                    \n",
    "                    end_idx = start_idx + len(term)\n",
    "                    span = doc.char_span(start_idx, end_idx, label='TERM', alignment_mode='expand')\n",
    "                    if span is not None:\n",
    "                        entities.append((start_idx, end_idx, 'TERM'))\n",
    "                    \n",
    "                    start_idx = end_idx\n",
    "            \n",
    "            # Remove overlapping entities\n",
    "            entities = self._remove_overlapping(entities)\n",
    "            example = Example.from_dict(doc, {'entities': entities})\n",
    "            training_data.append(example)\n",
    "        \n",
    "        return training_data\n",
    "    \n",
    "    def _remove_overlapping(self, entities: List[Tuple[int, int, str]]) -> List[Tuple[int, int, str]]:\n",
    "        \"\"\"Keep longer spans when entities overlap.\"\"\"\n",
    "        if not entities:\n",
    "            return []\n",
    "        \n",
    "        # Sort by start, then by length (descending)\n",
    "        entities = sorted(entities, key=lambda x: (x[0], -(x[1] - x[0])))\n",
    "        \n",
    "        non_overlapping = []\n",
    "        for start, end, label in entities:\n",
    "            overlaps = False\n",
    "            for prev_start, prev_end, _ in non_overlapping:\n",
    "                if not (end <= prev_start or start >= prev_end):\n",
    "                    overlaps = True\n",
    "                    break\n",
    "            if not overlaps:\n",
    "                non_overlapping.append((start, end, label))\n",
    "        \n",
    "        return non_overlapping\n",
    "    \n",
    "    def train(self, sentences: List[str], term_lists: List[List[str]], \n",
    "              n_iter: int = 30, dropout: float = 0.2, batch_size: int = 8):\n",
    "        \"\"\"Train NER model on labeled data.\"\"\"\n",
    "        print(f\"Initializing model: {self.model_name}\")\n",
    "        \n",
    "        # Load base model\n",
    "        try:\n",
    "            self.nlp = spacy.load(self.model_name)\n",
    "        except:\n",
    "            print(f\"Model not found, using blank Italian model\")\n",
    "            self.nlp = spacy.blank('it')\n",
    "        \n",
    "        # Setup NER\n",
    "        if 'ner' not in self.nlp.pipe_names:\n",
    "            ner = self.nlp.add_pipe('ner')\n",
    "        else:\n",
    "            ner = self.nlp.get_pipe('ner')\n",
    "        ner.add_label('TERM')\n",
    "        \n",
    "        # Prepare training data\n",
    "        print(\"Preparing training examples...\")\n",
    "        train_examples = self._prepare_training_data(sentences, term_lists)\n",
    "        train_examples = [ex for ex in train_examples if len(ex.reference.ents) > 0] # Keep only examples with entities\n",
    "        print(f\"Training on {len(train_examples)} examples\")\n",
    "        \n",
    "        # Train\n",
    "        other_pipes = [pipe for pipe in self.nlp.pipe_names if pipe != 'ner']\n",
    "        with self.nlp.disable_pipes(*other_pipes):\n",
    "            #optimizer = self.nlp.begin_training()\n",
    "            if self.model_name == 'it_core_news_sm':\n",
    "                optimizer = self.nlp.resume_training()\n",
    "            else:\n",
    "                optimizer = self.nlp.begin_training()\n",
    "\n",
    "            for iteration in tqdm(range(n_iter), desc=\"Training\", total=n_iter):\n",
    "                random.shuffle(train_examples)\n",
    "                losses = {}\n",
    "                batches = minibatch(train_examples, size=compounding(4.0, batch_size, 1.001))\n",
    "                \n",
    "                for batch in batches:\n",
    "                    self.nlp.update(batch, drop=dropout, losses=losses)\n",
    "                \n",
    "                if iteration % 5 == 0:\n",
    "                    print(f\"  Iteration {iteration}: Loss = {losses.get('ner', 0):.3f}\")\n",
    "        \n",
    "        print(\"Training complete!\")\n",
    "    \n",
    "    def predict(self, sentences: List[str]) -> List[List[str]]:\n",
    "        \"\"\"Extract terms from sentences.\"\"\"\n",
    "        if self.nlp is None:\n",
    "            raise RuntimeError(\"Model not trained. Call train() or load() first.\")\n",
    "        \n",
    "        results = []\n",
    "        for doc in self.nlp.pipe(sentences, batch_size=32):\n",
    "            terms = [ent.text for ent in doc.ents if ent.label_ == 'TERM']\n",
    "            results.append(terms)\n",
    "        return results\n",
    "    \n",
    "    def save(self, path: str):\n",
    "        \"\"\"Save trained model.\"\"\"\n",
    "        if self.nlp is None:\n",
    "            raise RuntimeError(\"No model to save\")\n",
    "        \n",
    "        output_dir = Path(path)\n",
    "        output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        self.nlp.to_disk(output_dir)\n",
    "        print(f\"Model saved to {output_dir}\")\n",
    "    \n",
    "    def load(self, path: str):\n",
    "        \"\"\"Load trained model.\"\"\"\n",
    "        self.nlp = spacy.load(path)\n",
    "        if 'ner' not in self.nlp.pipe_names:\n",
    "            raise ValueError(\"Loaded model doesn't have NER component\")\n",
    "        print(f\"Model loaded from {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c444e273",
   "metadata": {},
   "source": [
    "### Train on TRAIN+DEV\n",
    "\n",
    "Note: This cell might take several minutes to run.\n",
    "\n",
    "**Additional configurations to test**\n",
    "- Keep overlapping entities\n",
    "- Keep documents with 0 entities in the training set\n",
    "- Change hyperparameters (*n_iter*, *dropout*, *batch_size*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9cdbf93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing model: it_core_news_md\n",
      "Preparing training examples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\super\\Documents\\UniPd\\ATA\\ATE-IT_SofiaMaule\\.venv\\Lib\\site-packages\\spacy\\training\\iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"Valore econ unit frazione= valore economico unitar...\" with entities \"[(17, 25, 'TERM'), (68, 76, 'TERM'), (132, 158, 'T...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "c:\\Users\\super\\Documents\\UniPd\\ATA\\ATE-IT_SofiaMaule\\.venv\\Lib\\site-packages\\spacy\\training\\iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"Scarti di cucina, di frutta e di verdura, avanzi d...\" with entities \"[(72, 74, 'TERM'), (113, 115, 'TERM'), (164, 166, ...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 763 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   2%|▎         | 1/40 [00:07<04:51,  7.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Iteration 0: Loss = 3532.880\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  15%|█▌        | 6/40 [00:44<04:12,  7.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Iteration 5: Loss = 710.821\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  28%|██▊       | 11/40 [01:22<03:41,  7.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Iteration 10: Loss = 549.970\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  40%|████      | 16/40 [02:00<03:03,  7.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Iteration 15: Loss = 264.305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  52%|█████▎    | 21/40 [02:39<02:25,  7.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Iteration 20: Loss = 181.076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  65%|██████▌   | 26/40 [03:17<01:46,  7.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Iteration 25: Loss = 193.852\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  78%|███████▊  | 31/40 [03:56<01:10,  7.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Iteration 30: Loss = 114.109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  90%|█████████ | 36/40 [04:35<00:30,  7.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Iteration 35: Loss = 120.093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 40/40 [05:07<00:00,  7.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Prepare training data\n",
    "trained_model = SpacyTrainedModel(model=\"it_core_news_md\")\n",
    "\n",
    "trained_model.train(\n",
    "    train_dev_texts,\n",
    "    train_dev_term_lists,\n",
    "    n_iter=40,   \n",
    "    dropout=0.1,\n",
    "    batch_size=8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4dc5fb1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models\\spacy_trained_train_dev\n"
     ]
    }
   ],
   "source": [
    "# Save trained model\n",
    "trained_model.save('models/spacy_trained_train_dev')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d9ea5bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================== PREDICTION ON TEST (FINAL RUN) ==================\n",
    "\n",
    "test_data = load_jsonl('../../data/test.json')\n",
    "test_sentences = build_sentence_gold_map(test_data)\n",
    "test_texts = [s[\"sentence_text\"] for s in test_sentences]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6ce3c97f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Inference completed: 1142 predictions\n"
     ]
    }
   ],
   "source": [
    "SPACY_MODEL_PATH = '../../src/final_train_dev_training/models/spacy_trained_train_dev'\n",
    "nlp = spacy.load(SPACY_MODEL_PATH)\n",
    "test_preds = []\n",
    "for doc in nlp.pipe(test_texts, batch_size=32):\n",
    "    # estrai SOLO entità TERM (quelle che hai addestrato)\n",
    "    terms = [ent.text.lower() for ent in doc.ents if ent.label_ == \"TERM\"]\n",
    "    # deduplica\n",
    "    uniq = []\n",
    "    seen = set()\n",
    "    for t in terms:\n",
    "        if t not in seen:\n",
    "            seen.add(t)\n",
    "            uniq.append(t)\n",
    "    test_preds.append(uniq)\n",
    "\n",
    "print(f\"✓ Inference completed: {len(test_preds)} predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95db8f7d",
   "metadata": {},
   "source": [
    "## Save Predictions to Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "67116b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_predictions(predictions: List[List[str]], \n",
    "                     sentences: List[Dict], \n",
    "                     output_path: str):\n",
    "    \"\"\"Save predictions in competition format.\"\"\"\n",
    "    output = {'data': []}\n",
    "    for pred, sent in zip(predictions, sentences):\n",
    "        output['data'].append({\n",
    "            'document_id': sent['document_id'],\n",
    "            'paragraph_id': sent['paragraph_id'],\n",
    "            'sentence_id': sent['sentence_id'],\n",
    "            'term_list': pred\n",
    "        })\n",
    "    \n",
    "    os.makedirs(os.path.dirname(output_path) or '.', exist_ok=True)\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(output, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"Saved {len(predictions)} predictions to {output_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5eb7be0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 1142 predictions to predictions/subtask_a_test_spacy_trained_preds_train_dev.json\n"
     ]
    }
   ],
   "source": [
    "save_predictions(\n",
    "    test_preds,\n",
    "    test_sentences,\n",
    "    'predictions/subtask_a_test_spacy_trained_preds_train_dev.json'\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
