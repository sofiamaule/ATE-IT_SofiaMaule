{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "300afcee",
   "metadata": {},
   "source": [
    "# BERT Token Classification for Italian Term Extraction\n",
    "\n",
    "This notebook demonstrates a BERT-based approach to term extraction:\n",
    "- Uses BIO tagging scheme (Beginning-Inside-Outside)\n",
    "- Fine-tunes Italian BERT model for token classification\n",
    "- Trains on labeled data to recognize term boundaries\n",
    "\n",
    "Dataset: EvalITA 2025 ATE-IT (Automatic Term Extraction - Italian Testbed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257d342c",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bacd6f61",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T10:43:33.424051Z",
     "start_time": "2025-11-11T10:42:33.407768Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete\n",
      "PyTorch version: 2.8.0+cpu\n",
      "CUDA available: False\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer, #choose this\n",
    "    AutoModelForTokenClassification,  #choose this\n",
    "    TrainingArguments, \n",
    "    Trainer,\n",
    "    DataCollatorForTokenClassification\n",
    ")\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Setup complete\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8767bd33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels: ['O', 'B-TERM', 'I-TERM']\n",
      "Label to ID: {'O': 0, 'B-TERM': 1, 'I-TERM': 2}\n",
      "\n",
      "Model: dbmdz/bert-base-italian-uncased\n",
      "Output directory: models/bert_token_classification_2e-5_uncased_train_dev\n"
     ]
    }
   ],
   "source": [
    "# Define label mappings for BIO tagging scheme\n",
    "label_list = ['O', 'B-TERM', 'I-TERM']\n",
    "label2id = {k: v for v, k in enumerate(label_list)}\n",
    "id2label = {v: k for v, k in enumerate(label_list)}\n",
    "\n",
    "print(f\"Labels: {label_list}\")\n",
    "print(f\"Label to ID: {label2id}\")\n",
    "\n",
    "# Model configuration\n",
    "model_name = \"dbmdz/bert-base-italian-uncased\" \n",
    "output_model_dir = \"models/bert_token_classification_2e-5_uncased_train_dev\" \n",
    "\n",
    "print(f\"\\nModel: {model_name}\")\n",
    "print(f\"Output directory: {output_model_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8142a69c",
   "metadata": {},
   "source": [
    "## Data Loading and Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dfe3bbfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Data loading functions defined\n"
     ]
    }
   ],
   "source": [
    "def load_jsonl(path: str):\n",
    "    \"\"\"Load a JSON lines file or JSON array file.\"\"\"\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        text = f.read().strip()\n",
    "    if not text:\n",
    "        return []\n",
    "    try:\n",
    "        data = json.loads(text)\n",
    "    except json.JSONDecodeError:\n",
    "        data = []\n",
    "        for line in text.splitlines():\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                data.append(json.loads(line))\n",
    "    return data\n",
    "\n",
    "#aggregate the terms concatenating the paragraphs\n",
    "def build_sentence_gold_map(records):\n",
    "    \"\"\"Convert dataset rows into list of sentences with aggregated terms.\"\"\"\n",
    "    out = {}\n",
    "    \n",
    "    if isinstance(records, dict) and 'data' in records:\n",
    "        rows = records['data']\n",
    "    else:\n",
    "        rows = records\n",
    "    \n",
    "    for r in rows:\n",
    "        key = (r.get('document_id'), r.get('paragraph_id'), r.get('sentence_id'))\n",
    "        if key not in out:\n",
    "            out[key] = {\n",
    "                'document_id': r.get('document_id'),\n",
    "                'paragraph_id': r.get('paragraph_id'),\n",
    "                'sentence_id': r.get('sentence_id'),\n",
    "                'sentence_text': r.get('sentence_text', ''),\n",
    "                'terms': []\n",
    "            }\n",
    "        \n",
    "        if isinstance(r.get('term_list'), list):\n",
    "            for t in r.get('term_list'):\n",
    "                if t and t not in out[key]['terms']:\n",
    "                    out[key]['terms'].append(t)\n",
    "        else:\n",
    "            term = r.get('term')\n",
    "            if term and term not in out[key]['terms']:\n",
    "                out[key]['terms'].append(term)\n",
    "    \n",
    "    return list(out.values())\n",
    "\n",
    "\n",
    "print(\"✓ Data loading functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05486208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training sentences: 2308\n",
      "Dev sentences: 577\n",
      "\n",
      "Example sentence:\n",
      "  Text: AFFIDAMENTO DEL “SERVIZIO DI SPAZZAMENTO, RACCOLTA, TRASPORTO E SMALTIMENTO/RECUPERO DEI RIFIUTI URBANI ED ASSIMILATI E SERVIZI COMPLEMENTARI DELLA CITTA' DI AGROPOLI” VALEVOLE PER UN QUINQUENNIO\n",
      "  Terms: ['raccolta', 'recupero', 'servizio di raccolta', 'servizio di spazzamento', 'smaltimento', 'trasporto']\n"
     ]
    }
   ],
   "source": [
    "# Load training and dev data\n",
    "train_data = load_jsonl('../../data/subtask_a_train.json')\n",
    "dev_data = load_jsonl('../../data/subtask_a_dev.json')\n",
    "\n",
    "train_sentences = build_sentence_gold_map(train_data)\n",
    "dev_sentences = build_sentence_gold_map(dev_data)\n",
    "\n",
    "print(f\"Training sentences: {len(train_sentences)}\")\n",
    "print(f\"Dev sentences: {len(dev_sentences)}\")\n",
    "print(f\"\\nExample sentence:\")\n",
    "print(f\"  Text: {train_sentences[6]['sentence_text']}\")\n",
    "print(f\"  Terms: {train_sentences[6]['terms']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f56f629d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text, force_lower=True):\n",
    "    # fix encoding issues\n",
    "    text = text.replace(\"\\u00a0\", \" \")\n",
    "\n",
    "    # normalize spaces\n",
    "    text = \" \".join(text.split())\n",
    "\n",
    "    # unify apostrophes\n",
    "    text = text.replace(\"’\", \"'\").replace(\"`\", \"'\")\n",
    "\n",
    "    # lowercase if model is uncased\n",
    "    if force_lower:\n",
    "        text = text.lower()\n",
    "\n",
    "    # remove weird control characters\n",
    "    text = \"\".join(c for c in text if c.isprintable())\n",
    "\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9e0c703",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total TRAIN+DEV sentences: 2885\n"
     ]
    }
   ],
   "source": [
    "for entry in train_sentences:\n",
    "    # pulisci il testo della frase\n",
    "    entry[\"sentence_text\"] = preprocess_text(entry[\"sentence_text\"])\n",
    "    # (opzionale ma consigliato) pulisci anche i termini gold\n",
    "    entry[\"terms\"] = [preprocess_text(t) for t in entry[\"terms\"]]\n",
    "\n",
    "for entry in dev_sentences:\n",
    "    entry[\"sentence_text\"] = preprocess_text(entry[\"sentence_text\"])\n",
    "    entry[\"terms\"] = [preprocess_text(t) for t in entry[\"terms\"]]\n",
    "\n",
    "all_sentences = train_sentences + dev_sentences\n",
    "print(f\"Total TRAIN+DEV sentences: {len(all_sentences)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddfac0bc",
   "metadata": {},
   "source": [
    "## Initialize BERT Model and Tokenizer\n",
    "\n",
    "Always load\n",
    "- tokenizer\n",
    "- bert model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2bb1b27f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing BERT tokenizer and model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at dbmdz/bert-base-italian-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Tokenizer loaded: BertTokenizerFast\n",
      "✓ Model loaded with 3 labels\n",
      "  Vocabulary size: 31102\n"
     ]
    }
   ],
   "source": [
    "# Initialize tokenizer and model\n",
    "print(\"Initializing BERT tokenizer and model...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_name, \n",
    "    num_labels=len(label_list), #labels we're trying to predict\n",
    "    id2label=id2label, \n",
    "    label2id=label2id\n",
    ")\n",
    "\n",
    "print(f\"✓ Tokenizer loaded: {tokenizer.__class__.__name__}\")\n",
    "print(f\"✓ Model loaded with {model.num_labels} labels\")\n",
    "print(f\"  Vocabulary size: {tokenizer.vocab_size}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681cd148",
   "metadata": {},
   "source": [
    "## BIO Tag Generation for Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "158f6ddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing BERT tokenizer and model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at dbmdz/bert-base-italian-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Tokenizer loaded: BertTokenizerFast\n",
      "✓ Model loaded with 3 labels\n",
      "  Vocabulary size: 31102\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "# Initialize tokenizer and model\n",
    "print(\"Initializing BERT tokenizer and model...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_name, \n",
    "    num_labels=len(label_list), #labels we're trying to predict\n",
    "    id2label=id2label, \n",
    "    label2id=label2id\n",
    ")\n",
    "\n",
    "print(f\"✓ Tokenizer loaded: {tokenizer.__class__.__name__}\")\n",
    "print(f\"✓ Model loaded with {model.num_labels} labels\")\n",
    "print(f\"  Vocabulary size: {tokenizer.vocab_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0668cf5c",
   "metadata": {},
   "source": [
    "## Process Training + Dev Data with BIO Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d0b466b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ner_tags(text: str, terms: list[str], tokenizer, label2id: dict):\n",
    "    \"\"\"\n",
    "    Crea token e BIO tag per una frase, dato l'elenco dei termini gold.\n",
    "\n",
    "    text: frase pre-processata (come in preprocess_text)\n",
    "    terms: lista di termini gold pre-processati\n",
    "    tokenizer: tokenizer HuggingFace\n",
    "    label2id: dict, es. {'O': 0, 'B-TERM': 1, 'I-TERM': 2}\n",
    "\n",
    "    Ritorna:\n",
    "        tokens: list[str]\n",
    "        ner_tags: list[int] (stessa lunghezza di tokens)\n",
    "    \"\"\"\n",
    "\n",
    "    # --- 1) Trova tutti gli span (start_char, end_char) dei termini nel testo ---\n",
    "\n",
    "    def is_boundary(ch: str | None) -> bool:\n",
    "        \"\"\"True se il carattere è None o non alfanumerico (quindi buon confine di parola).\"\"\"\n",
    "        if ch is None:\n",
    "            return True\n",
    "        return not ch.isalnum()\n",
    "\n",
    "    spans = []  # lista di (start, end)\n",
    "    for term in terms:\n",
    "        term = term.strip()\n",
    "        if not term:\n",
    "            continue\n",
    "\n",
    "        start = 0\n",
    "        while True:\n",
    "            idx = text.find(term, start)\n",
    "            if idx == -1:\n",
    "                break\n",
    "\n",
    "            end = idx + len(term)\n",
    "\n",
    "            # Controllo confini di parola\n",
    "            before = text[idx - 1] if idx > 0 else None\n",
    "            after = text[end] if end < len(text) else None\n",
    "\n",
    "            if is_boundary(before) and is_boundary(after):\n",
    "                spans.append((idx, end))\n",
    "\n",
    "            start = idx + len(term)\n",
    "\n",
    "    # opzionale: ordina gli span per inizio\n",
    "    spans.sort(key=lambda x: x[0])\n",
    "\n",
    "    # --- 2) Tokenizza con offset mapping ---\n",
    "    encoded = tokenizer(\n",
    "        text,\n",
    "        return_offsets_mapping=True,\n",
    "        add_special_tokens=False\n",
    "    )\n",
    "\n",
    "    tokens = tokenizer.convert_ids_to_tokens(encoded[\"input_ids\"])\n",
    "    offsets = encoded[\"offset_mapping\"]\n",
    "\n",
    "    ner_tags = [label2id[\"O\"]] * len(tokens)\n",
    "\n",
    "    # --- 3) Assegna BIO tag in base agli span ---\n",
    "    for i, (tok_start, tok_end) in enumerate(offsets):\n",
    "        # alcuni tokenizer possono dare (0, 0) per token speciali, ma noi add_special_tokens=False\n",
    "        if tok_start == tok_end:\n",
    "            ner_tags[i] = label2id[\"O\"]\n",
    "            continue\n",
    "\n",
    "        tag = \"O\"\n",
    "        for span_start, span_end in spans:\n",
    "            # se il token inizia dentro uno span\n",
    "            if tok_start >= span_start and tok_start < span_end:\n",
    "                if tok_start == span_start:\n",
    "                    tag = \"B-TERM\"\n",
    "                else:\n",
    "                    tag = \"I-TERM\"\n",
    "                break\n",
    "\n",
    "        ner_tags[i] = label2id[tag]\n",
    "\n",
    "    return tokens, ner_tags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f1b6518b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing TRAIN+DEV data...\n",
      "  Processed 0/2885\n",
      "  Processed 1000/2885\n",
      "  Processed 2000/2885\n",
      "✓ TRAIN+DEV data processed: 2885 sentences\n",
      "\n",
      "Sample sentence:\n",
      "  Text: affidamento del “servizio di spazzamento, raccolta, trasporto e smaltimento/recupero dei rifiuti urbani ed assimilati e servizi complementari della citta' di agropoli” valevole per un quinquennio\n",
      "  Terms: ['raccolta', 'recupero', 'servizio di raccolta', 'servizio di spazzamento', 'smaltimento', 'trasporto']\n",
      "\n",
      "|    | Token         | Tag    |\n",
      "|---:|:--------------|:-------|\n",
      "|  0 | affidamento   | O      |\n",
      "|  1 | del           | O      |\n",
      "|  2 | “             | O      |\n",
      "|  3 | servizio      | B-TERM |\n",
      "|  4 | di            | I-TERM |\n",
      "|  5 | spa           | I-TERM |\n",
      "|  6 | ##zzamento    | I-TERM |\n",
      "|  7 | ,             | O      |\n",
      "|  8 | raccolta      | B-TERM |\n",
      "|  9 | ,             | O      |\n",
      "| 10 | trasporto     | B-TERM |\n",
      "| 11 | e             | O      |\n",
      "| 12 | smaltimento   | B-TERM |\n",
      "| 13 | /             | O      |\n",
      "| 14 | recupero      | B-TERM |\n",
      "| 15 | dei           | O      |\n",
      "| 16 | rifiuti       | O      |\n",
      "| 17 | urbani        | O      |\n",
      "| 18 | ed            | O      |\n",
      "| 19 | assimi        | O      |\n",
      "| 20 | ##lati        | O      |\n",
      "| 21 | e             | O      |\n",
      "| 22 | servizi       | O      |\n",
      "| 23 | complementari | O      |\n",
      "| 24 | della         | O      |\n",
      "| 25 | citta         | O      |\n",
      "| 26 | '             | O      |\n",
      "| 27 | di            | O      |\n",
      "| 28 | agro          | O      |\n",
      "| 29 | ##poli        | O      |\n",
      "| 30 | ”             | O      |\n",
      "| 31 | vale          | O      |\n",
      "| 32 | ##vole        | O      |\n",
      "| 33 | per           | O      |\n",
      "| 34 | un            | O      |\n",
      "| 35 | quinqu        | O      |\n",
      "| 36 | ##ennio       | O      |\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Process training+dev data\n",
    "print(\"Processing TRAIN+DEV data...\")\n",
    "for i, entry in enumerate(all_sentences):\n",
    "    text = entry[\"sentence_text\"]\n",
    "    terms = entry[\"terms\"]\n",
    "\n",
    "    tokens, ner_tags = create_ner_tags(text, terms, tokenizer, label2id)\n",
    "    entry[\"tokens\"] = tokens\n",
    "    entry[\"ner_tags\"] = ner_tags\n",
    "\n",
    "    if i % 1000 == 0:\n",
    "        print(f\"  Processed {i}/{len(all_sentences)}\")\n",
    "\n",
    "print(f\"✓ TRAIN+DEV data processed: {len(all_sentences)} sentences\")\n",
    "\n",
    "print(f\"\\nSample sentence:\")\n",
    "print(f\"  Text: {all_sentences[6]['sentence_text']}\")\n",
    "print(f\"  Terms: {all_sentences[6]['terms']}\")\n",
    "token_tags = []\n",
    "for token, tag in zip(all_sentences[6]['tokens'], all_sentences[6]['ner_tags']):\n",
    "    token_tags.append((token, id2label[tag]))\n",
    "print(f\"\\n{pd.DataFrame(token_tags, columns=['Token', 'Tag']).to_markdown()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f0740c",
   "metadata": {},
   "source": [
    "## Prepare Dataset for BERT Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "98289860",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T10:53:14.408009Z",
     "start_time": "2025-11-11T10:53:14.377560Z"
    }
   },
   "outputs": [],
   "source": [
    "class TokenClassificationDataset(Dataset):\n",
    "    \"\"\"Dataset per token classification usando i token BERT e le ner_tags già pre-calcolate.\"\"\"\n",
    "    \n",
    "    def __init__(self, sentences, tokenizer, max_length=512):\n",
    "        \"\"\"\n",
    "        sentences: lista di dict (train_sentences / dev_sentences),\n",
    "                   ognuno con 'sentence_text', 'tokens', 'ner_tags'\n",
    "        \"\"\"\n",
    "        self.sentences = sentences\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        entry = self.sentences[idx]\n",
    "        \n",
    "        # subtoken BERT (senza special tokens) e label allineate 1:1\n",
    "        bert_tokens = entry[\"tokens\"]\n",
    "        bert_labels = entry[\"ner_tags\"]  # lista di int (id delle label)\n",
    "\n",
    "        # converti i token in ids\n",
    "        subtoken_ids = self.tokenizer.convert_tokens_to_ids(bert_tokens)\n",
    "\n",
    "        # rispetta il max_length: lasciamo spazio per CLS e SEP\n",
    "        max_subtokens = self.max_length - 2\n",
    "        subtoken_ids = subtoken_ids[:max_subtokens]\n",
    "        bert_labels = bert_labels[:max_subtokens]\n",
    "\n",
    "        # costruisci input_ids con CLS e SEP\n",
    "        input_ids = [self.tokenizer.cls_token_id] + subtoken_ids + [self.tokenizer.sep_token_id]\n",
    "\n",
    "        # mask: 1 per token reali (CLS + subtokens + SEP)\n",
    "        attention_mask = [1] * len(input_ids)\n",
    "\n",
    "        # labels: -100 per CLS/SEP, poi le nostre label\n",
    "        labels = [-100] + bert_labels + [-100]\n",
    "\n",
    "        assert len(input_ids) == len(attention_mask) == len(labels)\n",
    "\n",
    "        # NON facciamo padding qui: ci pensa DataCollatorForTokenClassification\n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(input_ids, dtype=torch.long),\n",
    "            \"attention_mask\": torch.tensor(attention_mask, dtype=torch.long),\n",
    "            \"labels\": torch.tensor(labels, dtype=torch.long),\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d622d3be",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T10:53:15.058372Z",
     "start_time": "2025-11-11T10:53:14.442557Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating training dataset (train+dev)...\n",
      "✓ TRAIN+DEV dataset: 2885 examples\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating training dataset (train+dev)...\")\n",
    "\n",
    "train_dev_dataset = TokenClassificationDataset(\n",
    "    sentences=all_sentences,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=512\n",
    ")\n",
    "\n",
    "print(f\"✓ TRAIN+DEV dataset: {len(train_dev_dataset)} examples\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75ca5f8",
   "metadata": {},
   "source": [
    "## Configure Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ebe321ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Data collator initialized\n"
     ]
    }
   ],
   "source": [
    "# Setup data collator for token classification\n",
    "# Data collator is used to dynamically pad inputs and labels\n",
    "data_collator = DataCollatorForTokenClassification(\n",
    "    tokenizer=tokenizer,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "print(\"✓ Data collator initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "52f622a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqeval.metrics import precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "\n",
    "def align_predictions(predictions, label_ids):\n",
    "    preds = np.argmax(predictions, axis=2)\n",
    "    batch_true = []\n",
    "    batch_pred = []\n",
    "\n",
    "    for pred, lab in zip(preds, label_ids):\n",
    "        true_labels = []\n",
    "        pred_labels = []\n",
    "        for p, l in zip(pred, lab):\n",
    "            if l == -100:\n",
    "                continue\n",
    "            true_labels.append(id2label[l])\n",
    "            pred_labels.append(id2label[p])\n",
    "        batch_true.append(true_labels)\n",
    "        batch_pred.append(pred_labels)\n",
    "\n",
    "    return batch_pred, batch_true\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    y_pred, y_true = align_predictions(logits, labels)\n",
    "\n",
    "    return {\n",
    "        \"precision\": precision_score(y_true, y_pred),\n",
    "        \"recall\":    recall_score(y_true, y_pred),\n",
    "        \"f1\":        f1_score(y_true, y_pred),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b250c850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Training configuration ready (FINAL TRAIN+DEV)\n",
      "  Batch size: 16\n",
      "  Epochs: 7\n",
      "  Learning rate: 2e-05\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=output_model_dir,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=7,\n",
    "    weight_decay=0.01,\n",
    "\n",
    "    # no evaluation --> everything as training\n",
    "    eval_strategy=\"no\",\n",
    "    save_strategy=\"epoch\",   \n",
    "    load_best_model_at_end=False,\n",
    "\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    warmup_ratio=0.1,\n",
    "\n",
    "    logging_steps=100,\n",
    "    save_total_limit=2,\n",
    "    seed=42,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "print(\"✓ Training configuration ready (FINAL TRAIN+DEV)\")\n",
    "print(f\"  Batch size: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"  Epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"  Learning rate: {training_args.learning_rate}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b55df57",
   "metadata": {},
   "source": [
    "## Train BERT Model\n",
    "\n",
    "Note: This cell might take several minutes to run.\n",
    "\n",
    "\n",
    "**Additional configurations to test**\n",
    "- Aggregate training samples per paragraph/document\n",
    "- Change hyperparameters (*learning_rate*, *batch_size*, *num_train_epochs*, *weight_decay*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b6c44051",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Trainer initialized (FINAL TRAIN+DEV)\n",
      "  Training samples: 2885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\super\\AppData\\Local\\Temp\\ipykernel_29536\\549237410.py:3: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dev_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "print(\"✓ Trainer initialized (FINAL TRAIN+DEV)\")\n",
    "print(f\"  Training samples: {len(train_dev_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "900dd7f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Starting model training...\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\super\\Documents\\UniPd\\ATA\\ATE-IT_SofiaMaule\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1267' max='1267' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1267/1267 34:47, Epoch 7/7]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.544200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.182900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.111200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.098900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.071500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.050900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.045600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.035700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.029800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.022100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.018900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.015800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\super\\Documents\\UniPd\\ATA\\ATE-IT_SofiaMaule\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\super\\Documents\\UniPd\\ATA\\ATE-IT_SofiaMaule\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\super\\Documents\\UniPd\\ATA\\ATE-IT_SofiaMaule\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\super\\Documents\\UniPd\\ATA\\ATE-IT_SofiaMaule\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\super\\Documents\\UniPd\\ATA\\ATE-IT_SofiaMaule\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\super\\Documents\\UniPd\\ATA\\ATE-IT_SofiaMaule\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "✓ TRAINING COMPLETED!\n",
      "============================================================\n",
      "Training time: 34.84 minutes\n"
     ]
    }
   ],
   "source": [
    "# Start training\n",
    "print(\"=\"*60)\n",
    "print(\"Starting model training...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "import time\n",
    "training_start_time = time.time()\n",
    "\n",
    "train_result = trainer.train()\n",
    "training_duration = time.time() - training_start_time\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"✓ TRAINING COMPLETED!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Training time: {training_duration/60:.2f} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9d8a7e",
   "metadata": {},
   "source": [
    "## Save Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4be37024",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving trained model...\n",
      "✓ Model saved to: models/bert_token_classification_2e-5_uncased_train_dev\n"
     ]
    }
   ],
   "source": [
    "# Save the trained model\n",
    "print(\"Saving trained model...\")\n",
    "\n",
    "os.makedirs(output_model_dir, exist_ok=True)\n",
    "trainer.save_model(output_model_dir)\n",
    "tokenizer.save_pretrained(output_model_dir)\n",
    "\n",
    "print(f\"✓ Model saved to: {output_model_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca318c0",
   "metadata": {},
   "source": [
    "## Inference Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "25a253ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "test_data = load_jsonl('../../data/test.json')\n",
    "test_sentences = build_sentence_gold_map(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bfb4cef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading trained model for inference...\n",
      "✓ Model loaded from: models/bert_token_classification_2e-5_uncased_train_dev\n"
     ]
    }
   ],
   "source": [
    "# Load the trained model for inference\n",
    "print(\"Loading trained model for inference...\")\n",
    "\n",
    "inference_model = AutoModelForTokenClassification.from_pretrained(output_model_dir)\n",
    "inference_tokenizer = AutoTokenizer.from_pretrained(output_model_dir)\n",
    "inference_model.eval()\n",
    "id2label = inference_model.config.id2label\n",
    "\n",
    "\n",
    "\n",
    "print(f\"✓ Model loaded from: {output_model_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8ef207fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_term(term: str) -> str:\n",
    "    t = term.strip()\n",
    "    # normalizza spazi\n",
    "    t = \" \".join(t.split())\n",
    "    # togli punteggiatura solo ai bordi (non in mezzo)\n",
    "    t = t.strip(string.punctuation + \"«»“”'\\\"\")\n",
    "    return t.lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2699ea53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_inference(model, tokenizer, text: str, id2label: dict) -> list[str]:\n",
    "    \"\"\"\n",
    "    Perform token classification inference on a single text and\n",
    "    extract TERM spans using the BIO scheme.\n",
    "    \"\"\"\n",
    "    # Preprocess text exactly as in training\n",
    "    text = preprocess_text(text)\n",
    "\n",
    "    # Tokenize with offset mapping (we keep it in case we want spans later)\n",
    "    encoded = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        return_offsets_mapping=True\n",
    "    )\n",
    "\n",
    "    # Move tensors to the same device as the model\n",
    "    encoded = {k: v.to(device) if isinstance(v, torch.Tensor) else v\n",
    "               for k, v in encoded.items()}\n",
    "\n",
    "    offset_mapping = encoded.pop(\"offset_mapping\")  # not used directly now\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**encoded)\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.nn.functional.softmax(logits, dim=-1)\n",
    "        predicted_labels = torch.argmax(predictions, dim=-1)\n",
    "\n",
    "    # Convert ids back to tokens and labels\n",
    "    input_ids = encoded[\"input_ids\"][0]\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "    labels = [id2label[p.item()] for p in predicted_labels[0]]\n",
    "\n",
    "    # Extract terms using BIO scheme\n",
    "    predicted_terms = []\n",
    "    current_term_tokens = []\n",
    "\n",
    "    for token, label in zip(tokens, labels):\n",
    "        # Skip special tokens\n",
    "        if token in [tokenizer.cls_token, tokenizer.sep_token, tokenizer.pad_token]:\n",
    "            continue\n",
    "\n",
    "        if label == \"B-TERM\":\n",
    "            # If we were building a previous term, close it\n",
    "            if current_term_tokens:\n",
    "                term_str = tokenizer.convert_tokens_to_string(current_term_tokens)\n",
    "                term_str = clean_term(term_str)\n",
    "                if term_str:\n",
    "                    predicted_terms.append(term_str)\n",
    "            # Start a new term\n",
    "            current_term_tokens = [token]\n",
    "\n",
    "        elif label == \"I-TERM\" and current_term_tokens:\n",
    "            # Continue the current term\n",
    "            current_term_tokens.append(token)\n",
    "\n",
    "        else:\n",
    "            # Outside a term or I-TERM without a current B-TERM\n",
    "            if current_term_tokens:\n",
    "                term_str = tokenizer.convert_tokens_to_string(current_term_tokens)\n",
    "                term_str = clean_term(term_str)\n",
    "                if term_str:\n",
    "                    predicted_terms.append(term_str)\n",
    "                current_term_tokens = []\n",
    "\n",
    "    # Close any pending term at the end\n",
    "    if current_term_tokens:\n",
    "        term_str = tokenizer.convert_tokens_to_string(current_term_tokens)\n",
    "        term_str = clean_term(term_str)\n",
    "        if term_str:\n",
    "            predicted_terms.append(term_str)\n",
    "\n",
    "    return predicted_terms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e6a9127b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running inference on dev set...\n",
      "  Processing 0/1142\n",
      "  Processing 200/1142\n",
      "  Processing 400/1142\n",
      "  Processing 600/1142\n",
      "  Processing 800/1142\n",
      "  Processing 1000/1142\n",
      "Inference completed: 1142 predictions\n"
     ]
    }
   ],
   "source": [
    "# Run inference on all dev sentences\n",
    "import string\n",
    "\n",
    "print(\"Running inference on dev set...\")\n",
    "bert_preds = []\n",
    "test_sentences = build_sentence_gold_map(test_data)\n",
    "for i, sentence in enumerate(test_sentences):\n",
    "    if i % 200 == 0:\n",
    "        print(f\"  Processing {i}/{len(test_sentences)}\")\n",
    "    \n",
    "    predicted_terms = perform_inference(\n",
    "        inference_model,\n",
    "        inference_tokenizer,\n",
    "        sentence[\"sentence_text\"], \n",
    "        id2label\n",
    "    )\n",
    "    bert_preds.append(predicted_terms)\n",
    "\n",
    "print(f\"Inference completed: {len(bert_preds)} predictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "96a1c3ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Saved 1142 predictions to predictions/subtask_a_dev_bert_preds_train_dev.json\n"
     ]
    }
   ],
   "source": [
    "# Save predictions in competition format\n",
    "def save_predictions(predictions, sentences, output_path):\n",
    "    \"\"\"Save predictions in competition format.\"\"\"\n",
    "    output = {'data': []}\n",
    "    for pred, sent in zip(predictions, sentences):\n",
    "        output['data'].append({\n",
    "            'document_id': sent['document_id'],\n",
    "            'paragraph_id': sent['paragraph_id'],\n",
    "            'sentence_id': sent['sentence_id'],\n",
    "            'term_list': pred\n",
    "        })\n",
    "    \n",
    "    os.makedirs(os.path.dirname(output_path) or '.', exist_ok=True)\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(output, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"✓ Saved {len(predictions)} predictions to {output_path}\")\n",
    "\n",
    "\n",
    "save_predictions(bert_preds, test_sentences, 'predictions/subtask_a_dev_bert_preds_train_dev.json') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "465e42ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMUNE DI AMATO\n",
      "→ []\n",
      "\n",
      "PROVINCIA DI CATANZARO\n",
      "→ []\n",
      "\n",
      "(UFFICIO DEL SINDACO)\n",
      "→ []\n",
      "\n",
      "Via Marconi, 14 – 88040 Amato (CZ)\n",
      "→ []\n",
      "\n",
      "protocollo.amato@asmepec.it - 0961993045\n",
      "→ []\n",
      "\n",
      "NUOVO CALENDARIO RACCOLTA RIFIUTI\n",
      "→ ['calendario raccolta rifiuti']\n",
      "\n",
      "(Decorrenza 12/06/2023)\n",
      "→ []\n",
      "\n",
      "CENTRO ABITATO\n",
      "→ []\n",
      "\n",
      "Lunedì\n",
      "→ []\n",
      "\n",
      "Martedì\n",
      "→ []\n",
      "\n",
      "Mercoledì\n",
      "→ []\n",
      "\n",
      "Giovedì\n",
      "→ []\n",
      "\n",
      "Venerdì\n",
      "→ []\n",
      "\n",
      "\"UMIDO\"\n",
      "→ ['umido']\n",
      "\n",
      "e\n",
      "→ []\n",
      "\n",
      "\"INDIFFERENZIATO\"\n",
      "→ ['indifferenziato']\n",
      "\n",
      "\"VETRO\"\n",
      "→ ['vetro']\n",
      "\n",
      "(1°, 3° ed eventuale 5° martedì del mese)\n",
      "→ []\n",
      "\n",
      "\"UMIDO\"\n",
      "→ ['umido']\n",
      "\n",
      "e\n",
      "→ []\n",
      "\n",
      "\"CARTA/CARTONE\"\n",
      "→ ['carta / cartone']\n",
      "\n",
      "\"MULTIMATERIALE\"\n",
      "→ ['multimateriale']\n",
      "\n",
      "(plastica, alluminio, lattine)\n",
      "→ ['plastica', 'alluminio', 'lattine']\n",
      "\n",
      "\"UMIDO\"\n",
      "→ ['umido']\n",
      "\n",
      "e\n",
      "→ []\n",
      "\n",
      "(Su richiesta)\n",
      "→ []\n",
      "\n",
      "Ausili per l'igiene di bambini, anziani e malati\n",
      "→ []\n",
      "\n",
      "CONTRADE\n",
      "→ []\n",
      "\n",
      "Lunedì\n",
      "→ []\n",
      "\n",
      "Giovedì\n",
      "→ []\n",
      "\n",
      "\"INDIFFERENZIATO\" e \"VETRO\"\n",
      "→ ['indifferenziato', 'vetro']\n",
      "\n",
      "\"MULTIMATERIALE\" (plastica, alluminio, lattine) e \"CARTA/CARTONE\"\n",
      "→ ['multimateriale', 'plastica', 'alluminio', 'carta / cartone']\n",
      "\n",
      "RIFIUTI INGOMBRANTI E RAEE (frigoriferi, congelatori, lavatrici, lavastoviglie, tv e monitor, etc.): il servizio sarà espletato su richiesta solo ed esclusivamente previa prenotazione presso gli uffici comunali.\n",
      "→ ['rifiuti ingombranti e raee']\n",
      "\n",
      "Amato, 29/05/2023\n",
      "→ []\n",
      "\n",
      "[sigillo e firma del Sindaco]\n",
      "→ []\n",
      "\n",
      "Nella plastica devono essere conferiti anche gli imballaggi in plastica delle merendine, di alimenti in genere, etc.\n",
      "→ ['plastica', 'essere conferiti', 'imballaggi in plastica']\n",
      "\n",
      "Non utilizzare mai sacchi neri!\n",
      "→ ['sacchi neri']\n",
      "\n",
      "Sacchi e contenitori dei rifiuti vanno posizionati all'esterno della propria abitazione sul marciapiede o lungo la strada pubblica dopo le ore 21,00 e entro le 24,00 del giorno precedente la raccolta prevista da calendario.\n",
      "→ ['sacchi e contenitori dei rifiuti', 'raccolta']\n",
      "\n",
      "Salute e sicurezza sul lavoro. Si ricorda a tutti gli utenti che il peso massimo dei contenitori o del materiale conferibile non deve superare i 15 Kg.\n",
      "→ ['utenti', 'materiale conferibile']\n",
      "\n",
      "TIPOLOGIA DI RIFIUTO\n",
      "→ ['rifiuto']\n",
      "\n",
      "Beni durevoli / Raee\n",
      "→ ['beni durevoli', 'raee']\n",
      "\n",
      "Haushaltsgeräte / Durable goods\n",
      "→ []\n",
      "\n",
      "- calcolatrici, giochi elettrici\n",
      "→ []\n",
      "\n",
      "- congelatori, frigoriferi, condizionatori\n",
      "→ []\n",
      "\n",
      "- lampade a neon\n",
      "→ []\n",
      "\n",
      "- lampade a risparmio energetico\n",
      "→ []\n",
      "\n",
      "- lavastoviglie e lavatrici\n",
      "→ []\n",
      "\n",
      "- lettori DVD, cellulari, PC, tablet, ecc.\n",
      "→ []\n",
      "\n",
      "- piccoli elettrodomestici\n",
      "→ []\n",
      "\n",
      "- sigarette elettroniche\n",
      "→ []\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(50):\n",
    "    print(test_sentences[i][\"sentence_text\"])\n",
    "    print(\"→\", bert_preds[i])\n",
    "    print()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
