{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52af72d2",
   "metadata": {},
   "source": [
    "# **Term Extraction Ensemble (BERT + spaCy + Dictionary)**\n",
    "\n",
    "This notebook implements a complete post-processing pipeline for the ATE-IT Subtask A (Automatic Term Extraction).  \n",
    "It takes the raw predictions from a fine-tuned **BERT token classification model** and combines them with **spaCy noun-chunk spans** and a **gold-derived domain vocabulary** to produce a higher-quality list of domain terms for each sentence.\n",
    "\n",
    "### Pipeline Summary\n",
    "1. **Load BERT and spaCy predictions**  \n",
    "   - Import model outputs in ATE-IT JSON format.  \n",
    "   - Map predictions to sentence identifiers for easy lookup.\n",
    "\n",
    "2. **Normalize and clean BERT terms**  \n",
    "   - Remove punctuation, unify quotes, lowercase, collapse whitespace.  \n",
    "   - Filter out spurious or generic one-word candidates.\n",
    "\n",
    "3. **Build a domain vocabulary from the gold training set**  \n",
    "   - Normalize gold terms.  \n",
    "   - Track frequencies to identify strong (repeated) vs. weak (rare) terms.\n",
    "\n",
    "4. **Merge BERT + spaCy + Dictionary knowledge**  \n",
    "   - **Upgrade** short BERT terms to longer spaCy spans when they form a valid multi-word expression present in the gold vocabulary.  \n",
    "   - **Add** additional spaCy multi-word spans only if they appear in the gold vocabulary.  \n",
    "   - **Filter out** generic, meaningless, or uninformative unigrams.  \n",
    "   - **Normalize and deduplicate** final terms.\n",
    "\n",
    "5. **Generate final ensemble predictions**  \n",
    "   - For each sentence, produce an improved term list combining all signals.  \n",
    "   - Output saved in ATE-IT JSON format.\n",
    "\n",
    "### Goal\n",
    "The notebook improves recall and precision of automatic term extraction by combining:\n",
    "- contextual predictions (BERT),\n",
    "- linguistic structure (spaCy),\n",
    "- and domain consistency (gold vocabulary).\n",
    "\n",
    "This hybrid ensemble typically outperforms each component alone.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "35713571",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, os\n",
    "def load_json(path: str):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "    \n",
    "\n",
    "def save_json(obj, path: str):\n",
    "    os.makedirs(os.path.dirname(path) or \".\", exist_ok=True)\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(obj, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"✓ Saved cleaned predictions to {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fc351393",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"../data/\"\n",
    "PRED_DIR = \"../src/predictions/\"\n",
    "\n",
    "TRAIN_FILE = os.path.join(DATA_DIR, \"subtask_a_train.json\")\n",
    "DEV_FILE = os.path.join(DATA_DIR, \"subtask_a_dev.json\")\n",
    "\n",
    "# BERT_DEV_PRED_FILE = os.path.join(\n",
    "#     PRED_DIR, \"subtask_a_dev_bert_token_classification_preds_clean.json\"\n",
    "# )\n",
    "BERT_DEV_PRED_FILE = os.path.join(\n",
    "    PRED_DIR, \"subtask_a_dev_bert_preds_2e-5_changed_cleaned.json\"\n",
    ")\n",
    "SPACY_DEV_PRED_FILE = os.path.join(\n",
    "    PRED_DIR, \"subtask_a_dev_spacy_trained_preds.json\"\n",
    ")\n",
    "\n",
    "ENSEMBLE_OUT_FILE = os.path.join(\n",
    "    PRED_DIR, \"subtask_a_dev_ensemble_bert_2e-5_changed_spacy_dictfilter2.json\"\n",
    ")\n",
    "\n",
    "os.makedirs(PRED_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b91e87aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../src/predictions/subtask_a_dev_bert_preds_2e-5_changed_cleaned.json\n",
      "../src/predictions/subtask_a_dev_spacy_trained_preds.json\n"
     ]
    }
   ],
   "source": [
    "print(BERT_DEV_PRED_FILE)\n",
    "print(SPACY_DEV_PRED_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fb6b47ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "\n",
    "def norm(t: str) -> str:\n",
    "    \"\"\"\n",
    "    Canonical normalization used everywhere for:\n",
    "      - train vocabulary\n",
    "      - predictions\n",
    "      - matching / dedup\n",
    "    \"\"\"\n",
    "    if not t:\n",
    "        return \"\"\n",
    "    t = t.lower()\n",
    "    t = unicodedata.normalize(\"NFKC\", t)\n",
    "    t = t.replace(\"’\", \"'\").replace(\"`\", \"'\")\n",
    "    t = t.replace(\"“\", '\"').replace(\"”\", '\"')\n",
    "    t = \" \".join(t.split())\n",
    "    # strip punctuation at boundaries\n",
    "    t = t.strip(\".,;:-'\\\"()[]{}\")\n",
    "    return t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a49f47",
   "metadata": {},
   "source": [
    "\n",
    "## Train vocabulary from gold terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3beb4aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def build_train_vocab(train_data: dict) -> set:\n",
    "    \"\"\"\n",
    "    Build a normalized vocabulary of gold terms from the training set.\n",
    "    Each term is normalized with `norm`.\n",
    "    \"\"\"\n",
    "    vocab = set()\n",
    "    for entry in train_data[\"data\"]:\n",
    "        for term in entry.get(\"term_list\", []):\n",
    "            n = norm(term)\n",
    "            if n:\n",
    "                vocab.add(n)\n",
    "    return vocab\n",
    "\n",
    "\n",
    "def build_term_map(pred_json: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Build a mapping:\n",
    "        (document_id, paragraph_id, sentence_id) -> list of predicted terms\n",
    "    from a prediction JSON in the ATE-IT format.\n",
    "    \"\"\"\n",
    "    m = {}\n",
    "    for e in pred_json[\"data\"]:\n",
    "        key = (e[\"document_id\"], e[\"paragraph_id\"], e[\"sentence_id\"])\n",
    "        m[key] = e.get(\"term_list\", []) or []\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4014db25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def build_train_vocab_with_freq(train_data):\n",
    "    freq = Counter()\n",
    "    for e in train_data[\"data\"]:\n",
    "        for term in e.get(\"term_list\", []):\n",
    "            norm = norm(term)\n",
    "            if norm:\n",
    "                freq[norm] += 1\n",
    "    strong = {t for t, c in freq.items() if c >= 3}\n",
    "    weak   = {t for t, c in freq.items() if c == 1}\n",
    "    return freq, strong, weak\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75ba29d",
   "metadata": {},
   "source": [
    "### Generic / acronyms handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1090afc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "GENERIC_HEADS = {\n",
    "    \"rifiuti\", \"materiali\", \"utenti\", \"plastica\", \"carta\",\n",
    "    \"residui\", \"tariffe\", \"gestore\", \"servizio\", \"modalità\",\n",
    "    \"conferimento\", \"costi\", \"parte\", \"quota\", \"impianto\"\n",
    "}\n",
    "GENERIC_BAD = {\n",
    "    \"parte\", \"gestione\", \"città\", \"territorio\", \"comune\",\n",
    "    \"ore\", \"no\", \"si\", \"anno\", \"mese\", \"giorno\"\n",
    "} \n",
    "def looks_like_acronym(n: str) -> bool:\n",
    "    \"\"\"\n",
    "    Heuristic for acronyms:\n",
    "      - remove dots\n",
    "      - length 2–6\n",
    "      - alphanumeric with at least one letter\n",
    "    E.g. 'tmb', 'raee', 'r.a.e.e', 'tari'.\n",
    "    \"\"\"\n",
    "    n_clean = n.replace(\".\", \"\")\n",
    "    if not (2 <= len(n_clean) <= 6):\n",
    "        return False\n",
    "    # at least one letter\n",
    "    has_letter = any(ch.isalpha() for ch in n_clean)\n",
    "    if not has_letter:\n",
    "        return False\n",
    "    # letters and digits are allowed (for things like R1)\n",
    "    if not all(ch.isalnum() for ch in n_clean):\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8b15a7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_generic_unigrams(terms, train_vocab_norm):\n",
    "    \"\"\"\n",
    "    Filter out generic one-word heads ONLY if:\n",
    "      - they are in GENERIC_HEADS\n",
    "      - they do NOT appear in the gold vocabulary\n",
    "      - they do NOT look like acronyms\n",
    "    All other unigrams are kept.\n",
    "    \"\"\"\n",
    "    filtered = []\n",
    "    for t in terms:\n",
    "        n = norm(t)\n",
    "        tokens = n.split()\n",
    "        if len(tokens) == 1:\n",
    "            if n in GENERIC_HEADS and n not in train_vocab_norm and not looks_like_acronym(n):\n",
    "                # drop generic heads not validated by gold vocab\n",
    "                continue\n",
    "        filtered.append(t)\n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f3c07e",
   "metadata": {},
   "source": [
    "### Multiword upgrade logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e6b8e8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def contains_as_subspan(longer: str, shorter: str) -> bool:\n",
    "    \"\"\"\n",
    "    Check if `shorter` (already normed) appears as a contiguous\n",
    "    token subsequence inside `longer` (already normed).\n",
    "    \"\"\"\n",
    "    long_tokens = longer.split()\n",
    "    short_tokens = shorter.split()\n",
    "    L, S = len(long_tokens), len(short_tokens)\n",
    "    if S > L or S == 0:\n",
    "        return False\n",
    "    for i in range(L - S + 1):\n",
    "        if long_tokens[i:i+S] == short_tokens:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "64f161d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upgrade_with_longer_spacy(bert_terms, spacy_terms, train_vocab_norm):\n",
    "    \"\"\"\n",
    "    Upgrade BERT terms to longer spaCy spans ONLY WHEN BENEFICIAL.\n",
    "\n",
    "    For each BERT term:\n",
    "      - search among spaCy spans:\n",
    "          * multiword (len >= 2)\n",
    "          * present in train_vocab_norm\n",
    "          * whose tokens contain the BERT term tokens as a subspan\n",
    "      - pick the longest such span if any, otherwise keep BERT term.\n",
    "\n",
    "    This is what helps recover multiword FNs like\n",
    "    'materiali ferrosi', 'modalità di conferimento', ecc.\n",
    "    \"\"\"\n",
    "    final = []\n",
    "    seen = set()\n",
    "    \n",
    "    spacy_norm_map = {norm(t): t for t in (spacy_terms or [])}\n",
    "\n",
    "    for b in bert_terms or []:\n",
    "        b_norm = norm(b)\n",
    "        if not b_norm or b_norm in GENERIC_BAD:\n",
    "            continue\n",
    "\n",
    "        best = None\n",
    "\n",
    "        for s_norm, s in spacy_norm_map.items():\n",
    "            # only multiword spaCy spans\n",
    "            if len(s_norm.split()) < 2:\n",
    "                continue\n",
    "            # keep only spans that appear as gold terms\n",
    "            if s_norm not in train_vocab_norm:\n",
    "                continue\n",
    "            # check if BERT term tokens appear inside the spaCy span\n",
    "            if contains_as_subspan(s_norm, b_norm):\n",
    "                if best is None or len(s_norm.split()) > len(norm(best).split()):\n",
    "                    best = s\n",
    "\n",
    "        chosen = best if best else b\n",
    "        c_norm = norm(chosen)\n",
    "\n",
    "        if c_norm not in seen and c_norm not in GENERIC_BAD:\n",
    "            final.append(chosen)\n",
    "            seen.add(c_norm)\n",
    "    return final\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658add3f",
   "metadata": {},
   "source": [
    "\n",
    "### Dictionary-based multiword mining (train vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7b925deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def find_dict_multiwords_in_sentence(sentence_text: str, train_vocab_norm: set):\n",
    "    \"\"\"\n",
    "    Use the train gold vocabulary as a gazetteer:\n",
    "    - normalize sentence_text\n",
    "    - return all multiword terms from train_vocab_norm whose tokens\n",
    "      appear as a contiguous subsequence in the sentence.\n",
    "    \"\"\"\n",
    "    sent_norm = norm(sentence_text)\n",
    "    sent_tokens = sent_norm.split()\n",
    "    results = []\n",
    "\n",
    "    for term in train_vocab_norm:\n",
    "        # focus on multiword only\n",
    "        term_tokens = term.split()\n",
    "        if len(term_tokens) < 2:\n",
    "            continue\n",
    "        if contains_as_subspan(sent_norm, term):\n",
    "            results.append(term)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d948eb5e",
   "metadata": {},
   "source": [
    "### Merge BERT + spaCy + dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "32bb4936",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_bert_spacy_with_dict(bert_terms, spacy_terms, train_vocab_norm):\n",
    "    \"\"\"\n",
    "    Ensemble strategy:\n",
    "      1) Upgrade BERT spans to longer valid spaCy spans when possible.\n",
    "      2) Add extra spaCy multiword spans if:\n",
    "           - multiword\n",
    "           - present in train gold vocabulary\n",
    "           - not already included (by normalized form)\n",
    "           - not in GENERIC_BAD\n",
    "    \"\"\"\n",
    "    upgraded = upgrade_with_longer_spacy(\n",
    "        bert_terms=bert_terms,\n",
    "        spacy_terms=spacy_terms,\n",
    "        train_vocab_norm=train_vocab_norm,\n",
    "    )\n",
    "\n",
    "    final = upgraded[:]\n",
    "    seen = {norm(t) for t in upgraded}\n",
    "\n",
    "    for s in spacy_terms or []:\n",
    "        s_norm = norm(s)\n",
    "\n",
    "        # only multiword additions here\n",
    "        if len(s_norm.split()) < 2:\n",
    "            continue\n",
    "        if s_norm not in train_vocab_norm:\n",
    "            continue\n",
    "        if s_norm in seen:\n",
    "            continue\n",
    "        if s_norm in GENERIC_BAD:\n",
    "            continue\n",
    "\n",
    "        final.append(s)\n",
    "        seen.add(s_norm)\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "35d36815",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_sentence(bert_terms, spacy_terms, train_vocab_norm, sentence_text: str):\n",
    "    \"\"\"\n",
    "    High-level per-sentence merge:\n",
    "      - combine BERT and spaCy with vocab constraints\n",
    "      - add dictionary multiwords that appear in the sentence\n",
    "      - filter generic unigrams (but keep all terms seen in gold)\n",
    "      - normalize and deduplicate.\n",
    "    \"\"\"\n",
    "    # 1) merge BERT + spaCy\n",
    "    merged = merge_bert_spacy_with_dict(\n",
    "        bert_terms=bert_terms,\n",
    "        spacy_terms=spacy_terms,\n",
    "        train_vocab_norm=train_vocab_norm,\n",
    "    )\n",
    "\n",
    "    # 2) gazetteer: add multiword from train vocab that appear in the sentence\n",
    "    dict_terms = find_dict_multiwords_in_sentence(sentence_text, train_vocab_norm)\n",
    "    merged.extend(dict_terms)\n",
    "\n",
    "    # 3) remove only truly generic unigrams\n",
    "    merged = filter_generic_unigrams(merged, train_vocab_norm)\n",
    "\n",
    "    # 4) dedupe by normalized form\n",
    "    seen = set()\n",
    "    final = []\n",
    "    for t in merged:\n",
    "        n = norm(t)\n",
    "        if n not in seen:\n",
    "            final.append(n)   # output normalized term\n",
    "            seen.add(n)\n",
    "    return final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "47d23d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "                   \n",
    "def micro_f1_score(gold_standard, system_output):\n",
    "  \"\"\"\n",
    "  Evaluates a term extraction system's performance using Precision, Recall,\n",
    "  and F1 score based on individual term matching (micro-average).\n",
    "\n",
    "  Args:\n",
    "    gold_standard: A list of lists, where each inner list contains the\n",
    "        gold standard terms for an item.\n",
    "    system_output: A list of lists, where each inner list contains the\n",
    "                   terms extracted by the system for the corresponding item.\n",
    "\n",
    "  Returns:\n",
    "    A tuple containing the Precision, Recall, and F1 score.\n",
    "  \"\"\"\n",
    "  total_true_positives = 0\n",
    "  total_false_positives = 0\n",
    "  total_false_negatives = 0\n",
    "\n",
    "  # Iterate through each item's gold standard and system output terms\n",
    "  for gold, system in zip(gold_standard, system_output):\n",
    "    # Convert to sets for efficient comparison\n",
    "    gold_set = set(gold)\n",
    "    system_set = set(system)\n",
    "\n",
    "    # Calculate True Positives, False Positives, and False Negatives for the current item\n",
    "    true_positives = len(gold_set.intersection(system_set))\n",
    "    false_positives = len(system_set - gold_set)\n",
    "    false_negatives = len(gold_set - system_set)\n",
    "\n",
    "    # Accumulate totals across all items\n",
    "    total_true_positives += true_positives\n",
    "    total_false_positives += false_positives\n",
    "    total_false_negatives += false_negatives\n",
    "\n",
    "  # Calculate Precision, Recall, and F1 score (micro-average)\n",
    "  precision = total_true_positives / (total_true_positives + total_false_positives) if (total_true_positives + total_false_positives) > 0 else 0\n",
    "  recall = total_true_positives / (total_true_positives + total_false_negatives) if (total_true_positives + total_false_negatives) > 0 else 0\n",
    "  f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "  return precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1536f7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def type_f1_score(gold_standard, system_output):\n",
    "  \"\"\"\n",
    "  Evaluates a term extraction system's performance using Type Precision,\n",
    "  Type Recall, and Type F1 score based on the set of unique terms extracted\n",
    "  at least once across the entire dataset.\n",
    "\n",
    "  Args:\n",
    "    gold_standard: A list of lists, where each inner list contains the\n",
    "                   gold standard terms for an item.\n",
    "    system_output: A list of lists, where each inner list contains the\n",
    "                   terms extracted by the system for the corresponding item.\n",
    "\n",
    "  Returns:\n",
    "    A tuple containing the Type Precision, Type Recall, and Type F1 score.\n",
    "  \"\"\"\n",
    "\n",
    "  # Get the set of all unique gold standard terms across the dataset\n",
    "  all_gold_terms = set()\n",
    "  for item_terms in gold_standard:\n",
    "    all_gold_terms.update(item_terms)\n",
    "\n",
    "  # Get the set of all unique system extracted terms across the dataset\n",
    "  all_system_terms = set()\n",
    "  for item_terms in system_output:\n",
    "    all_system_terms.update(item_terms)\n",
    "\n",
    "  # Calculate True Positives (terms present in both sets)\n",
    "  type_true_positives = len(all_gold_terms.intersection(all_system_terms))\n",
    "\n",
    "  # Calculate False Positives (terms in system output but not in gold standard)\n",
    "  type_false_positives = len(all_system_terms - all_gold_terms)\n",
    "\n",
    "  # Calculate False Negatives (terms in gold standard but not in system output)\n",
    "  type_false_negatives = len(all_gold_terms - all_system_terms)\n",
    "\n",
    "  # Calculate Type Precision, Type Recall, and Type F1 score\n",
    "  type_precision = type_true_positives / (type_true_positives + type_false_positives) if (type_true_positives + type_false_positives) > 0 else 0\n",
    "  type_recall = type_true_positives / (type_true_positives + type_false_negatives) if (type_true_positives + type_false_negatives) > 0 else 0\n",
    "  type_f1 = 2 * (type_precision * type_recall) / (type_precision + type_recall) if (type_precision + type_recall) > 0 else 0\n",
    "\n",
    "  return type_precision, type_recall, type_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9d3d67",
   "metadata": {},
   "source": [
    "###   BUILD BERT + SPACY ENSEMBLE USING merge_sentence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "48532509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# unique normalized terms from train gold: 710\n",
      "Building improved BERT+spaCy ensemble ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/577 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------------------------------\n",
      "Sentence 0\n",
      "TEXT: Non Domestica; CAMPEGGI, DISTRIBUTORI CARBURANTI, PARCHEGGI; 1,22; 4,73 \n",
      "  BERT  : []\n",
      "  SPACY : []\n",
      "  MERGED: []\n",
      "\n",
      "---------------------------------------\n",
      "Sentence 1\n",
      "TEXT: Il presente disciplinare per la gestione dei centri di raccolta comunali è stato redatto ai sensi e per effetto del DM 13/05/2009, pubblicato sulla G.U. n. 165 del 18/07/2009, con il quale sono state apportate le modifiche sostanziali al DM 08/04/2008, Disciplina dei centri di raccolta dei rifiuti urbani raccolti in modo differenziato, come previsto dall'art. 183, comma 7, lettera cc) del Dlgs 3 aprile 2006, n. 152, e ss.mm.ii.\n",
      "  BERT  : ['disciplinare per la gestione dei centri di raccolta comunali', 'centri di raccolta dei rifiuti urbani raccolti in']\n",
      "  SPACY : ['gestione dei centri di raccolta comunali', 'centri di raccolta dei rifiuti urbani raccolti']\n",
      "  MERGED: ['disciplinare per la gestione dei centri di raccolta comunali', 'centri di raccolta dei rifiuti urbani raccolti in', 'raccolta dei rifiuti urbani', 'rifiuti urbani', 'raccolta dei rifiuti', 'centri di raccolta', 'disciplina dei centri di raccolta', 'centri di raccolta comunali']\n",
      "\n",
      "---------------------------------------\n",
      "Sentence 2\n",
      "TEXT: Voti astenuti: 2 (Acampora Alessandro, Gargiulo Mario)\n",
      "  BERT  : []\n",
      "  SPACY : []\n",
      "  MERGED: []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 577/577 [00:00<00:00, 941.62it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "# ---- Load train data and build vocabulary ----\n",
    "with open(TRAIN_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "    train_data = json.load(f)\n",
    "\n",
    "train_vocab_norm = build_train_vocab(train_data)\n",
    "print(f\"# unique normalized terms from train gold: {len(train_vocab_norm)}\")\n",
    "\n",
    "# ---- Load dev gold (for evaluation) ----\n",
    "with open(DEV_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "    dev_data = json.load(f)\n",
    "\n",
    "# ---- Load BERT and spaCy predictions ----\n",
    "with open(BERT_DEV_PRED_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "    bert_pred = json.load(f)\n",
    "\n",
    "with open(SPACY_DEV_PRED_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "    spacy_pred = json.load(f)\n",
    "\n",
    "# Convert JSON predictions → dict[(doc,par,sent)] → [terms...]\n",
    "bert_map = build_term_map(bert_pred)\n",
    "spacy_map = build_term_map(spacy_pred)\n",
    "\n",
    "# ---- Build ensemble predictions using merge_sentence ----\n",
    "ensemble_output = {\"data\": []}\n",
    "\n",
    "print(\"Building improved BERT+spaCy ensemble ...\")\n",
    "\n",
    "for idx, row in enumerate(tqdm(dev_data[\"data\"])):\n",
    "\n",
    "    key = (row[\"document_id\"], row[\"paragraph_id\"], row[\"sentence_id\"])\n",
    "\n",
    "    bert_terms = bert_map.get(key, []) or []\n",
    "    spacy_terms = spacy_map.get(key, []) or []\n",
    "\n",
    "    merged_terms = merge_sentence(\n",
    "        bert_terms=bert_terms,\n",
    "        spacy_terms=spacy_terms,\n",
    "        train_vocab_norm=train_vocab_norm,\n",
    "        sentence_text=row[\"sentence_text\"],\n",
    "    )\n",
    "\n",
    "    # Debug on first 3\n",
    "    if idx < 3:\n",
    "        print(\"\\n---------------------------------------\")\n",
    "        print(\"Sentence\", idx)\n",
    "        print(\"TEXT:\", row[\"sentence_text\"])\n",
    "        print(\"  BERT  :\", bert_terms)\n",
    "        print(\"  SPACY :\", spacy_terms)\n",
    "        print(\"  MERGED:\", merged_terms)\n",
    "\n",
    "    ensemble_output[\"data\"].append({\n",
    "        \"document_id\": row[\"document_id\"],\n",
    "        \"paragraph_id\": row[\"paragraph_id\"],\n",
    "        \"sentence_id\": row[\"sentence_id\"],\n",
    "        \"term_list\": merged_terms,\n",
    "    })\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5f2d4e",
   "metadata": {},
   "source": [
    "#### Save predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "921fb996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ensemble predictions saved to: ../src/predictions/subtask_a_dev_ensemble_bert_2e-5_changed_spacy_dictfilter2.json\n"
     ]
    }
   ],
   "source": [
    "# ---- Save final merged predictions ----\n",
    "with open(ENSEMBLE_OUT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(ensemble_output, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"\\nEnsemble predictions saved to: {ENSEMBLE_OUT_FILE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7162f311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=====================================================\n",
      "    IMPROVED BERT + SPACY + DICTIONARY MERGE\n",
      "=====================================================\n",
      "\n",
      "Micro-averaged Metrics:\n",
      "  Precision: 0.6875\n",
      "  Recall:    0.7561\n",
      "  F1 Score:  0.7202\n",
      "\n",
      "Type-level Metrics:\n",
      "  Type Precision: 0.6667\n",
      "  Type Recall:    0.7025\n",
      "  Type F1 Score:  0.6841\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Extract gold + predicted lists\n",
    "dev_gold = [entry[\"term_list\"] for entry in dev_data[\"data\"]]\n",
    "ensemble_preds = [entry[\"term_list\"] for entry in ensemble_output[\"data\"]]\n",
    "\n",
    "precision, recall, f1 = micro_f1_score(dev_gold, ensemble_preds)\n",
    "type_precision, type_recall, type_f1 = type_f1_score(dev_gold, ensemble_preds)\n",
    "\n",
    "print(\"\\n=====================================================\")\n",
    "print(\"    IMPROVED BERT + SPACY + DICTIONARY MERGE\")\n",
    "print(\"=====================================================\")\n",
    "\n",
    "print(\"\\nMicro-averaged Metrics:\")\n",
    "print(f\"  Precision: {precision:.4f}\")\n",
    "print(f\"  Recall:    {recall:.4f}\")\n",
    "print(f\"  F1 Score:  {f1:.4f}\")\n",
    "\n",
    "print(\"\\nType-level Metrics:\")\n",
    "print(f\"  Type Precision: {type_precision:.4f}\")\n",
    "print(f\"  Type Recall:    {type_recall:.4f}\")\n",
    "print(f\"  Type F1 Score:  {type_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b0d24159",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def get_fp_fn_from_listformat(gold_entries, pred_entries):\n",
    "    \"\"\"\n",
    "    gold_entries: list of rows from dev_data[\"data\"]\n",
    "    pred_entries: list of rows from ensemble_output[\"data\"]\n",
    "    \n",
    "    Each entry has:\n",
    "        - document_id\n",
    "        - paragraph_id\n",
    "        - sentence_id\n",
    "        - term_list (list of terms)\n",
    "    \n",
    "    Returns DataFrames:\n",
    "        fp_df (false positives)\n",
    "        fn_df (false negatives)\n",
    "    \"\"\"\n",
    "\n",
    "    gold_rows = []\n",
    "    pred_rows = []\n",
    "\n",
    "    # --- Expand GOLD ---\n",
    "    for e in gold_entries:\n",
    "        doc = e[\"document_id\"]\n",
    "        par = e[\"paragraph_id\"]\n",
    "        sid = e[\"sentence_id\"]\n",
    "        for t in e[\"term_list\"]:\n",
    "            t_norm = norm(t)\n",
    "            if t_norm:\n",
    "                gold_rows.append((doc, par, sid, t_norm))\n",
    "\n",
    "    # --- Expand PRED ---\n",
    "    for e in pred_entries:\n",
    "        doc = e[\"document_id\"]\n",
    "        par = e[\"paragraph_id\"]\n",
    "        sid = e[\"sentence_id\"]\n",
    "        for t in e[\"term_list\"]:\n",
    "            t_norm = norm(t)\n",
    "            if t_norm:\n",
    "                pred_rows.append((doc, par, sid, t_norm))\n",
    "\n",
    "    gold_set = set(gold_rows)\n",
    "    pred_set = set(pred_rows)\n",
    "\n",
    "    fp = pred_set - gold_set\n",
    "    fn = gold_set - pred_set\n",
    "\n",
    "    fp_df = pd.DataFrame(list(fp),\n",
    "                         columns=[\"document_id\", \"paragraph_id\", \"sentence_id\", \"term\"])\n",
    "    fn_df = pd.DataFrame(list(fn),\n",
    "                         columns=[\"document_id\", \"paragraph_id\", \"sentence_id\", \"term\"])\n",
    "\n",
    "    return fp_df, fn_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7c99fdde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False Positives: 155\n",
      "False Negatives: 110\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document_id</th>\n",
       "      <th>paragraph_id</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>term</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>doc_battipaglia_13</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>manutenzione verde pubblico</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>doc_salerno_06</td>\n",
       "      <td>64</td>\n",
       "      <td>1</td>\n",
       "      <td>utenze</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>doc_santegidiodelmontealbino_03</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>esporre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>doc_salerno_05</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>vanno conferiti</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>doc_praiano_07</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>superficie ka</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>doc_sorrento_10</td>\n",
       "      <td>28</td>\n",
       "      <td>2</td>\n",
       "      <td>parte variabile</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>doc_capaccio_28</td>\n",
       "      <td>14</td>\n",
       "      <td>5</td>\n",
       "      <td>concorrente</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>doc_sorrento_05</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>gestione dei rifiuti</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>doc_poggiomarino_12</td>\n",
       "      <td>17</td>\n",
       "      <td>5</td>\n",
       "      <td>carta chimica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>doc_salerno_06</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>svuotamento</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>doc_salerno_06</td>\n",
       "      <td>48</td>\n",
       "      <td>2</td>\n",
       "      <td>differenziata</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>doc_nola_02</td>\n",
       "      <td>19</td>\n",
       "      <td>2</td>\n",
       "      <td>incendi dei rifiuti</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>doc_sorrento_22</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>ferro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>doc_sorrento_16</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>pulizia dei fondali</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>doc_praiano_02</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>lattine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>doc_nocerainferiore_06</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>operatori</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>doc_praiano_13</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>raccolta rifiuti</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>doc_battipaglia_13</td>\n",
       "      <td>15</td>\n",
       "      <td>5</td>\n",
       "      <td>raccolta dei rifiuti urbani</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>doc_capaccio_28</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>gestione rifiuti</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>doc_caserta_06</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>centri di raccolta dei rifiuti urbani raccolti in</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         document_id  paragraph_id  sentence_id  \\\n",
       "135               doc_battipaglia_13            20            0   \n",
       "136                   doc_salerno_06            64            1   \n",
       "137  doc_santegidiodelmontealbino_03            12            2   \n",
       "138                   doc_salerno_05             7            6   \n",
       "139                   doc_praiano_07            21            1   \n",
       "140                  doc_sorrento_10            28            2   \n",
       "141                  doc_capaccio_28            14            5   \n",
       "142                  doc_sorrento_05             3            0   \n",
       "143              doc_poggiomarino_12            17            5   \n",
       "144                   doc_salerno_06            27            1   \n",
       "145                   doc_salerno_06            48            2   \n",
       "146                      doc_nola_02            19            2   \n",
       "147                  doc_sorrento_22             2            0   \n",
       "148                  doc_sorrento_16             1            0   \n",
       "149                   doc_praiano_02             8            1   \n",
       "150           doc_nocerainferiore_06             5            1   \n",
       "151                   doc_praiano_13             3            1   \n",
       "152               doc_battipaglia_13            15            5   \n",
       "153                  doc_capaccio_28             3            0   \n",
       "154                   doc_caserta_06             3            1   \n",
       "\n",
       "                                                  term  \n",
       "135                        manutenzione verde pubblico  \n",
       "136                                             utenze  \n",
       "137                                            esporre  \n",
       "138                                    vanno conferiti  \n",
       "139                                      superficie ka  \n",
       "140                                    parte variabile  \n",
       "141                                        concorrente  \n",
       "142                               gestione dei rifiuti  \n",
       "143                                      carta chimica  \n",
       "144                                        svuotamento  \n",
       "145                                      differenziata  \n",
       "146                                incendi dei rifiuti  \n",
       "147                                              ferro  \n",
       "148                                pulizia dei fondali  \n",
       "149                                            lattine  \n",
       "150                                          operatori  \n",
       "151                                   raccolta rifiuti  \n",
       "152                        raccolta dei rifiuti urbani  \n",
       "153                                   gestione rifiuti  \n",
       "154  centri di raccolta dei rifiuti urbani raccolti in  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document_id</th>\n",
       "      <th>paragraph_id</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>term</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>doc_caserta_02</td>\n",
       "      <td>64</td>\n",
       "      <td>9</td>\n",
       "      <td>r.a.e.e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>doc_nocerainferiore_06</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>campana</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>doc_praiano_05</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>raccolta/ritiro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>doc_sorrento_10</td>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "      <td>ka</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>doc_santegidiodelmontealbino_03</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>allumino</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>doc_salerno_06</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>interruzione programmata del servizio</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>doc_marigliano_01</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>abbandono di rifiuti non pericolosi e non ingo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>doc_santegidiodelmontealbino_03</td>\n",
       "      <td>48</td>\n",
       "      <td>0</td>\n",
       "      <td>conferire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>doc_salerno_06</td>\n",
       "      <td>12</td>\n",
       "      <td>15</td>\n",
       "      <td>autorizzazione unica ambientale</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>doc_auletta_13</td>\n",
       "      <td>36</td>\n",
       "      <td>1</td>\n",
       "      <td>gestore dello spazzamento e lavaggio delle strade</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>doc_poggiomarino_12</td>\n",
       "      <td>17</td>\n",
       "      <td>65</td>\n",
       "      <td>r3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>doc_poggiomarino_12</td>\n",
       "      <td>23</td>\n",
       "      <td>2</td>\n",
       "      <td>gestori del centro di raccolta differenziata</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>doc_poggiomarino_12</td>\n",
       "      <td>17</td>\n",
       "      <td>65</td>\n",
       "      <td>frigoriferi e sistemi per il condizionamento</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>doc_poggiomarino_12</td>\n",
       "      <td>17</td>\n",
       "      <td>65</td>\n",
       "      <td>r1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>doc_poggiomarino_01</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>vetro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>doc_caserta_06</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>rifiuti conferibili</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>doc_poggiomarino_01</td>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "      <td>contenitori siglati t/f/x</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>doc_capaccio_06</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>grassi animali</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>doc_nola_02</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>totale annuale conferito</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>doc_salerno_03</td>\n",
       "      <td>2</td>\n",
       "      <td>25</td>\n",
       "      <td>analisi merceologiche</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         document_id  paragraph_id  sentence_id  \\\n",
       "90                    doc_caserta_02            64            9   \n",
       "91            doc_nocerainferiore_06             4            0   \n",
       "92                    doc_praiano_05            10            0   \n",
       "93                   doc_sorrento_10            60            1   \n",
       "94   doc_santegidiodelmontealbino_03            15            0   \n",
       "95                    doc_salerno_06            12            5   \n",
       "96                 doc_marigliano_01             9            1   \n",
       "97   doc_santegidiodelmontealbino_03            48            0   \n",
       "98                    doc_salerno_06            12           15   \n",
       "99                    doc_auletta_13            36            1   \n",
       "100              doc_poggiomarino_12            17           65   \n",
       "101              doc_poggiomarino_12            23            2   \n",
       "102              doc_poggiomarino_12            17           65   \n",
       "103              doc_poggiomarino_12            17           65   \n",
       "104              doc_poggiomarino_01            11            1   \n",
       "105                   doc_caserta_06             6            3   \n",
       "106              doc_poggiomarino_01             8           11   \n",
       "107                  doc_capaccio_06             6            4   \n",
       "108                      doc_nola_02             8            9   \n",
       "109                   doc_salerno_03             2           25   \n",
       "\n",
       "                                                  term  \n",
       "90                                             r.a.e.e  \n",
       "91                                             campana  \n",
       "92                                     raccolta/ritiro  \n",
       "93                                                  ka  \n",
       "94                                            allumino  \n",
       "95               interruzione programmata del servizio  \n",
       "96   abbandono di rifiuti non pericolosi e non ingo...  \n",
       "97                                           conferire  \n",
       "98                     autorizzazione unica ambientale  \n",
       "99   gestore dello spazzamento e lavaggio delle strade  \n",
       "100                                                 r3  \n",
       "101       gestori del centro di raccolta differenziata  \n",
       "102       frigoriferi e sistemi per il condizionamento  \n",
       "103                                                 r1  \n",
       "104                                              vetro  \n",
       "105                                rifiuti conferibili  \n",
       "106                          contenitori siglati t/f/x  \n",
       "107                                     grassi animali  \n",
       "108                           totale annuale conferito  \n",
       "109                              analisi merceologiche  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gold_entries = dev_data[\"data\"]      # gold JSON\n",
    "pred_entries = ensemble_output[\"data\"]  # merged predictions JSON\n",
    "\n",
    "fp_df, fn_df = get_fp_fn_from_listformat(gold_entries, pred_entries)\n",
    "\n",
    "print(\"False Positives:\", len(fp_df))\n",
    "print(\"False Negatives:\", len(fn_df))\n",
    "\n",
    "display(fp_df.tail(20))\n",
    "display(fn_df.tail(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c075f7c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error table shape: (132, 10)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document_id</th>\n",
       "      <th>paragraph_id</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>n_gold</th>\n",
       "      <th>n_pred</th>\n",
       "      <th>n_missing</th>\n",
       "      <th>missing_terms</th>\n",
       "      <th>n_extra</th>\n",
       "      <th>extra_terms</th>\n",
       "      <th>sentence_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>doc_caserta_06</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>[disciplina dei centri di raccolta dei rifiuti...</td>\n",
       "      <td>7</td>\n",
       "      <td>[centri di raccolta, centri di raccolta comuna...</td>\n",
       "      <td>Il presente disciplinare per la gestione dei c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>doc_poggiomarino_01</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[raccolta]</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>È un Servizio Supplementare di raccolta, rivol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>doc_nola_05</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>3</td>\n",
       "      <td>[raccolta dei rifiuti, servizio di raccolta, s...</td>\n",
       "      <td>ll servizio di raccolta dei rifiuti derivanti ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>doc_poggiomarino_12</td>\n",
       "      <td>17</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "      <td>[carta]</td>\n",
       "      <td>- giornali; - la carta per alimenti;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>doc_capaccio_10</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[sacchetto trasparente]</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>MULTIMATERIALE; Sacchetto blu trasparente; Lun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>doc_salerno_05</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "      <td>[tessuto]</td>\n",
       "      <td>Indumenti usati, accessori, lenzuola, coperte,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>doc_capaccio_15</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "      <td>[utenze turistiche]</td>\n",
       "      <td>UTENZE TURISTICHE NON DOMESTICHE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>doc_caserta_06</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[gestione del centro di raccolta]</td>\n",
       "      <td>1</td>\n",
       "      <td>[centro di raccolta]</td>\n",
       "      <td>- alla vigilanza nel rispetto delle norme del ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>doc_poggiomarino_02</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "      <td>[secco residuale]</td>\n",
       "      <td>Secco residuale non riciclabile</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>doc_capaccio_15</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>2</td>\n",
       "      <td>[accumulatori al piombo, utenze domestiche]</td>\n",
       "      <td>PILE PORTATILI, BATTERIE E ACCUMULATORI AL PIO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>doc_nola_05</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[ritiro]</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>RITIRO FRAZIONE VERDE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>doc_salerno_05</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>[conferiti]</td>\n",
       "      <td>5</td>\n",
       "      <td>[acciaio, alluminio, cartoni, plastica, vanno ...</td>\n",
       "      <td>I cartoni per liquidi vanno conferiti con plas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>doc_santegidiodelmontealbino_03</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "      <td>[scarti e avanzi]</td>\n",
       "      <td>- scarti e avanzi di frutta, ortaggi e verdura;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>doc_sorrento_10</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "      <td>[tassa sui rifiuti]</td>\n",
       "      <td>OGGETTO: APPROVAZIONE PIANO TARIFFARIO TASSA S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>doc_salerno_03</td>\n",
       "      <td>2</td>\n",
       "      <td>27</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[sacchetti, spazzatura]</td>\n",
       "      <td>1</td>\n",
       "      <td>[materiali finiti]</td>\n",
       "      <td>Risultati di rilievo se si considera che a set...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>doc_santegidiodelmontealbino_03</td>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "      <td>[depositare]</td>\n",
       "      <td>In caso di abitazioni condominiali depositare ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>doc_capaccio_15</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "      <td>[sacchetti]</td>\n",
       "      <td>Carta: sacchetti, giornali, riviste, libri, qu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>doc_nola_02</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>[responsabile del centro di raccolta]</td>\n",
       "      <td>2</td>\n",
       "      <td>[centro di raccolta, responsabile del centro]</td>\n",
       "      <td>a)Nominare il responsabile del centro di racco...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>doc_nocerainferiore_06</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>[calendario utenze domestiche per la raccolta ...</td>\n",
       "      <td>3</td>\n",
       "      <td>[raccolta differenziata, utenze domestiche, ut...</td>\n",
       "      <td>CALENDARIO UTENZE DOMESTICHE PER LA RACCOLTA D...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>doc_marigliano_01</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>[abbandono di rifiuti non pericolosi e non ing...</td>\n",
       "      <td>1</td>\n",
       "      <td>[abbandono di rifiuti non pericolosi]</td>\n",
       "      <td>a) € 51,65 per l'abbandono di rifiuti non peri...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        document_id  paragraph_id  sentence_id  n_gold  \\\n",
       "0                    doc_caserta_06             3            1       2   \n",
       "1               doc_poggiomarino_01             6            1       1   \n",
       "2                       doc_nola_05             2            2       2   \n",
       "3               doc_poggiomarino_12            17            4       0   \n",
       "4                   doc_capaccio_10             3            3       2   \n",
       "5                    doc_salerno_05            11            2       1   \n",
       "6                   doc_capaccio_15             7            1       1   \n",
       "7                    doc_caserta_06             6            2       1   \n",
       "8               doc_poggiomarino_02            15            0       1   \n",
       "9                   doc_capaccio_15             5           12       1   \n",
       "10                      doc_nola_05             2            0       2   \n",
       "11                   doc_salerno_05             7            6       2   \n",
       "12  doc_santegidiodelmontealbino_03             6            4       0   \n",
       "13                  doc_sorrento_10             3            0       1   \n",
       "14                   doc_salerno_03             2           27       3   \n",
       "15  doc_santegidiodelmontealbino_03            15            4       3   \n",
       "16                  doc_capaccio_15            10            4       1   \n",
       "17                      doc_nola_02            12            2       1   \n",
       "18           doc_nocerainferiore_06            10            0       1   \n",
       "19                doc_marigliano_01             9            1       2   \n",
       "\n",
       "    n_pred  n_missing                                      missing_terms  \\\n",
       "0        8          1  [disciplina dei centri di raccolta dei rifiuti...   \n",
       "1        0          1                                         [raccolta]   \n",
       "2        5          0                                                 []   \n",
       "3        1          0                                                 []   \n",
       "4        1          1                            [sacchetto trasparente]   \n",
       "5        2          0                                                 []   \n",
       "6        2          0                                                 []   \n",
       "7        1          1                  [gestione del centro di raccolta]   \n",
       "8        2          0                                                 []   \n",
       "9        3          0                                                 []   \n",
       "10       1          1                                           [ritiro]   \n",
       "11       6          1                                        [conferiti]   \n",
       "12       1          0                                                 []   \n",
       "13       2          0                                                 []   \n",
       "14       2          2                            [sacchetti, spazzatura]   \n",
       "15       4          0                                                 []   \n",
       "16       2          0                                                 []   \n",
       "17       2          1              [responsabile del centro di raccolta]   \n",
       "18       3          1  [calendario utenze domestiche per la raccolta ...   \n",
       "19       1          2  [abbandono di rifiuti non pericolosi e non ing...   \n",
       "\n",
       "    n_extra                                        extra_terms  \\\n",
       "0         7  [centri di raccolta, centri di raccolta comuna...   \n",
       "1         0                                                 []   \n",
       "2         3  [raccolta dei rifiuti, servizio di raccolta, s...   \n",
       "3         1                                            [carta]   \n",
       "4         0                                                 []   \n",
       "5         1                                          [tessuto]   \n",
       "6         1                                [utenze turistiche]   \n",
       "7         1                               [centro di raccolta]   \n",
       "8         1                                  [secco residuale]   \n",
       "9         2        [accumulatori al piombo, utenze domestiche]   \n",
       "10        0                                                 []   \n",
       "11        5  [acciaio, alluminio, cartoni, plastica, vanno ...   \n",
       "12        1                                  [scarti e avanzi]   \n",
       "13        1                                [tassa sui rifiuti]   \n",
       "14        1                                 [materiali finiti]   \n",
       "15        1                                       [depositare]   \n",
       "16        1                                        [sacchetti]   \n",
       "17        2      [centro di raccolta, responsabile del centro]   \n",
       "18        3  [raccolta differenziata, utenze domestiche, ut...   \n",
       "19        1              [abbandono di rifiuti non pericolosi]   \n",
       "\n",
       "                                        sentence_text  \n",
       "0   Il presente disciplinare per la gestione dei c...  \n",
       "1   È un Servizio Supplementare di raccolta, rivol...  \n",
       "2   ll servizio di raccolta dei rifiuti derivanti ...  \n",
       "3                - giornali; - la carta per alimenti;  \n",
       "4   MULTIMATERIALE; Sacchetto blu trasparente; Lun...  \n",
       "5   Indumenti usati, accessori, lenzuola, coperte,...  \n",
       "6                    UTENZE TURISTICHE NON DOMESTICHE  \n",
       "7   - alla vigilanza nel rispetto delle norme del ...  \n",
       "8                     Secco residuale non riciclabile  \n",
       "9   PILE PORTATILI, BATTERIE E ACCUMULATORI AL PIO...  \n",
       "10                              RITIRO FRAZIONE VERDE  \n",
       "11  I cartoni per liquidi vanno conferiti con plas...  \n",
       "12    - scarti e avanzi di frutta, ortaggi e verdura;  \n",
       "13  OGGETTO: APPROVAZIONE PIANO TARIFFARIO TASSA S...  \n",
       "14  Risultati di rilievo se si considera che a set...  \n",
       "15  In caso di abitazioni condominiali depositare ...  \n",
       "16  Carta: sacchetti, giornali, riviste, libri, qu...  \n",
       "17  a)Nominare il responsabile del centro di racco...  \n",
       "18  CALENDARIO UTENZE DOMESTICHE PER LA RACCOLTA D...  \n",
       "19  a) € 51,65 per l'abbandono di rifiuti non peri...  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def build_sentence_error_table(dev_data, pred_data):\n",
    "    \"\"\"\n",
    "    Build a table where each row corresponds to a sentence that has errors.\n",
    "    Coherent with:\n",
    "      - norm()\n",
    "      - collect_sentence_errors()\n",
    "      - merge structure\n",
    "      - (doc, par, sent) key\n",
    "    \"\"\"\n",
    "\n",
    "    # ---- Build gold map ----\n",
    "    gold_map = {}\n",
    "    for e in dev_data[\"data\"]:\n",
    "        key = (e[\"document_id\"], e[\"paragraph_id\"], e[\"sentence_id\"])\n",
    "        gold_map[key] = set(norm(t) for t in e.get(\"term_list\", []))\n",
    "\n",
    "    # ---- Build pred map ----\n",
    "    pred_map = {}\n",
    "    for e in pred_data[\"data\"]:\n",
    "        key = (e[\"document_id\"], e[\"paragraph_id\"], e[\"sentence_id\"])\n",
    "        pred_map[key] = set(norm(t) for t in e.get(\"term_list\", []))\n",
    "\n",
    "    # ---- Build rows ----\n",
    "    rows = []\n",
    "    for key in gold_map:\n",
    "        doc, par, sent = key\n",
    "        gold_set = gold_map[key]\n",
    "        pred_set = pred_map.get(key, set())\n",
    "\n",
    "        missing = gold_set - pred_set\n",
    "        extra   = pred_set - gold_set\n",
    "\n",
    "        if missing or extra:\n",
    "            rows.append({\n",
    "                \"document_id\": doc,\n",
    "                \"paragraph_id\": par,\n",
    "                \"sentence_id\": sent,\n",
    "                \"n_gold\": len(gold_set),\n",
    "                \"n_pred\": len(pred_set),\n",
    "                \"n_missing\": len(missing),\n",
    "                \"missing_terms\": sorted(missing),\n",
    "                \"n_extra\": len(extra),\n",
    "                \"extra_terms\": sorted(extra),\n",
    "                \"sentence_text\": next(\n",
    "                    r[\"sentence_text\"] \n",
    "                    for r in dev_data[\"data\"] \n",
    "                    if (r[\"document_id\"], r[\"paragraph_id\"], r[\"sentence_id\"]) == key\n",
    "                )\n",
    "            })\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    return df\n",
    "errors_df = build_sentence_error_table(dev_data, ensemble_output)\n",
    "print(f\"Error table shape: {errors_df.shape}\")\n",
    "errors_df.head(20)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
