{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e376e65c",
   "metadata": {},
   "source": [
    "# NLTK Term Extraction for Italian Text\n",
    "\n",
    "This notebook demonstrates two approaches to term extraction:\n",
    "1. **Baseline**: Simple substring and fuzzy matching\n",
    "2. **Trained**: Statistical model using TF-IDF and collocations\n",
    "\n",
    "Dataset: EvalITA 2025 ATE-IT (Automatic Term Extraction - Italian Testbed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c06f1a",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "300d1ab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import os\n",
    "import pickle\n",
    "import math\n",
    "import difflib\n",
    "from collections import Counter, defaultdict\n",
    "from typing import List, Dict, Set, Tuple\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "from nltk.collocations import BigramCollocationFinder, TrigramCollocationFinder\n",
    "from nltk.metrics import BigramAssocMeasures, TrigramAssocMeasures\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('punkt', quiet=True)\n",
    "print(\"Setup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3292de",
   "metadata": {},
   "source": [
    "## Data Loading and Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c60ae57d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Data loading functions work correctly\n"
     ]
    }
   ],
   "source": [
    "def load_jsonl(path: str) -> List[Dict]:\n",
    "    \"\"\"Load a JSON lines file or JSON array file.\"\"\"\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        text = f.read().strip()\n",
    "    if not text:\n",
    "        return []\n",
    "    try:\n",
    "        # Try parsing as single JSON object/array\n",
    "        data = json.loads(text)\n",
    "    except json.JSONDecodeError:\n",
    "        # Fall back to JSONL (one JSON per line)\n",
    "        data = []\n",
    "        for line in text.splitlines():\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                data.append(json.loads(line))\n",
    "    return data\n",
    "\n",
    "\n",
    "def build_sentence_gold_map(records: List[Dict]) -> List[Dict]:\n",
    "    \"\"\"Convert dataset rows into list of sentences with aggregated terms.\n",
    "    \n",
    "    Handles both formats:\n",
    "    - Records with 'term_list' field (list of terms)\n",
    "    - Records with individual 'term' field (one term per row)\n",
    "    \"\"\"\n",
    "    out = {}\n",
    "    \n",
    "    # Support both dict with 'data' key and plain list\n",
    "    if isinstance(records, dict) and 'data' in records:\n",
    "        rows = records['data']\n",
    "    else:\n",
    "        rows = records\n",
    "    \n",
    "    for r in rows:\n",
    "        key = (r.get('document_id'), r.get('paragraph_id'), r.get('sentence_id'))\n",
    "        if key not in out:\n",
    "            out[key] = {\n",
    "                'document_id': r.get('document_id'),\n",
    "                'paragraph_id': r.get('paragraph_id'),\n",
    "                'sentence_id': r.get('sentence_id'),\n",
    "                'sentence_text': r.get('sentence_text', ''),\n",
    "                'terms': []\n",
    "            }\n",
    "        \n",
    "        # Support both 'term_list' (list) and 'term' (single value)\n",
    "        if isinstance(r.get('term_list'), list):\n",
    "            for t in r.get('term_list'):\n",
    "                if t and t not in out[key]['terms']:\n",
    "                    out[key]['terms'].append(t)\n",
    "        else:\n",
    "            term = r.get('term')\n",
    "            if term and term not in out[key]['terms']:\n",
    "                out[key]['terms'].append(term)\n",
    "    \n",
    "    return list(out.values())\n",
    "\n",
    "\n",
    "# Test: Load a small sample\n",
    "test_data = {\n",
    "    'data': [\n",
    "        {\n",
    "            'document_id': 'doc1',\n",
    "            'paragraph_id': 'p1',\n",
    "            'sentence_id': 's1',\n",
    "            'sentence_text': 'La tassa di successione è un tributo.',\n",
    "            'term_list': ['tassa di successione', 'tributo']\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "test_sentences = build_sentence_gold_map(test_data)\n",
    "assert len(test_sentences) == 1\n",
    "assert test_sentences[0]['terms'] == ['tassa di successione', 'tributo']\n",
    "print(\"✓ Data loading functions work correctly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03053080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training sentences: 2308\n",
      "Dev sentences: 577\n",
      "\n",
      "Example sentence:\n",
      "  Text: AFFIDAMENTO DEL “SERVIZIO DI SPAZZAMENTO, RACCOLTA, TRASPORTO E SMALTIMENTO/RECUPERO DEI RIFIUTI URBANI ED ASSIMILATI E SERVIZI COMPLEMENTARI DELLA CITTA' DI AGROPOLI” VALEVOLE PER UN QUINQUENNIO...\n",
      "  Terms: ['raccolta', 'recupero', 'servizio di raccolta', 'servizio di spazzamento', 'smaltimento', 'trasporto']\n"
     ]
    }
   ],
   "source": [
    "# Load actual training and dev data\n",
    "train_data = load_jsonl('../data/subtask_a_train.json')\n",
    "dev_data = load_jsonl('../data/subtask_a_dev.json')\n",
    "\n",
    "train_sentences = build_sentence_gold_map(train_data)\n",
    "dev_sentences = build_sentence_gold_map(dev_data)\n",
    "\n",
    "print(f\"Training sentences: {len(train_sentences)}\")\n",
    "print(f\"Dev sentences: {len(dev_sentences)}\")\n",
    "print(f\"\\nExample sentence:\")\n",
    "print(f\"  Text: {train_sentences[6]['sentence_text']}...\")\n",
    "print(f\"  Terms: {train_sentences[6]['terms']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac687738",
   "metadata": {},
   "source": [
    "## Evaluation Metrics\n",
    "\n",
    "Using the official evaluation metrics from the competition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eedbc500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Evaluation functions work correctly\n",
      "  Test metrics: P=0.67, R=0.67, F1=0.67\n",
      "  Type metrics: P=0.67, R=0.67, F1=0.67\n"
     ]
    }
   ],
   "source": [
    "def micro_f1_score(gold_standard, system_output):\n",
    "    \"\"\"\n",
    "    Evaluates performance using Precision, Recall, and F1 score \n",
    "    based on individual term matching (micro-average).\n",
    "    \n",
    "    Args:\n",
    "        gold_standard: List of lists, where each inner list contains gold standard terms\n",
    "        system_output: List of lists, where each inner list contains extracted terms\n",
    "    \n",
    "    Returns:\n",
    "        Tuple containing (precision, recall, f1, tp, fp, fn)\n",
    "    \"\"\"\n",
    "    total_true_positives = 0\n",
    "    total_false_positives = 0\n",
    "    total_false_negatives = 0\n",
    "    \n",
    "    # Iterate through each item's gold standard and system output terms\n",
    "    for gold, system in zip(gold_standard, system_output):\n",
    "        # Convert to sets for efficient comparison\n",
    "        gold_set = set(gold)\n",
    "        system_set = set(system)\n",
    "        \n",
    "        # Calculate TP, FP, FN for the current item\n",
    "        true_positives = len(gold_set.intersection(system_set))\n",
    "        false_positives = len(system_set - gold_set)\n",
    "        false_negatives = len(gold_set - system_set)\n",
    "        \n",
    "        # Accumulate totals across all items\n",
    "        total_true_positives += true_positives\n",
    "        total_false_positives += false_positives\n",
    "        total_false_negatives += false_negatives\n",
    "    \n",
    "    # Calculate Precision, Recall, and F1 score (micro-average)\n",
    "    precision = total_true_positives / (total_true_positives + total_false_positives) if (total_true_positives + total_false_positives) > 0 else 0\n",
    "    recall = total_true_positives / (total_true_positives + total_false_negatives) if (total_true_positives + total_false_negatives) > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    return precision, recall, f1, total_true_positives, total_false_positives, total_false_negatives\n",
    "\n",
    "\n",
    "def type_f1_score(gold_standard, system_output):\n",
    "    \"\"\"\n",
    "    Evaluates performance using Type Precision, Type Recall, and Type F1 score\n",
    "    based on the set of unique terms extracted at least once across the entire dataset.\n",
    "    \n",
    "    Args:\n",
    "        gold_standard: List of lists, where each inner list contains gold standard terms\n",
    "        system_output: List of lists, where each inner list contains extracted terms\n",
    "    \n",
    "    Returns:\n",
    "        Tuple containing (type_precision, type_recall, type_f1)\n",
    "    \"\"\"\n",
    "    # Get the set of all unique gold standard terms across the dataset\n",
    "    all_gold_terms = set()\n",
    "    for item_terms in gold_standard:\n",
    "        all_gold_terms.update(item_terms)\n",
    "    \n",
    "    # Get the set of all unique system extracted terms across the dataset\n",
    "    all_system_terms = set()\n",
    "    for item_terms in system_output:\n",
    "        all_system_terms.update(item_terms)\n",
    "    \n",
    "    # Calculate True Positives (terms present in both sets)\n",
    "    type_true_positives = len(all_gold_terms.intersection(all_system_terms))\n",
    "    \n",
    "    # Calculate False Positives (terms in system output but not in gold standard)\n",
    "    type_false_positives = len(all_system_terms - all_gold_terms)\n",
    "    \n",
    "    # Calculate False Negatives (terms in gold standard but not in system output)\n",
    "    type_false_negatives = len(all_gold_terms - all_system_terms)\n",
    "    \n",
    "    # Calculate Type Precision, Type Recall, and Type F1 score\n",
    "    type_precision = type_true_positives / (type_true_positives + type_false_positives) if (type_true_positives + type_false_positives) > 0 else 0\n",
    "    type_recall = type_true_positives / (type_true_positives + type_false_negatives) if (type_true_positives + type_false_negatives) > 0 else 0\n",
    "    type_f1 = 2 * (type_precision * type_recall) / (type_precision + type_recall) if (type_precision + type_recall) > 0 else 0\n",
    "    \n",
    "    return type_precision, type_recall, type_f1\n",
    "\n",
    "\n",
    "# Test: Simple case\n",
    "gold_test = [['term1', 'term2'], ['term3']]\n",
    "pred_test = [['term1', 'term4'], ['term3']]\n",
    "precision, recall, f1, tp, fp, fn = micro_f1_score(gold_test, pred_test)\n",
    "assert tp == 2  # term1 and term3\n",
    "assert fp == 1  # term4\n",
    "assert fn == 1  # term2\n",
    "print(\"✓ Evaluation functions work correctly\")\n",
    "print(f\"  Test metrics: P={precision:.2f}, R={recall:.2f}, F1={f1:.2f}\")\n",
    "\n",
    "# Test type-level metrics\n",
    "type_p, type_r, type_f1 = type_f1_score(gold_test, pred_test)\n",
    "print(f\"  Type metrics: P={type_p:.2f}, R={type_r:.2f}, F1={type_f1:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f8ef64",
   "metadata": {},
   "source": [
    "## Baseline Model: Substring and Fuzzy Matching\n",
    "\n",
    "Simple approach:\n",
    "- Exact substring matching on normalized text\n",
    "- Fuzzy matching with similarity threshold for approximate matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527949fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|██████████| 1/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Baseline model works correctly\n",
      "  Test predictions: ['tassa di successione', 'tributo']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "class NLTKSubstringBaseline:\n",
    "    \"\"\"Baseline using substring and fuzzy matching.\"\"\"\n",
    "    \n",
    "    def __init__(self, threshold: float = 0.8, n_matches: int = 3):\n",
    "        self.terms = []\n",
    "        self.norm_terms = []\n",
    "        self.threshold = threshold  # Fuzzy match threshold\n",
    "        self.n_matches = n_matches  # Max fuzzy matches per token\n",
    "    \n",
    "    def _normalize(self, text: str) -> str:\n",
    "        \"\"\"Lowercase, remove punctuation, normalize whitespace.\"\"\"\n",
    "        text = text.lower()\n",
    "        text = re.sub(r\"[^\\w\\sÀ-ÖØ-öø-ÿ]+\", \" \", text)\n",
    "        text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "        return text\n",
    "    \n",
    "    def build(self, term_list: List[str]):\n",
    "        \"\"\"Prepare term vocabulary.\"\"\"\n",
    "        self.terms = [t for t in term_list if t]\n",
    "        self.norm_terms = [self._normalize(t) for t in self.terms]\n",
    "    \n",
    "    def predict(self, sentences: List[str]) -> List[List[str]]:\n",
    "        \"\"\"Extract terms from sentences.\"\"\"\n",
    "        results = []\n",
    "        for s in tqdm(sentences, desc=\"Predicting\", total=len(sentences)):\n",
    "            ns = self._normalize(s)\n",
    "            found = set()\n",
    "            \n",
    "            # Exact substring match\n",
    "            for orig, nt in zip(self.terms, self.norm_terms):\n",
    "                if nt and nt in ns:\n",
    "                    found.add(orig)\n",
    "            \n",
    "            # Fuzzy matching on tokens\n",
    "            tokens = ns.split()\n",
    "            for t in tokens:\n",
    "                matches = difflib.get_close_matches(\n",
    "                    t, self.norm_terms, \n",
    "                    n=self.n_matches, \n",
    "                    cutoff=self.threshold\n",
    "                )\n",
    "                for m in matches:\n",
    "                    idx = self.norm_terms.index(m)\n",
    "                    found.add(self.terms[idx])\n",
    "            \n",
    "            results.append(sorted(found))\n",
    "        return results\n",
    "    \n",
    "    def save(self, path: str):\n",
    "        \"\"\"Save model to disk.\"\"\"\n",
    "        model_data = {\n",
    "            'terms': self.terms,\n",
    "            'norm_terms': self.norm_terms,\n",
    "            'threshold': self.threshold,\n",
    "            'n_matches': self.n_matches\n",
    "        }\n",
    "        with open(path, 'wb') as f:\n",
    "            pickle.dump(model_data, f)\n",
    "        print(f\"Model saved to {path}\")\n",
    "    \n",
    "    def load(self, path: str):\n",
    "        \"\"\"Load model from disk.\"\"\"\n",
    "        with open(path, 'rb') as f:\n",
    "            model_data = pickle.load(f)\n",
    "        self.terms = model_data['terms']\n",
    "        self.norm_terms = model_data['norm_terms']\n",
    "        self.threshold = model_data['threshold']\n",
    "        self.n_matches = model_data['n_matches']\n",
    "        print(f\"Model loaded from {path}\")\n",
    "\n",
    "\n",
    "# Test: Simple predictions\n",
    "baseline = NLTKSubstringBaseline(threshold=0.8, n_matches=3)\n",
    "baseline.build(['tributo', 'tassa di successione'])\n",
    "test_preds = baseline.predict(['Il tributo è una tassa di successione.'])\n",
    "assert 'tributo' in test_preds[0]\n",
    "assert 'tassa di successione' in test_preds[0]\n",
    "print(\"✓ Baseline model works correctly\")\n",
    "print(f\"  Test predictions: {test_preds[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02c9d81",
   "metadata": {},
   "source": [
    "### Run and Evaluate Baseline Model\n",
    "\n",
    "\n",
    "**Additional configurations to test**\n",
    "- Test with different *threshold* and *n_matches* values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f84787",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique training terms: 713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|██████████| 577/577 [00:02<00:00, 196.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Baseline Model Results:\n",
      "Micro-averaged metrics:\n",
      "  Precision: 0.1599\n",
      "  Recall:    0.7428\n",
      "  F1 Score:  0.2632\n",
      "  TP=335, FP=1760, FN=116\n",
      "\n",
      "Type-level metrics:\n",
      "  Type Precision: 0.5569\n",
      "  Type Recall:    0.5661\n",
      "  Type F1 Score:  0.5615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Extract unique terms from training data\n",
    "train_terms = set()\n",
    "for s in train_sentences:\n",
    "    train_terms.update(t for t in s['terms'] if t)\n",
    "\n",
    "print(f\"Unique training terms: {len(train_terms)}\")\n",
    "\n",
    "# Build baseline model\n",
    "baseline_model = NLTKSubstringBaseline(threshold=0.8, n_matches=3)\n",
    "baseline_model.build(sorted(train_terms))\n",
    "\n",
    "# Predict on dev set\n",
    "dev_texts = [s['sentence_text'] for s in dev_sentences]\n",
    "dev_gold = [s['terms'] for s in dev_sentences]\n",
    "\n",
    "baseline_preds = baseline_model.predict(dev_texts)\n",
    "\n",
    "# Evaluate\n",
    "precision, recall, f1, tp, fp, fn = micro_f1_score(dev_gold, baseline_preds)\n",
    "type_precision, type_recall, type_f1 = type_f1_score(dev_gold, baseline_preds)\n",
    "\n",
    "print(\"\\nBaseline Model Results:\")\n",
    "print(\"Micro-averaged metrics:\")\n",
    "print(f\"  Precision: {precision:.4f}\")\n",
    "print(f\"  Recall:    {recall:.4f}\")\n",
    "print(f\"  F1 Score:  {f1:.4f}\")\n",
    "print(f\"  TP={tp}, FP={fp}, FN={fn}\")\n",
    "print(\"\\nType-level metrics:\")\n",
    "print(f\"  Type Precision: {type_precision:.4f}\")\n",
    "print(f\"  Type Recall:    {type_recall:.4f}\")\n",
    "print(f\"  Type F1 Score:  {type_f1:.4f}\")\n",
    "\n",
    "# Store metrics for later comparison\n",
    "baseline_metrics = {\n",
    "    'precision': precision,\n",
    "    'recall': recall,\n",
    "    'f1': f1,\n",
    "    'type_precision': type_precision,\n",
    "    'type_recall': type_recall,\n",
    "    'type_f1': type_f1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "baa6aad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/nltk_baseline.pkl\n"
     ]
    }
   ],
   "source": [
    "# Save baseline model\n",
    "baseline_model.save('models/nltk_baseline.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf88f49",
   "metadata": {},
   "source": [
    "## Trained Model: TF-IDF and Collocation Detection\n",
    "\n",
    "Statistical approach that learns from training data:\n",
    "- **TF-IDF**: Identifies important terms based on frequency and document distribution\n",
    "- **Collocations**: Detects statistically significant n-grams (bigrams, trigrams)\n",
    "- **PMI scores**: Measures how often words appear together vs. independently (c.f. https://web.stanford.edu/~jurafsky/slp3/J.pdf)\n",
    "\n",
    "\n",
    "**Additional configurations to test**\n",
    "- Test with different *tfidf_threshold*, *collocation_threshold*, and *min_freq* values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5dc8b1ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete:\n",
      "  Unique terms: 2\n",
      "  Bigrams: 0\n",
      "  Trigrams: 0\n",
      "✓ Trained model initialization works correctly\n"
     ]
    }
   ],
   "source": [
    "class NLTKTrainedModel:\n",
    "    \"\"\"Statistical term extractor using TF-IDF and collocations.\"\"\"\n",
    "    \n",
    "    def __init__(self, tfidf_threshold=0.1, collocation_threshold=3.0, min_freq=2):\n",
    "        self.tfidf_threshold = tfidf_threshold\n",
    "        self.collocation_threshold = collocation_threshold\n",
    "        self.min_freq = min_freq\n",
    "        \n",
    "        # Learned features\n",
    "        self.term_tfidf = {}\n",
    "        self.term_freq = Counter()\n",
    "        self.doc_freq = Counter()\n",
    "        self.bigram_scores = {}\n",
    "        self.trigram_scores = {}\n",
    "        self.known_terms = set()\n",
    "        self.vocab = set()\n",
    "        self.n_docs = 0\n",
    "    \n",
    "    def _normalize(self, text: str) -> str:\n",
    "        text = text.lower()\n",
    "        text = re.sub(r\"[^\\w\\sÀ-ÖØ-öø-ÿ]+\", \" \", text)\n",
    "        text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "        return text\n",
    "    \n",
    "    def _tokenize(self, text: str) -> List[str]:\n",
    "        try:\n",
    "            return word_tokenize(text, language='italian')\n",
    "        except:\n",
    "            return text.split()\n",
    "    \n",
    "    def _compute_tfidf(self):\n",
    "        \"\"\"Compute TF-IDF: term frequency × log(N / document frequency).\"\"\"\n",
    "        self.term_tfidf = {}\n",
    "        for term in self.known_terms:\n",
    "            if term not in self.term_freq or term not in self.doc_freq:\n",
    "                continue\n",
    "            tf = self.term_freq[term]\n",
    "            df = self.doc_freq[term]\n",
    "            idf = math.log(self.n_docs / df) if df > 0 else 0\n",
    "            self.term_tfidf[term] = tf * idf\n",
    "    \n",
    "    def _extract_collocations(self, sentences: List[str]):\n",
    "        \"\"\"Find statistically significant word combinations.\"\"\"\n",
    "        all_tokens = []\n",
    "        for s in sentences:\n",
    "            tokens = self._tokenize(self._normalize(s))\n",
    "            all_tokens.extend(tokens)\n",
    "            self.vocab.update(tokens)\n",
    "        \n",
    "        # Bigrams\n",
    "        if len(all_tokens) >= 2:\n",
    "            finder = BigramCollocationFinder.from_words(all_tokens)\n",
    "            finder.apply_freq_filter(self.min_freq)\n",
    "            measures = BigramAssocMeasures()\n",
    "            \n",
    "            for (w1, w2), score in finder.score_ngrams(measures.pmi):\n",
    "                if score >= self.collocation_threshold:\n",
    "                    self.bigram_scores[(w1, w2)] = score\n",
    "        \n",
    "        # Trigrams\n",
    "        if len(all_tokens) >= 3:\n",
    "            finder = TrigramCollocationFinder.from_words(all_tokens)\n",
    "            finder.apply_freq_filter(self.min_freq)\n",
    "            measures = TrigramAssocMeasures()\n",
    "            \n",
    "            for (w1, w2, w3), score in finder.score_ngrams(measures.pmi):\n",
    "                if score >= self.collocation_threshold:\n",
    "                    self.trigram_scores[(w1, w2, w3)] = score\n",
    "    \n",
    "    def train(self, sentences: List[str], term_lists: List[List[str]]):\n",
    "        \"\"\"Learn statistical features from labeled data.\"\"\"\n",
    "        self.n_docs = len(sentences)\n",
    "        \n",
    "        # Collect term statistics\n",
    "        for sent, terms in zip(sentences, term_lists):\n",
    "            terms_in_doc = set()\n",
    "            for term in terms:\n",
    "                if term:\n",
    "                    self.known_terms.add(term)\n",
    "                    self.term_freq[term] += 1\n",
    "                    terms_in_doc.add(term)\n",
    "            \n",
    "            for term in terms_in_doc:\n",
    "                self.doc_freq[term] += 1\n",
    "        \n",
    "        # Compute TF-IDF scores\n",
    "        self._compute_tfidf()\n",
    "        \n",
    "        # Extract collocations\n",
    "        self._extract_collocations(sentences)\n",
    "        \n",
    "        print(f\"Training complete:\")\n",
    "        print(f\"  Unique terms: {len(self.known_terms)}\")\n",
    "        print(f\"  Bigrams: {len(self.bigram_scores)}\")\n",
    "        print(f\"  Trigrams: {len(self.trigram_scores)}\")\n",
    "    \n",
    "    def _find_ngrams(self, tokens: List[str]) -> List[str]:\n",
    "        \"\"\"Find learned collocations in token sequence.\"\"\"\n",
    "        matches = []\n",
    "        \n",
    "        # Trigrams\n",
    "        for i in range(len(tokens) - 2):\n",
    "            trigram = (tokens[i], tokens[i+1], tokens[i+2])\n",
    "            if trigram in self.trigram_scores:\n",
    "                matches.append(\" \".join(trigram))\n",
    "        \n",
    "        # Bigrams\n",
    "        for i in range(len(tokens) - 1):\n",
    "            bigram = (tokens[i], tokens[i+1])\n",
    "            if bigram in self.bigram_scores:\n",
    "                matches.append(\" \".join(bigram))\n",
    "        \n",
    "        return matches\n",
    "    \n",
    "    def predict(self, sentences: List[str]) -> List[List[str]]:\n",
    "        \"\"\"Extract terms from new sentences.\"\"\"\n",
    "        results = []\n",
    "        for sent in sentences:\n",
    "            norm_sent = self._normalize(sent)\n",
    "            tokens = self._tokenize(norm_sent)\n",
    "            found = set()\n",
    "            \n",
    "            # Match high-scoring known terms\n",
    "            for term in self.known_terms:\n",
    "                norm_term = self._normalize(term)\n",
    "                if norm_term in norm_sent:\n",
    "                    if self.term_tfidf.get(term, 0) >= self.tfidf_threshold:\n",
    "                        found.add(term)\n",
    "            \n",
    "            # Find collocations\n",
    "            ngrams = self._find_ngrams(tokens)\n",
    "            for ng in ngrams:\n",
    "                for term in self.known_terms:\n",
    "                    if self._normalize(term) == ng:\n",
    "                        found.add(term)\n",
    "            \n",
    "            results.append(sorted(found))\n",
    "        return results\n",
    "    \n",
    "    def save(self, path: str):\n",
    "        \"\"\"Save trained model.\"\"\"\n",
    "        os.makedirs(os.path.dirname(path) or '.', exist_ok=True)\n",
    "        model_data = {\n",
    "            'tfidf_threshold': self.tfidf_threshold,\n",
    "            'collocation_threshold': self.collocation_threshold,\n",
    "            'min_freq': self.min_freq,\n",
    "            'term_tfidf': self.term_tfidf,\n",
    "            'term_freq': dict(self.term_freq),\n",
    "            'doc_freq': dict(self.doc_freq),\n",
    "            'bigram_scores': self.bigram_scores,\n",
    "            'trigram_scores': self.trigram_scores,\n",
    "            'known_terms': list(self.known_terms),\n",
    "            'n_docs': self.n_docs,\n",
    "            'vocab': list(self.vocab)\n",
    "        }\n",
    "        with open(path, 'wb') as f:\n",
    "            pickle.dump(model_data, f)\n",
    "        print(f\"Model saved to {path}\")\n",
    "    \n",
    "    def load(self, path: str):\n",
    "        \"\"\"Load trained model.\"\"\"\n",
    "        with open(path, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        self.tfidf_threshold = data['tfidf_threshold']\n",
    "        self.collocation_threshold = data['collocation_threshold']\n",
    "        self.min_freq = data['min_freq']\n",
    "        self.term_tfidf = data['term_tfidf']\n",
    "        self.term_freq = Counter(data['term_freq'])\n",
    "        self.doc_freq = Counter(data['doc_freq'])\n",
    "        self.bigram_scores = data['bigram_scores']\n",
    "        self.trigram_scores = data['trigram_scores']\n",
    "        self.known_terms = set(data['known_terms'])\n",
    "        self.n_docs = data['n_docs']\n",
    "        self.vocab = set(data['vocab'])\n",
    "        print(f\"Model loaded from {path}\")\n",
    "\n",
    "\n",
    "# Test: Simple training\n",
    "test_model = NLTKTrainedModel(tfidf_threshold=0.1, collocation_threshold=2.0)\n",
    "test_sents = ['Il tributo è importante.', 'La tassa di successione è un tributo.']\n",
    "test_terms = [['tributo'], ['tassa di successione', 'tributo']]\n",
    "test_model.train(test_sents, test_terms)\n",
    "assert len(test_model.known_terms) == 2\n",
    "assert test_model.term_freq['tributo'] == 2\n",
    "print(\"✓ Trained model initialization works correctly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4de847d",
   "metadata": {},
   "source": [
    "### Train and Evaluate Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2b235d34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete:\n",
      "  Unique terms: 713\n",
      "  Bigrams: 4198\n",
      "  Trigrams: 3671\n",
      "\n",
      "Top 10 terms by TF-IDF:\n",
      "  vetro: 242.19\n",
      "  porta a porta: 234.60\n",
      "  conferire: 210.96\n",
      "  rifiuti: 210.96\n",
      "  multimateriale: 208.25\n",
      "  conferimento: 205.52\n",
      "  indifferenziato: 162.21\n",
      "  carta e cartone: 149.78\n",
      "  plastica: 136.91\n",
      "  rifiuti organici: 130.29\n",
      "\n",
      "Example bigrams (first 10):\n",
      "  177 178: 14.09\n",
      "  178 179: 14.09\n",
      "  179 183: 14.09\n",
      "  234 341: 14.09\n",
      "  autotrasporti distributori: 14.09\n",
      "  birrerie hamburgerie: 14.09\n",
      "  bonifico bancario: 14.09\n",
      "  carburante librerie: 14.09\n",
      "  carrozzerie autofficine: 14.09\n",
      "  case vacanze: 14.09\n"
     ]
    }
   ],
   "source": [
    "# Prepare training data\n",
    "train_texts = [s['sentence_text'] for s in train_sentences]\n",
    "train_term_lists = [s['terms'] for s in train_sentences]\n",
    "\n",
    "# Initialize and train model\n",
    "trained_model = NLTKTrainedModel(\n",
    "    tfidf_threshold=0.1,\n",
    "    collocation_threshold=3.0,\n",
    "    min_freq=2\n",
    ")\n",
    "\n",
    "trained_model.train(train_texts, train_term_lists)\n",
    "\n",
    "# Show some learned features\n",
    "print(\"\\nTop 10 terms by TF-IDF:\")\n",
    "top_terms = sorted(trained_model.term_tfidf.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "for term, score in top_terms:\n",
    "    print(f\"  {term}: {score:.2f}\")\n",
    "\n",
    "print(f\"\\nExample bigrams (first 10):\")\n",
    "for bigram, score in list(trained_model.bigram_scores.items())[:10]:\n",
    "    print(f\"  {' '.join(bigram)}: {score:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dd8e480e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained Model Results:\n",
      "Micro-averaged metrics:\n",
      "  Precision: 0.2198\n",
      "  Recall:    0.7428\n",
      "  F1 Score:  0.3392\n",
      "  TP=335, FP=1189, FN=116\n",
      "\n",
      "Type-level metrics:\n",
      "  Type Precision: 0.6143\n",
      "  Type Recall:    0.5661\n",
      "  Type F1 Score:  0.5892\n"
     ]
    }
   ],
   "source": [
    "# Predict on dev set\n",
    "trained_preds = trained_model.predict(dev_texts)\n",
    "\n",
    "# Evaluate\n",
    "precision, recall, f1, tp, fp, fn = micro_f1_score(dev_gold, trained_preds)\n",
    "type_precision, type_recall, type_f1 = type_f1_score(dev_gold, trained_preds)\n",
    "\n",
    "print(\"Trained Model Results:\")\n",
    "print(\"Micro-averaged metrics:\")\n",
    "print(f\"  Precision: {precision:.4f}\")\n",
    "print(f\"  Recall:    {recall:.4f}\")\n",
    "print(f\"  F1 Score:  {f1:.4f}\")\n",
    "print(f\"  TP={tp}, FP={fp}, FN={fn}\")\n",
    "print(\"\\nType-level metrics:\")\n",
    "print(f\"  Type Precision: {type_precision:.4f}\")\n",
    "print(f\"  Type Recall:    {type_recall:.4f}\")\n",
    "print(f\"  Type F1 Score:  {type_f1:.4f}\")\n",
    "\n",
    "# Store metrics for later comparison\n",
    "trained_metrics = {\n",
    "    'precision': precision,\n",
    "    'recall': recall,\n",
    "    'f1': f1,\n",
    "    'type_precision': type_precision,\n",
    "    'type_recall': type_recall,\n",
    "    'type_f1': type_f1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bc7567bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/nltk_trained.pkl\n"
     ]
    }
   ],
   "source": [
    "# Save trained model\n",
    "trained_model.save('models/nltk_trained.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fda1556",
   "metadata": {},
   "source": [
    "## Results Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0a72ccbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro-averaged Metrics:\n",
      "| Model    |   Precision |   Recall |       F1 |\n",
      "|:---------|------------:|---------:|---------:|\n",
      "| Baseline |    0.159905 | 0.742794 | 0.263158 |\n",
      "| Trained  |    0.219816 | 0.742794 | 0.339241 |\n",
      "\n",
      "\n",
      "Type-level Metrics:\n",
      "| Model    |   Type Precision |   Type Recall |   Type F1 |\n",
      "|:---------|-----------------:|--------------:|----------:|\n",
      "| Baseline |         0.556911 |      0.566116 |  0.561475 |\n",
      "| Trained  |         0.61435  |      0.566116 |  0.589247 |\n",
      "\n",
      "\n",
      "Micro F1 Score improvement: +28.9%\n",
      "Type F1 Score improvement: +4.9%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Micro-averaged comparison\n",
    "results_df = pd.DataFrame([\n",
    "    {\n",
    "        'Model': 'Baseline',\n",
    "        'Precision': baseline_metrics['precision'],\n",
    "        'Recall': baseline_metrics['recall'],\n",
    "        'F1': baseline_metrics['f1']\n",
    "    },\n",
    "    {\n",
    "        'Model': 'Trained',\n",
    "        'Precision': trained_metrics['precision'],\n",
    "        'Recall': trained_metrics['recall'],\n",
    "        'F1': trained_metrics['f1']\n",
    "    }\n",
    "])\n",
    "\n",
    "print(\"Micro-averaged Metrics:\")\n",
    "print(results_df.to_markdown(index=False))\n",
    "\n",
    "# Type-level comparison\n",
    "type_results_df = pd.DataFrame([\n",
    "    {\n",
    "        'Model': 'Baseline',\n",
    "        'Type Precision': baseline_metrics['type_precision'],\n",
    "        'Type Recall': baseline_metrics['type_recall'],\n",
    "        'Type F1': baseline_metrics['type_f1']\n",
    "    },\n",
    "    {\n",
    "        'Model': 'Trained',\n",
    "        'Type Precision': trained_metrics['type_precision'],\n",
    "        'Type Recall': trained_metrics['type_recall'],\n",
    "        'Type F1': trained_metrics['type_f1']\n",
    "    }\n",
    "])\n",
    "\n",
    "print(\"\\n\\nType-level Metrics:\")\n",
    "print(type_results_df.to_markdown(index=False))\n",
    "\n",
    "# Show improvement\n",
    "f1_improvement = (trained_metrics['f1'] - baseline_metrics['f1']) / baseline_metrics['f1'] * 100\n",
    "type_f1_improvement = (trained_metrics['type_f1'] - baseline_metrics['type_f1']) / baseline_metrics['type_f1'] * 100\n",
    "print(f\"\\n\\nMicro F1 Score improvement: {f1_improvement:+.1f}%\")\n",
    "print(f\"Type F1 Score improvement: {type_f1_improvement:+.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acdecc47",
   "metadata": {},
   "source": [
    "## Save Predictions to Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3649f50d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 577 predictions to predictions/subtask_a_dev_nltk_baseline_preds.json\n",
      "Saved 577 predictions to predictions/subtask_a_dev_nltk_trained_preds.json\n"
     ]
    }
   ],
   "source": [
    "def save_predictions(predictions: List[List[str]], \n",
    "                     sentences: List[Dict], \n",
    "                     output_path: str):\n",
    "    \"\"\"Save predictions in competition format.\"\"\"\n",
    "    output = {'data': []}\n",
    "    for pred, sent in zip(predictions, sentences):\n",
    "        output['data'].append({\n",
    "            'document_id': sent['document_id'],\n",
    "            'paragraph_id': sent['paragraph_id'],\n",
    "            'sentence_id': sent['sentence_id'],\n",
    "            'term_list': pred\n",
    "        })\n",
    "    \n",
    "    os.makedirs(os.path.dirname(output_path) or '.', exist_ok=True)\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(output, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"Saved {len(predictions)} predictions to {output_path}\")\n",
    "\n",
    "\n",
    "# Save both sets of predictions\n",
    "save_predictions(baseline_preds, dev_sentences, 'predictions/subtask_a_dev_nltk_baseline_preds.json')\n",
    "save_predictions(trained_preds, dev_sentences, 'predictions/subtask_a_dev_nltk_trained_preds.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0abd9e",
   "metadata": {},
   "source": [
    "## Load and Test Saved Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "73d22692",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from models/nltk_baseline.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|██████████| 1/1 [00:00<00:00, 261.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Baseline model saved and loaded correctly\n",
      "Model loaded from models/nltk_trained.pkl\n",
      "✓ Trained model saved and loaded correctly\n",
      "\n",
      "All models successfully saved and can be reloaded!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Test loading baseline\n",
    "loaded_baseline = NLTKSubstringBaseline()\n",
    "loaded_baseline.load('models/nltk_baseline.pkl')\n",
    "test_preds_baseline = loaded_baseline.predict([dev_texts[0]])\n",
    "assert test_preds_baseline[0] == baseline_preds[0]\n",
    "print(\"✓ Baseline model saved and loaded correctly\")\n",
    "\n",
    "# Test loading trained model\n",
    "loaded_trained = NLTKTrainedModel()\n",
    "loaded_trained.load('models/nltk_trained.pkl')\n",
    "test_preds_trained = loaded_trained.predict([dev_texts[0]])\n",
    "assert test_preds_trained[0] == trained_preds[0]\n",
    "print(\"✓ Trained model saved and loaded correctly\")\n",
    "\n",
    "print(\"\\nAll models successfully saved and can be reloaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d03e700",
   "metadata": {},
   "source": [
    "## Example Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1736adba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: a. per incendi dei rifiuti nei contenitori € 2.000\n",
      "\n",
      "Gold terms: ['rifiuti']\n",
      "\n",
      "Baseline predictions: ['pe', 'rifiuti', 'rifiuto']\n",
      "Trained predictions: ['pe', 'rifiuti']\n",
      "\n",
      "Baseline correct: {'rifiuti'}\n",
      "Trained correct: {'rifiuti'}\n"
     ]
    }
   ],
   "source": [
    "# Show example predictions from both models\n",
    "example_idx = 120\n",
    "example_text = dev_texts[example_idx]\n",
    "example_gold = dev_gold[example_idx]\n",
    "example_baseline = baseline_preds[example_idx]\n",
    "example_trained = trained_preds[example_idx]\n",
    "\n",
    "print(f\"Sentence: {example_text}\\n\")\n",
    "print(f\"Gold terms: {example_gold}\\n\")\n",
    "print(f\"Baseline predictions: {example_baseline}\")\n",
    "print(f\"Trained predictions: {example_trained}\\n\")\n",
    "\n",
    "# Show what each model got right/wrong\n",
    "baseline_correct = set(example_baseline) & set(example_gold)\n",
    "trained_correct = set(example_trained) & set(example_gold)\n",
    "\n",
    "print(f\"Baseline correct: {baseline_correct}\")\n",
    "print(f\"Trained correct: {trained_correct}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ate-it",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
