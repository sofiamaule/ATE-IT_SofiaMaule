{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "# LLM-Based Term Extraction (Zero-Shot)\n",
        "\n",
        "This notebook demonstrates a zero-shot term extraction approach using Large Language Models:\n",
        "- Uses Google Gemini API for term extraction\n",
        "- Processes sentences in batches for efficiency\n",
        "- No training required - relies on LLM's general knowledge\n",
        "\n",
        "Dataset: EvalITA 2025 ATE-IT (Automatic Term Extraction - Italian Testbed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup and Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "7ax0pwKnmCS3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Libraries imported\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "import os\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "\n",
        "print(\"✓ Libraries imported\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Loading and Processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Data loading functions defined\n"
          ]
        }
      ],
      "source": [
        "def load_jsonl(path: str):\n",
        "    \"\"\"Load a JSON lines file or JSON array file.\"\"\"\n",
        "    with open(path, 'r', encoding='utf-8') as f:\n",
        "        text = f.read().strip()\n",
        "    if not text:\n",
        "        return []\n",
        "    try:\n",
        "        data = json.loads(text)\n",
        "    except json.JSONDecodeError:\n",
        "        data = []\n",
        "        for line in text.splitlines():\n",
        "            line = line.strip()\n",
        "            if line:\n",
        "                data.append(json.loads(line))\n",
        "    return data\n",
        "\n",
        "\n",
        "def build_sentence_gold_map(records):\n",
        "    \"\"\"Convert dataset rows into list of sentences with aggregated terms.\"\"\"\n",
        "    out = {}\n",
        "    \n",
        "    if isinstance(records, dict) and 'data' in records:\n",
        "        rows = records['data']\n",
        "    else:\n",
        "        rows = records\n",
        "    \n",
        "    for r in rows:\n",
        "        key = (r.get('document_id'), r.get('paragraph_id'), r.get('sentence_id'))\n",
        "        if key not in out:\n",
        "            out[key] = {\n",
        "                'document_id': r.get('document_id'),\n",
        "                'paragraph_id': r.get('paragraph_id'),\n",
        "                'sentence_id': r.get('sentence_id'),\n",
        "                'sentence_text': r.get('sentence_text', ''),\n",
        "                'terms': []\n",
        "            }\n",
        "        \n",
        "        if isinstance(r.get('term_list'), list):\n",
        "            for t in r.get('term_list'):\n",
        "                if t and t not in out[key]['terms']:\n",
        "                    out[key]['terms'].append(t)\n",
        "        else:\n",
        "            term = r.get('term')\n",
        "            if term and term not in out[key]['terms']:\n",
        "                out[key]['terms'].append(term)\n",
        "    \n",
        "    return list(out.values())\n",
        "\n",
        "\n",
        "print(\"✓ Data loading functions defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "jDhknjBdmKoI"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training sentences: 2308\n",
            "Dev sentences: 577\n",
            "\n",
            "Example sentence:\n",
            "  Text: Non Domestica; CAMPEGGI, DISTRIBUTORI CARBURANTI, PARCHEGGI; 1,22; 4,73 \n",
            "  Terms: []\n"
          ]
        }
      ],
      "source": [
        "# Load training and dev data\n",
        "train_data = load_jsonl('../data/subtask_a_train.json')\n",
        "dev_data = load_jsonl('../data/subtask_a_dev.json')\n",
        "\n",
        "train_sentences = build_sentence_gold_map(train_data)\n",
        "dev_sentences = build_sentence_gold_map(dev_data)\n",
        "\n",
        "print(f\"Training sentences: {len(train_sentences)}\")\n",
        "print(f\"Dev sentences: {len(dev_sentences)}\")\n",
        "print(f\"\\nExample sentence:\")\n",
        "print(f\"  Text: {dev_sentences[0]['sentence_text']}\")\n",
        "print(f\"  Terms: {dev_sentences[0]['terms']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluation Metrics\n",
        "\n",
        "Using the official evaluation metrics from the competition."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Evaluation functions defined\n"
          ]
        }
      ],
      "source": [
        "def micro_f1_score(gold_standard, system_output):\n",
        "    \"\"\"\n",
        "    Evaluates performance using Precision, Recall, and F1 score \n",
        "    based on individual term matching (micro-average).\n",
        "    \"\"\"\n",
        "    total_true_positives = 0\n",
        "    total_false_positives = 0\n",
        "    total_false_negatives = 0\n",
        "    \n",
        "    for gold, system in zip(gold_standard, system_output):\n",
        "        gold_set = set(gold)\n",
        "        system_set = set(system)\n",
        "        \n",
        "        true_positives = len(gold_set.intersection(system_set))\n",
        "        false_positives = len(system_set - gold_set)\n",
        "        false_negatives = len(gold_set - system_set)\n",
        "        \n",
        "        total_true_positives += true_positives\n",
        "        total_false_positives += false_positives\n",
        "        total_false_negatives += false_negatives\n",
        "    \n",
        "    precision = total_true_positives / (total_true_positives + total_false_positives) if (total_true_positives + total_false_positives) > 0 else 0\n",
        "    recall = total_true_positives / (total_true_positives + total_false_negatives) if (total_true_positives + total_false_negatives) > 0 else 0\n",
        "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "    \n",
        "    return precision, recall, f1, total_true_positives, total_false_positives, total_false_negatives\n",
        "\n",
        "\n",
        "def type_f1_score(gold_standard, system_output):\n",
        "    \"\"\"\n",
        "    Evaluates performance using Type Precision, Type Recall, and Type F1 score\n",
        "    based on the set of unique terms extracted at least once across the entire dataset.\n",
        "    \"\"\"\n",
        "    all_gold_terms = set()\n",
        "    for item_terms in gold_standard:\n",
        "        all_gold_terms.update(item_terms)\n",
        "    \n",
        "    all_system_terms = set()\n",
        "    for item_terms in system_output:\n",
        "        all_system_terms.update(item_terms)\n",
        "    \n",
        "    type_true_positives = len(all_gold_terms.intersection(all_system_terms))\n",
        "    type_false_positives = len(all_system_terms - all_gold_terms)\n",
        "    type_false_negatives = len(all_gold_terms - all_system_terms)\n",
        "    \n",
        "    type_precision = type_true_positives / (type_true_positives + type_false_positives) if (type_true_positives + type_false_positives) > 0 else 0\n",
        "    type_recall = type_true_positives / (type_true_positives + type_false_negatives) if (type_true_positives + type_false_negatives) > 0 else 0\n",
        "    type_f1 = 2 * (type_precision * type_recall) / (type_precision + type_recall) if (type_precision + type_recall) > 0 else 0\n",
        "    \n",
        "    return type_precision, type_recall, type_f1\n",
        "\n",
        "\n",
        "print(\"✓ Evaluation functions defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Initialize LLM Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "diQC4xoeouBd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\marti\\miniconda3\\envs\\ate-it\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "'bool' object is not subscriptable",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m env_vars = load_dotenv()\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Get API key from user\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m genai.configure(api_key=\u001b[43menv_vars\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mGEMINI_API_KEY\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Initialize Gemini model\u001b[39;00m\n\u001b[32m      9\u001b[39m model = genai.GenerativeModel(\u001b[33m'\u001b[39m\u001b[33mgemini-2.5-flash\u001b[39m\u001b[33m'\u001b[39m)\n",
            "\u001b[31mTypeError\u001b[39m: 'bool' object is not subscriptable"
          ]
        }
      ],
      "source": [
        "import google.generativeai as genai\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "\n",
        "# Get API key from environment\n",
        "genai.configure(api_key=os.getenv('GEMINI_API_KEY'))\n",
        "\n",
        "# Initialize Gemini model\n",
        "model = genai.GenerativeModel('gemini-2.5-flash')\n",
        "\n",
        "print(\"✓ Gemini model initialized\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bLdHG-m1mdrK"
      },
      "outputs": [],
      "source": [
        "# Configure batch size for efficient processing\n",
        "batch_size = 20\n",
        "\n",
        "# Define zero-shot prompt for term extraction\n",
        "system_prompt = f\"\"\"You are an automatic term extraction agent. You will receive a list of sentences as input.\n",
        "Your role is to extract waste management terms from the sentences. Output a list of terms for each sentence.\n",
        "\n",
        "Strictly adhere to the Example Output Format:\n",
        "\n",
        "Example Output Format:\n",
        "Sentence 1: [term1; term2; term3; term4]\n",
        "Sentence 2: [term5; term6]\n",
        "Sentence 3: []\n",
        "Sentence 4: []\n",
        "Sentence 5: [term7]\n",
        "\n",
        "Instructions:\n",
        "* Extract only terms, ignore named entities\n",
        "* Do not extract nested terms\n",
        "* Extract only terms related to waste management, ignoring other domains\n",
        "* If a sentence contains no terms, output an empty list for that sentence\n",
        "* You must output {batch_size} lists of terms, one for each sentence\n",
        "\"\"\"\n",
        "\n",
        "print(f\"✓ Zero-shot prompt configured (batch size: {batch_size})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configure Zero-Shot Prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lXrKQDeMqHqN"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "100%|██████████| 577/577 [11:52<00:00,  1.23s/it]\n"
          ]
        }
      ],
      "source": [
        "# Process sentences in batches with LLM\n",
        "print(f\"Processing {len(dev_sentences)} sentences in batches of {batch_size}...\")\n",
        "\n",
        "response_list = []\n",
        "user_prompt = \"\"\n",
        "\n",
        "for i, sent_data in enumerate(tqdm(dev_sentences)):\n",
        "    sent = sent_data['sentence_text']\n",
        "    \n",
        "    # Build prompt until batch size is reached\n",
        "    if (i + 1) % batch_size == 0:\n",
        "        user_prompt += f\"Sentence {i + 1}:\\n {sent}\"\n",
        "        \n",
        "        # Send batch to LLM\n",
        "        response = model.generate_content(\n",
        "            f\"System: {system_prompt}\\nUser: {user_prompt}\"\n",
        "        )\n",
        "        response_list.append(response.text)\n",
        "        \n",
        "        user_prompt = \"\"\n",
        "    else:\n",
        "        user_prompt += f\"Sentence {i + 1}:\\n {sent}\\n\\n\"\n",
        "\n",
        "# Process remaining sentences (last batch)\n",
        "if user_prompt:\n",
        "    user_prompt = user_prompt.rstrip()\n",
        "    response = model.generate_content(\n",
        "        f\"System: {system_prompt}\\nUser: {user_prompt}\"\n",
        "    )\n",
        "    response_list.append(response.text)\n",
        "\n",
        "print(f\"✓ Received {len(response_list)} batch responses from LLM\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Parse LLM responses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vKzlrjf7onwV"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Output data is the same length as input data.\n"
          ]
        }
      ],
      "source": [
        "# Parse LLM responses to extract term lists\n",
        "print(\"Parsing LLM responses...\")\n",
        "\n",
        "llm_preds = []\n",
        "for response in response_list:\n",
        "    for sent in response.split('\\n'):\n",
        "        if 'Sentence' in sent and '[' in sent:\n",
        "            try:\n",
        "                # Extract sentence ID\n",
        "                id_match = re.search(r'Sentence (\\d+):', sent)\n",
        "                if not id_match:\n",
        "                    continue\n",
        "                \n",
        "                # Extract terms from brackets\n",
        "                terms_match = re.search(r'\\[(.*?)\\]', sent)\n",
        "                if terms_match:\n",
        "                    terms = terms_match.group(1).split(';')\n",
        "                    terms = [term.strip().lower() for term in terms if term.strip()]\n",
        "                else:\n",
        "                    terms = []\n",
        "                \n",
        "                llm_preds.append(terms)\n",
        "            except Exception as e:\n",
        "                print(f\"Warning: Could not parse line: {sent[:50]}...\")\n",
        "                llm_preds.append([])\n",
        "\n",
        "# Verify output length matches input\n",
        "if len(llm_preds) != len(dev_sentences):\n",
        "    print(f\"Warning: Output length ({len(llm_preds)}) doesn't match input ({len(dev_sentences)})\")\n",
        "    # Pad or truncate to match\n",
        "    while len(llm_preds) < len(dev_sentences):\n",
        "        llm_preds.append([])\n",
        "    llm_preds = llm_preds[:len(dev_sentences)]\n",
        "\n",
        "print(f\"✓ Parsed {len(llm_preds)} predictions\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Save LLM predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i0Dzrbh90J-r"
      },
      "outputs": [],
      "source": [
        "# Save predictions in competition format\n",
        "def save_predictions(predictions, sentences, output_path):\n",
        "    \"\"\"Save predictions in competition format.\"\"\"\n",
        "    output = {'data': []}\n",
        "    for pred, sent in zip(predictions, sentences):\n",
        "        output['data'].append({\n",
        "            'document_id': sent['document_id'],\n",
        "            'paragraph_id': sent['paragraph_id'],\n",
        "            'sentence_id': sent['sentence_id'],\n",
        "            'term_list': pred\n",
        "        })\n",
        "    \n",
        "    os.makedirs(os.path.dirname(output_path) or '.', exist_ok=True)\n",
        "    with open(output_path, 'w', encoding='utf-8') as f:\n",
        "        json.dump(output, f, ensure_ascii=False, indent=2)\n",
        "    print(f\"✓ Saved {len(predictions)} predictions to {output_path}\")\n",
        "\n",
        "\n",
        "save_predictions(llm_preds, dev_sentences, 'predictions/subtask_a_dev_llm_zero_shot_preds.json')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluate Performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'llm_preds' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m\n",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 5\u001b[39m\n",
            "\u001b[32m      2\u001b[39m dev_gold = [s[\u001b[33m'\u001b[39m\u001b[33mterms\u001b[39m\u001b[33m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m dev_sentences]\n",
            "\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Calculate metrics\u001b[39;00m\n",
            "\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m precision, recall, f1, tp, fp, fn = micro_f1_score(dev_gold, \u001b[43mllm_preds\u001b[49m)\n",
            "\u001b[32m      6\u001b[39m type_precision, type_recall, type_f1 = type_f1_score(dev_gold, llm_preds)\n",
            "\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m + \u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m60\u001b[39m)\n",
            "\n",
            "\u001b[31mNameError\u001b[39m: name 'llm_preds' is not defined"
          ]
        }
      ],
      "source": [
        "# Prepare gold standard and predictions for evaluation\n",
        "dev_gold = [s['terms'] for s in dev_sentences]\n",
        "\n",
        "# Calculate metrics\n",
        "precision, recall, f1, tp, fp, fn = micro_f1_score(dev_gold, llm_preds)\n",
        "type_precision, type_recall, type_f1 = type_f1_score(dev_gold, llm_preds)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"LLM ZERO-SHOT BASELINE RESULTS\")\n",
        "print(\"=\"*60)\n",
        "print(\"\\nMicro-averaged Metrics:\")\n",
        "print(f\"  Precision: {precision:.4f}\")\n",
        "print(f\"  Recall:    {recall:.4f}\")\n",
        "print(f\"  F1 Score:  {f1:.4f}\")\n",
        "print(f\"  TP={tp}, FP={fp}, FN={fn}\")\n",
        "\n",
        "print(\"\\nType-level Metrics:\")\n",
        "print(f\"  Type Precision: {type_precision:.4f}\")\n",
        "print(f\"  Type Recall:    {type_recall:.4f}\")\n",
        "print(f\"  Type F1 Score:  {type_f1:.4f}\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Show example predictions\n",
        "print(\"Example Predictions:\\n\")\n",
        "\n",
        "count = 0\n",
        "for i in range(len(dev_sentences)):\n",
        "    if len(dev_gold[i]) > 0 and count < 5:\n",
        "        print(f\"Sentence: {dev_sentences[i]['sentence_text'][:100]}...\")\n",
        "        print(f\"Gold terms: {dev_gold[i][:5]}\")\n",
        "        print(f\"LLM predictions: {llm_preds[i][:5]}\")\n",
        "        \n",
        "        correct = set(dev_gold[i]) & set(llm_preds[i])\n",
        "        missed = set(dev_gold[i]) - set(llm_preds[i])\n",
        "        wrong = set(llm_preds[i]) - set(dev_gold[i])\n",
        "        \n",
        "        print(f\"✓ Correct: {len(correct)}\")\n",
        "        print(f\"✗ Missed: {len(missed)}\")\n",
        "        print(f\"✗ Wrong: {len(wrong)}\")\n",
        "        print(\"-\"*80)\n",
        "        print()\n",
        "        \n",
        "        count += 1"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyNPasnfXZT1l2LNzTXayrhd",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "ate-it",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
