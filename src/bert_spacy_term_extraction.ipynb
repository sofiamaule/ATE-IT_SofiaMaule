{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52af72d2",
   "metadata": {},
   "source": [
    "# **Term Extraction Ensemble (BERT + spaCy + Dictionary)**\n",
    "\n",
    "This notebook implements a complete post-processing pipeline for the ATE-IT Subtask A (Automatic Term Extraction).  \n",
    "It takes the raw predictions from a fine-tuned **BERT token classification model** and combines them with **spaCy noun-chunk spans** and a **gold-derived domain vocabulary** to produce a higher-quality list of domain terms for each sentence.\n",
    "\n",
    "### Pipeline Summary\n",
    "1. **Load BERT and spaCy predictions**  \n",
    "   - Import model outputs in ATE-IT JSON format.  \n",
    "   - Map predictions to sentence identifiers for easy lookup.\n",
    "\n",
    "2. **Normalize and clean BERT terms**  \n",
    "   - Remove punctuation, unify quotes, lowercase, collapse whitespace.  \n",
    "   - Filter out spurious or generic one-word candidates.\n",
    "\n",
    "3. **Build a domain vocabulary from the gold training set**  \n",
    "   - Normalize gold terms.  \n",
    "   - Track frequencies to identify strong (repeated) vs. weak (rare) terms.\n",
    "\n",
    "4. **Merge BERT + spaCy + Dictionary knowledge**  \n",
    "   - **Upgrade** short BERT terms to longer spaCy spans when they form a valid multi-word expression present in the gold vocabulary.  \n",
    "   - **Add** additional spaCy multi-word spans only if they appear in the gold vocabulary.  \n",
    "   - **Filter out** generic, meaningless, or uninformative unigrams.  \n",
    "   - **Normalize and deduplicate** final terms.\n",
    "\n",
    "5. **Generate final ensemble predictions**  \n",
    "   - For each sentence, produce an improved term list combining all signals.  \n",
    "   - Output saved in ATE-IT JSON format.\n",
    "\n",
    "### Goal\n",
    "The notebook improves recall and precision of automatic term extraction by combining:\n",
    "- contextual predictions (BERT),\n",
    "- linguistic structure (spaCy),\n",
    "- and domain consistency (gold vocabulary).\n",
    "\n",
    "This hybrid ensemble typically outperforms each component alone.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919c9b83",
   "metadata": {},
   "source": [
    "#### import and file paths\n",
    "We load:\n",
    "- the **train** file to extract the gold vocabulary,\n",
    "- the **dev** file (gold) for evaluation and text,\n",
    "- BERT and spaCy predictions on the dev set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "35713571",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, os\n",
    "def load_json(path: str):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "    \n",
    "\n",
    "def save_json(obj, path: str):\n",
    "    os.makedirs(os.path.dirname(path) or \".\", exist_ok=True)\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(obj, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"✓ Saved cleaned predictions to {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fc351393",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"../data/\"\n",
    "PRED_DIR = \"../src/predictions/\"\n",
    "\n",
    "TRAIN_FILE = os.path.join(DATA_DIR, \"subtask_a_train.json\")\n",
    "DEV_FILE = os.path.join(DATA_DIR, \"subtask_a_dev.json\")\n",
    "\n",
    "BERT_DEV_PRED_FILE = os.path.join(\n",
    "    PRED_DIR, \"subtask_a_dev_bert_preds_2e-5_cased_cleaned.json\"\n",
    ")\n",
    "SPACY_DEV_PRED_FILE = os.path.join(\n",
    "    PRED_DIR, \"subtask_a_dev_spacy_trained_preds.json\"\n",
    ")\n",
    "\n",
    "ENSEMBLE_OUT_FILE = os.path.join(\n",
    "    PRED_DIR, \"subtask_a_dev_ensemble_bert_2e-5_cased_spacy_dictfilter.json\"\n",
    ")\n",
    "\n",
    "os.makedirs(PRED_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b91e87aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../src/predictions/subtask_a_dev_bert_preds_2e-5_cased_cleaned.json\n",
      "../src/predictions/subtask_a_dev_spacy_trained_preds.json\n"
     ]
    }
   ],
   "source": [
    "print(BERT_DEV_PRED_FILE)\n",
    "print(SPACY_DEV_PRED_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114d1230",
   "metadata": {},
   "source": [
    "### 1. Normalization and training vocabulary\n",
    "\n",
    "We first define a canonical normalization function `norm()` and build:\n",
    "- a **normalized vocabulary** of gold terms from the training set,\n",
    "- a helper map from prediction JSONs to `(doc, par, sent) → term_list`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fb6b47ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "\n",
    "def norm(t: str) -> str:\n",
    "    if not t:\n",
    "        return \"\"\n",
    "    t = t.lower()\n",
    "    t = unicodedata.normalize(\"NFKC\", t)\n",
    "    t = t.replace(\"’\", \"'\").replace(\"`\", \"'\")\n",
    "    t = t.replace(\"“\", '\"').replace(\"”\", '\"')\n",
    "    t = \" \".join(t.split())\n",
    "    # strip punteggiatura ai bordi\n",
    "    t = t.strip(\".,;:-'\\\"()[]{}\")\n",
    "    return t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e070cd7",
   "metadata": {},
   "source": [
    "## 2. Build a gold-derived vocabulary\n",
    "\n",
    "We now construct a **normalized vocabulary** of domain terms from the gold training set.  \n",
    "This vocabulary is later used to:\n",
    "- validate candidate terms,\n",
    "- decide which spaCy spans are trustworthy,\n",
    "- avoid keeping generic words that never appear as gold terms.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3beb4aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_train_vocab(train_data: dict) -> set:\n",
    "    vocab = set()\n",
    "    for entry in train_data[\"data\"]:\n",
    "        for term in entry.get(\"term_list\", []):\n",
    "            n = norm(term)\n",
    "            if n:\n",
    "                vocab.add(n)\n",
    "    return vocab\n",
    "\n",
    "\n",
    "def build_term_map(pred_json: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Build a mapping:\n",
    "        (document_id, paragraph_id, sentence_id) -> list of predicted terms\n",
    "    from a prediction JSON in the ATE-IT format.\n",
    "    \"\"\"\n",
    "    m = {}\n",
    "    for e in pred_json[\"data\"]:\n",
    "        key = (e[\"document_id\"], e[\"paragraph_id\"], e[\"sentence_id\"])\n",
    "        m[key] = e.get(\"term_list\", []) or []\n",
    "    return m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49fcf46",
   "metadata": {},
   "source": [
    "#### 3. Helper: map predictions to sentence IDs\n",
    "\n",
    "We define a helper function that converts a prediction JSON in ATE-IT format into a dictionary:\n",
    "`(document_id, paragraph_id, sentence_id) -> list of predicted terms`.\n",
    "\n",
    "This makes it easy to align BERT and spaCy predictions for the same sentence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4014db25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def build_train_vocab_with_freq(train_data):\n",
    "    freq = Counter()\n",
    "    for e in train_data[\"data\"]:\n",
    "        for term in e.get(\"term_list\", []):\n",
    "            norm = norm(term)\n",
    "            if norm:\n",
    "                freq[norm] += 1\n",
    "    strong = {t for t, c in freq.items() if c >= 3}\n",
    "    weak   = {t for t, c in freq.items() if c == 1}\n",
    "    return freq, strong, weak\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2683b37a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' def norm(t: str) -> str:\\n    if not t:\\n        return \"\"\\n    t = t.lower().strip()\\n    t = \" \".join(t.split())\\n    t = t.replace(\"’\", \"\\'\")\\n    t = t.strip(\".,;:-\\'\"()[]{}\")\\n    return t '"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" def norm(t: str) -> str:\n",
    "    if not t:\n",
    "        return \"\"\n",
    "    t = t.lower().strip()\n",
    "    t = \" \".join(t.split())\n",
    "    t = t.replace(\"’\", \"'\")\n",
    "    t = t.strip(\".,;:-'\\\"()[]{}\")\n",
    "    return t \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68cac7ae",
   "metadata": {},
   "source": [
    "### 3. Generic heads and acronym heuristics\n",
    "\n",
    "We now define:\n",
    "- a small list of **generic heads** (e.g. *rifiuti, materiali, servizio*),\n",
    "- a heuristic to detect **acronyms** (e.g. *RAEE, R1, TARI*),\n",
    "- a filter for **generic unigrams** that never appear as gold terms.\n",
    "\n",
    "The goal is to:\n",
    "- keep important single-word terms if they are in the gold vocabulary,\n",
    "- discard only very generic heads that never occur as true domain terms.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1090afc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "GENERIC_HEADS = {\n",
    "    \"rifiuti\", \"materiali\", \"utenti\", \"plastica\", \"carta\",\n",
    "    \"residui\", \"tariffe\", \"gestore\", \"servizio\", \"modalità\",\n",
    "    \"conferimento\", \"costi\", \"parte\", \"quota\", \"impianto\"\n",
    "}\n",
    "def looks_like_acronym(n: str) -> bool:\n",
    "    # es: \"tmb\", \"raee\", \"r.a.e.e.\"\n",
    "    n_clean = n.replace(\".\", \"\")\n",
    "    return (len(n_clean) >= 2 and len(n_clean) <= 6 and n_clean.isalpha())\n",
    "\n",
    "def filter_generic_unigrams(terms, train_vocab_norm):\n",
    "    filtered = []\n",
    "    for t in terms:\n",
    "        n = norm(t)\n",
    "        tokens = n.split()\n",
    "        if len(tokens) == 1:\n",
    "            if n in GENERIC_HEADS and n not in train_vocab_norm and not looks_like_acronym(n):\n",
    "                continue\n",
    "            if n in GENERIC_HEADS and n not in train_vocab_norm:\n",
    "                # scarta \"quota\", \"parte\", ecc. se non compaiono mai come termini gold\n",
    "                continue\n",
    "        filtered.append(t)\n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de4ca50",
   "metadata": {},
   "source": [
    "## 4. Multiword upgrade: BERT → spaCy spans\n",
    "\n",
    "BERT sometimes predicts short fragments (e.g. *ferro*) where the gold term is a\n",
    "longer span (e.g. *materiali ferrosi*).\n",
    "\n",
    "We therefore:\n",
    "1. Look for **spaCy multiword spans** that:\n",
    "   - are present in the gold vocabulary,\n",
    "   - contain the BERT term as a contiguous token subsequence.\n",
    "2. If such a span exists, we **upgrade** the BERT term to the longer spaCy span.\n",
    "3. We also maintain a small list of **GENERIC_BAD** terms that we never keep.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "64f161d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "GENERIC_BAD = {\n",
    "    \"parte\", \"gestione\", \"città\", \"territorio\", \"comune\",\n",
    "    \"ore\", \"no\", \"si\", \"anno\", \"mese\", \"giorno\"\n",
    "} \n",
    "def contains_as_subspan(longer: str, shorter: str) -> bool:\n",
    "    long_tokens = longer.split()\n",
    "    short_tokens = shorter.split()\n",
    "    L, S = len(long_tokens), len(short_tokens)\n",
    "    if S > L:\n",
    "        return False\n",
    "    for i in range(L - S + 1):\n",
    "        if long_tokens[i:i+S] == short_tokens:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def upgrade_with_longer_spacy(bert_terms, spacy_terms, train_vocab_norm):\n",
    "    \"\"\"\n",
    "    Upgrade BERT terms to longer spaCy spans ONLY WHEN BENEFICIAL.\n",
    "    \"\"\"\n",
    "    final = []\n",
    "    seen = set()\n",
    "    \n",
    "    spacy_norm_map = {norm(t): t for t in spacy_terms or []}\n",
    "\n",
    "    for b in bert_terms or []:\n",
    "        b_norm = norm(b)\n",
    "        if not b_norm or b_norm in GENERIC_BAD:\n",
    "            continue\n",
    "\n",
    "        best = None\n",
    "\n",
    "        # search longest valid spaCy span containing the BERT term\n",
    "        for s_norm, s in spacy_norm_map.items():\n",
    "            if len(s_norm.split()) < 2:\n",
    "                continue\n",
    "            if s_norm not in train_vocab_norm:\n",
    "                continue\n",
    "            if contains_as_subspan(s_norm, b_norm):\n",
    "                if best is None or len(s_norm.split()) > len(norm(best).split()):\n",
    "                    best = s\n",
    "\n",
    "\n",
    "        chosen = best if best else b\n",
    "        c_norm = norm(chosen)\n",
    "\n",
    "        if c_norm not in seen and c_norm not in GENERIC_BAD:\n",
    "            final.append(chosen)\n",
    "            seen.add(c_norm)\n",
    "\n",
    "    return final\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79f9a59",
   "metadata": {},
   "source": [
    "### 5. BERT + spaCy + vocabulary merge\n",
    "now define the main merge function that combines:\n",
    "- cleaned **BERT terms**,\n",
    "- **spaCy noun-chunk spans**,\n",
    "- and the **gold-derived vocabulary**.\n",
    "\n",
    "The strategy is:\n",
    "1. First, **upgrade** BERT terms to longer spaCy spans when they match a gold term.\n",
    "2. Then, **add extra spaCy multiword spans** that:\n",
    "   - are in the gold vocabulary,\n",
    "   - are not already covered,\n",
    "   - are not clearly generic or meaningless.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "32bb4936",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_bert_spacy_with_dict(bert_terms, spacy_terms, train_vocab_norm):\n",
    "    \"\"\"\n",
    "    BEST ensemble so far:\n",
    "    1. upgrade BERT with spaCy\n",
    "    2. add dictionary-filtered spaCy spans\n",
    "    3. skip generic or meaningless words\n",
    "    \"\"\"\n",
    "    upgraded = upgrade_with_longer_spacy(\n",
    "        bert_terms=bert_terms,\n",
    "        spacy_terms=spacy_terms,\n",
    "        train_vocab_norm=train_vocab_norm,\n",
    "    )\n",
    "\n",
    "    final = upgraded[:]\n",
    "    seen = {norm(t) for t in upgraded}\n",
    "\n",
    "    for s in spacy_terms or []:\n",
    "        s_norm = norm(s)\n",
    "\n",
    "        if len(s.split()) < 2:\n",
    "            continue\n",
    "        if s_norm not in train_vocab_norm:\n",
    "            continue\n",
    "        if s_norm in seen:\n",
    "            continue\n",
    "        if s_norm in GENERIC_BAD:\n",
    "            continue\n",
    "\n",
    "        final.append(s)\n",
    "        seen.add(s_norm)\n",
    "\n",
    "    return final\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36fc5b01",
   "metadata": {},
   "source": [
    "### 6. Per-sentence merge helper\n",
    " helper `merge_sentence()` that:\n",
    "1. Applies the BERT+spaCy+vocabulary merge strategy.\n",
    "2. Removes only **truly generic unigrams** (using the gold vocabulary as a whitelist).\n",
    "3. Normalizes and deduplicates the final term list.\n",
    "\n",
    "This function is called once per sentence in the dev/test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "35d36815",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_sentence(bert_terms, spacy_terms, train_vocab_norm):\n",
    "    merged = merge_bert_spacy_with_dict(\n",
    "        bert_terms=bert_terms,\n",
    "        spacy_terms=spacy_terms,\n",
    "        train_vocab_norm=train_vocab_norm\n",
    "    )\n",
    "    merged = filter_generic_unigrams(merged, train_vocab_norm)\n",
    "\n",
    "    # dedupe and normalize\n",
    "    seen = set()\n",
    "    final = []\n",
    "    for t in merged:\n",
    "        n = norm(t)\n",
    "        if n not in seen:\n",
    "            final.append(n)\n",
    "            seen.add(n)\n",
    "    return final\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9d3d67",
   "metadata": {},
   "source": [
    "### 7. BUILD BERT + SPACY ENSEMBLE USING merge_sentence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "48532509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# unique normalized terms from train gold: 710\n",
      "Building improved BERT+spaCy ensemble ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 577/577 [00:00<00:00, 288555.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------------------------------\n",
      "Sentence 0\n",
      "TEXT: Non Domestica; CAMPEGGI, DISTRIBUTORI CARBURANTI, PARCHEGGI; 1,22; 4,73 \n",
      "  BERT  : []\n",
      "  SPACY : []\n",
      "  MERGED: []\n",
      "\n",
      "---------------------------------------\n",
      "Sentence 1\n",
      "TEXT: Il presente disciplinare per la gestione dei centri di raccolta comunali è stato redatto ai sensi e per effetto del DM 13/05/2009, pubblicato sulla G.U. n. 165 del 18/07/2009, con il quale sono state apportate le modifiche sostanziali al DM 08/04/2008, Disciplina dei centri di raccolta dei rifiuti urbani raccolti in modo differenziato, come previsto dall'art. 183, comma 7, lettera cc) del Dlgs 3 aprile 2006, n. 152, e ss.mm.ii.\n",
      "  BERT  : ['gestione dei centri di raccolta comunali', 'centri di raccolta dei rifiuti urbani raccolti in modo differenziato']\n",
      "  SPACY : ['gestione dei centri di raccolta comunali', 'centri di raccolta dei rifiuti urbani raccolti']\n",
      "  MERGED: ['gestione dei centri di raccolta comunali', 'centri di raccolta dei rifiuti urbani raccolti in modo differenziato']\n",
      "\n",
      "---------------------------------------\n",
      "Sentence 2\n",
      "TEXT: Voti astenuti: 2 (Acampora Alessandro, Gargiulo Mario)\n",
      "  BERT  : []\n",
      "  SPACY : []\n",
      "  MERGED: []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "# ---- Load train data and build vocabulary ----\n",
    "with open(TRAIN_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "    train_data = json.load(f)\n",
    "\n",
    "train_vocab_norm = build_train_vocab(train_data)\n",
    "print(f\"# unique normalized terms from train gold: {len(train_vocab_norm)}\")\n",
    "\n",
    "# ---- Load dev gold (for evaluation) ----\n",
    "with open(DEV_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "    dev_data = json.load(f)\n",
    "\n",
    "# ---- Load BERT and spaCy predictions ----\n",
    "with open(BERT_DEV_PRED_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "    bert_pred = json.load(f)\n",
    "\n",
    "with open(SPACY_DEV_PRED_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "    spacy_pred = json.load(f)\n",
    "\n",
    "# Convert JSON predictions → dict[(doc,par,sent)] → [terms...]\n",
    "bert_map = build_term_map(bert_pred)\n",
    "spacy_map = build_term_map(spacy_pred)\n",
    "\n",
    "# ---- Build ensemble predictions using merge_sentence ----\n",
    "ensemble_output = {\"data\": []}\n",
    "\n",
    "print(\"Building improved BERT+spaCy ensemble ...\")\n",
    "\n",
    "for idx, row in enumerate(tqdm(dev_data[\"data\"])):\n",
    "\n",
    "    key = (row[\"document_id\"], row[\"paragraph_id\"], row[\"sentence_id\"])\n",
    "\n",
    "    bert_terms = bert_map.get(key, []) or []\n",
    "    spacy_terms = spacy_map.get(key, []) or []\n",
    "\n",
    "    #  NEW MERGE FUNCTION \n",
    "    merged_terms = merge_sentence(\n",
    "        bert_terms=bert_terms,\n",
    "        spacy_terms=spacy_terms,\n",
    "        train_vocab_norm=train_vocab_norm\n",
    "    )\n",
    "\n",
    "    # Debug on first 3\n",
    "    if idx < 3:\n",
    "        print(\"\\n---------------------------------------\")\n",
    "        print(\"Sentence\", idx)\n",
    "        print(\"TEXT:\", row[\"sentence_text\"])\n",
    "        print(\"  BERT  :\", bert_terms)\n",
    "        print(\"  SPACY :\", spacy_terms)\n",
    "        print(\"  MERGED:\", merged_terms)\n",
    "\n",
    "    # Save\n",
    "    ensemble_output[\"data\"].append({\n",
    "        \"document_id\": row[\"document_id\"],\n",
    "        \"paragraph_id\": row[\"paragraph_id\"],\n",
    "        \"sentence_id\": row[\"sentence_id\"],\n",
    "        \"term_list\": merged_terms,\n",
    "    })\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5f2d4e",
   "metadata": {},
   "source": [
    "#### Save predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "921fb996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ensemble predictions saved to: ../src/predictions/subtask_a_dev_ensemble_bert_2e-5_cased_spacy_dictfilter.json\n"
     ]
    }
   ],
   "source": [
    "# ---- Save final merged predictions ----\n",
    "with open(ENSEMBLE_OUT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(ensemble_output, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"\\nEnsemble predictions saved to: {ENSEMBLE_OUT_FILE}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f5891f",
   "metadata": {},
   "source": [
    "### EVALUATION AND RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5fed8305",
   "metadata": {},
   "outputs": [],
   "source": [
    "def type_f1_score(gold_standard, system_output):\n",
    "  \"\"\"\n",
    "  Evaluates a term extraction system's performance using Type Precision,\n",
    "  Type Recall, and Type F1 score based on the set of unique terms extracted\n",
    "  at least once across the entire dataset.\n",
    "\n",
    "  Args:\n",
    "    gold_standard: A list of lists, where each inner list contains the\n",
    "                   gold standard terms for an item.\n",
    "    system_output: A list of lists, where each inner list contains the\n",
    "                   terms extracted by the system for the corresponding item.\n",
    "\n",
    "  Returns:\n",
    "    A tuple containing the Type Precision, Type Recall, and Type F1 score.\n",
    "  \"\"\"\n",
    "\n",
    "  # Get the set of all unique gold standard terms across the dataset\n",
    "  all_gold_terms = set()\n",
    "  for item_terms in gold_standard:\n",
    "    all_gold_terms.update(item_terms)\n",
    "\n",
    "  # Get the set of all unique system extracted terms across the dataset\n",
    "  all_system_terms = set()\n",
    "  for item_terms in system_output:\n",
    "    all_system_terms.update(item_terms)\n",
    "\n",
    "  # Calculate True Positives (terms present in both sets)\n",
    "  type_true_positives = len(all_gold_terms.intersection(all_system_terms))\n",
    "\n",
    "  # Calculate False Positives (terms in system output but not in gold standard)\n",
    "  type_false_positives = len(all_system_terms - all_gold_terms)\n",
    "\n",
    "  # Calculate False Negatives (terms in gold standard but not in system output)\n",
    "  type_false_negatives = len(all_gold_terms - all_system_terms)\n",
    "\n",
    "  # Calculate Type Precision, Type Recall, and Type F1 score\n",
    "  type_precision = type_true_positives / (type_true_positives + type_false_positives) if (type_true_positives + type_false_positives) > 0 else 0\n",
    "  type_recall = type_true_positives / (type_true_positives + type_false_negatives) if (type_true_positives + type_false_negatives) > 0 else 0\n",
    "  type_f1 = 2 * (type_precision * type_recall) / (type_precision + type_recall) if (type_precision + type_recall) > 0 else 0\n",
    "\n",
    "  return type_precision, type_recall, type_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5ad86e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "                   \n",
    "def micro_f1_score(gold_standard, system_output):\n",
    "  \"\"\"\n",
    "  Evaluates a term extraction system's performance using Precision, Recall,\n",
    "  and F1 score based on individual term matching (micro-average).\n",
    "\n",
    "  Args:\n",
    "    gold_standard: A list of lists, where each inner list contains the\n",
    "        gold standard terms for an item.\n",
    "    system_output: A list of lists, where each inner list contains the\n",
    "                   terms extracted by the system for the corresponding item.\n",
    "\n",
    "  Returns:\n",
    "    A tuple containing the Precision, Recall, and F1 score.\n",
    "  \"\"\"\n",
    "  total_true_positives = 0\n",
    "  total_false_positives = 0\n",
    "  total_false_negatives = 0\n",
    "\n",
    "  # Iterate through each item's gold standard and system output terms\n",
    "  for gold, system in zip(gold_standard, system_output):\n",
    "    # Convert to sets for efficient comparison\n",
    "    gold_set = set(gold)\n",
    "    system_set = set(system)\n",
    "\n",
    "    # Calculate True Positives, False Positives, and False Negatives for the current item\n",
    "    true_positives = len(gold_set.intersection(system_set))\n",
    "    false_positives = len(system_set - gold_set)\n",
    "    false_negatives = len(gold_set - system_set)\n",
    "\n",
    "    # Accumulate totals across all items\n",
    "    total_true_positives += true_positives\n",
    "    total_false_positives += false_positives\n",
    "    total_false_negatives += false_negatives\n",
    "\n",
    "  # Calculate Precision, Recall, and F1 score (micro-average)\n",
    "  precision = total_true_positives / (total_true_positives + total_false_positives) if (total_true_positives + total_false_positives) > 0 else 0\n",
    "  recall = total_true_positives / (total_true_positives + total_false_negatives) if (total_true_positives + total_false_negatives) > 0 else 0\n",
    "  f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "  return precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7162f311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=====================================================\n",
      "    IMPROVED BERT + SPACY + DICTIONARY MERGE\n",
      "=====================================================\n",
      "\n",
      "Micro-averaged Metrics:\n",
      "  Precision: 0.7277\n",
      "  Recall:    0.7406\n",
      "  F1 Score:  0.7341\n",
      "\n",
      "Type-level Metrics:\n",
      "  Type Precision: 0.6443\n",
      "  Type Recall:    0.6736\n",
      "  Type F1 Score:  0.6586\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Extract gold + predicted lists\n",
    "dev_gold = [entry[\"term_list\"] for entry in dev_data[\"data\"]]\n",
    "ensemble_preds = [entry[\"term_list\"] for entry in ensemble_output[\"data\"]]\n",
    "\n",
    "precision, recall, f1 = micro_f1_score(dev_gold, ensemble_preds)\n",
    "type_precision, type_recall, type_f1 = type_f1_score(dev_gold, ensemble_preds)\n",
    "\n",
    "print(\"\\n=====================================================\")\n",
    "print(\"    IMPROVED BERT + SPACY + DICTIONARY MERGE\")\n",
    "print(\"=====================================================\")\n",
    "\n",
    "print(\"\\nMicro-averaged Metrics:\")\n",
    "print(f\"  Precision: {precision:.4f}\")\n",
    "print(f\"  Recall:    {recall:.4f}\")\n",
    "print(f\"  F1 Score:  {f1:.4f}\")\n",
    "\n",
    "print(\"\\nType-level Metrics:\")\n",
    "print(f\"  Type Precision: {type_precision:.4f}\")\n",
    "print(f\"  Type Recall:    {type_recall:.4f}\")\n",
    "print(f\"  Type F1 Score:  {type_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc890426",
   "metadata": {},
   "source": [
    "#### Error Analysis: False Positives and False Negatives\n",
    "\n",
    "To better understand model behaviour, we compute:\n",
    "- **False Positives (FP):** predicted terms that are not present in the gold list.\n",
    "- **False Negatives (FN):** gold terms that the system failed to predict.\n",
    "\n",
    "This helps identify:\n",
    "- systematic missing multiword expressions,\n",
    "- over-predicted generic terms,\n",
    "- vocabulary mismatches,\n",
    "- potential improvements in filtering or upgrading logic.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b0d24159",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_fp_fn_from_listformat(gold_entries, pred_entries):\n",
    "    \"\"\"\n",
    "    Compute false positives and false negatives by expanding each sentence-level\n",
    "    term list into flat (doc, par, sent, term) rows and comparing them as sets.\n",
    "\n",
    "    Returns:\n",
    "        fp_df : DataFrame of false positives\n",
    "        fn_df : DataFrame of false negatives\n",
    "    \"\"\"\n",
    "\n",
    "    gold_rows = []\n",
    "    pred_rows = []\n",
    "\n",
    "    # ---- Expand GOLD ----\n",
    "    for e in gold_entries:\n",
    "        key = (e[\"document_id\"], e[\"paragraph_id\"], e[\"sentence_id\"])\n",
    "        for t in e.get(\"term_list\", []):\n",
    "            n = norm(t)\n",
    "            if n:\n",
    "                gold_rows.append((*key, n))\n",
    "\n",
    "    # ---- Expand PRED ----\n",
    "    for e in pred_entries:\n",
    "        key = (e[\"document_id\"], e[\"paragraph_id\"], e[\"sentence_id\"])\n",
    "        for t in e.get(\"term_list\", []):\n",
    "            n = norm(t)\n",
    "            if n:\n",
    "                pred_rows.append((*key, n))\n",
    "\n",
    "    # ---- Compute FP / FN ----\n",
    "    gold_set = set(gold_rows)\n",
    "    pred_set = set(pred_rows)\n",
    "\n",
    "    fp = sorted(list(pred_set - gold_set))\n",
    "    fn = sorted(list(gold_set - pred_set))\n",
    "\n",
    "    fp_df = pd.DataFrame(fp, columns=[\"document_id\", \"paragraph_id\", \"sentence_id\", \"term\"])\n",
    "    fn_df = pd.DataFrame(fn, columns=[\"document_id\", \"paragraph_id\", \"sentence_id\", \"term\"])\n",
    "\n",
    "    return fp_df, fn_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7c99fdde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False Positives: 125\n",
      "False Negatives: 117\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document_id</th>\n",
       "      <th>paragraph_id</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>term</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>doc_agropoli_09</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>denominazione conferita</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>doc_auletta_01</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>isee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>doc_auletta_13</td>\n",
       "      <td>36</td>\n",
       "      <td>1</td>\n",
       "      <td>segnalazione per disservizi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>doc_battipaglia_02</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>ambiente e gestione dei rifiuti</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>doc_battipaglia_02</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>forme di gestione dei rifiuti</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>doc_battipaglia_13</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>manutenzione verde pubblico</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>doc_capaccio_06</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>oli conferiti</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>doc_capaccio_06</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>oli esausti</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>doc_capaccio_06</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>recupero</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>doc_capaccio_06</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>trasporto</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>doc_capaccio_15</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>pile portatili</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>doc_capaccio_15</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>utenze domestiche</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>doc_capaccio_15</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>sacchetti</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>doc_capaccio_27</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>utenti</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>doc_caserta_02</td>\n",
       "      <td>51</td>\n",
       "      <td>2</td>\n",
       "      <td>materiale fertilizzante</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>doc_caserta_02</td>\n",
       "      <td>64</td>\n",
       "      <td>9</td>\n",
       "      <td>lavatrici</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>doc_caserta_02</td>\n",
       "      <td>66</td>\n",
       "      <td>2</td>\n",
       "      <td>neon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>doc_caserta_06</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>centri di raccolta dei rifiuti urbani raccolti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>doc_caserta_06</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>gestione dei centri di raccolta comunali</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>doc_caserta_06</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>centro di raccolta</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           document_id  paragraph_id  sentence_id  \\\n",
       "0      doc_agropoli_09            11            0   \n",
       "1       doc_auletta_01             9            0   \n",
       "2       doc_auletta_13            36            1   \n",
       "3   doc_battipaglia_02             6            0   \n",
       "4   doc_battipaglia_02            20            0   \n",
       "5   doc_battipaglia_13            20            0   \n",
       "6      doc_capaccio_06            10            1   \n",
       "7      doc_capaccio_06            10            1   \n",
       "8      doc_capaccio_06            10            1   \n",
       "9      doc_capaccio_06            10            1   \n",
       "10     doc_capaccio_15             5           12   \n",
       "11     doc_capaccio_15             5           12   \n",
       "12     doc_capaccio_15            10            4   \n",
       "13     doc_capaccio_27             4            4   \n",
       "14      doc_caserta_02            51            2   \n",
       "15      doc_caserta_02            64            9   \n",
       "16      doc_caserta_02            66            2   \n",
       "17      doc_caserta_06             3            1   \n",
       "18      doc_caserta_06             3            1   \n",
       "19      doc_caserta_06             6            2   \n",
       "\n",
       "                                                 term  \n",
       "0                             denominazione conferita  \n",
       "1                                                isee  \n",
       "2                         segnalazione per disservizi  \n",
       "3                     ambiente e gestione dei rifiuti  \n",
       "4                       forme di gestione dei rifiuti  \n",
       "5                         manutenzione verde pubblico  \n",
       "6                                       oli conferiti  \n",
       "7                                         oli esausti  \n",
       "8                                            recupero  \n",
       "9                                           trasporto  \n",
       "10                                     pile portatili  \n",
       "11                                  utenze domestiche  \n",
       "12                                          sacchetti  \n",
       "13                                             utenti  \n",
       "14                            materiale fertilizzante  \n",
       "15                                          lavatrici  \n",
       "16                                               neon  \n",
       "17  centri di raccolta dei rifiuti urbani raccolti...  \n",
       "18           gestione dei centri di raccolta comunali  \n",
       "19                                 centro di raccolta  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document_id</th>\n",
       "      <th>paragraph_id</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>term</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>doc_agropoli_13</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>conferire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>doc_auletta_13</td>\n",
       "      <td>36</td>\n",
       "      <td>1</td>\n",
       "      <td>gestore dello spazzamento e lavaggio delle strade</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>doc_auletta_13</td>\n",
       "      <td>40</td>\n",
       "      <td>2</td>\n",
       "      <td>condizioni igieniche e di decoro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>doc_battipaglia_02</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>gestione dei rifiuti</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>doc_battipaglia_02</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>emissione</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>doc_battipaglia_13</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>carta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>doc_capaccio_06</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>raccolta degli oli esausti</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>doc_capaccio_06</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>conferiti</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>doc_capaccio_06</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>recupero degli oli esausti</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>doc_capaccio_10</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>sacchetto trasparente</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>doc_capaccio_10</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>sacchetto trasparente</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>doc_capaccio_10</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>sacchetto trasparente</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>doc_capaccio_10</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>busta con legaccio</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>doc_capaccio_10</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>sacchetto trasparente</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>doc_capaccio_10</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "      <td>busta con legaccio</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>doc_capaccio_15</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>pile portatili, batterie e accumulatori al pio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>doc_capaccio_21</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>imballaggi in cartone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>doc_caserta_02</td>\n",
       "      <td>51</td>\n",
       "      <td>3</td>\n",
       "      <td>tarsu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>doc_caserta_02</td>\n",
       "      <td>64</td>\n",
       "      <td>9</td>\n",
       "      <td>r.a.e.e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>doc_caserta_02</td>\n",
       "      <td>64</td>\n",
       "      <td>12</td>\n",
       "      <td>r.a.e.e</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           document_id  paragraph_id  sentence_id  \\\n",
       "0      doc_agropoli_13             1           14   \n",
       "1       doc_auletta_13            36            1   \n",
       "2       doc_auletta_13            40            2   \n",
       "3   doc_battipaglia_02             6            0   \n",
       "4   doc_battipaglia_02            20            0   \n",
       "5   doc_battipaglia_13             2            3   \n",
       "6      doc_capaccio_06             7            1   \n",
       "7      doc_capaccio_06            10            1   \n",
       "8      doc_capaccio_06            10            1   \n",
       "9      doc_capaccio_10             3            3   \n",
       "10     doc_capaccio_10             6            6   \n",
       "11     doc_capaccio_10             8            3   \n",
       "12     doc_capaccio_10             9            4   \n",
       "13     doc_capaccio_10             9            6   \n",
       "14     doc_capaccio_10            14            4   \n",
       "15     doc_capaccio_15             5           12   \n",
       "16     doc_capaccio_21            21            1   \n",
       "17      doc_caserta_02            51            3   \n",
       "18      doc_caserta_02            64            9   \n",
       "19      doc_caserta_02            64           12   \n",
       "\n",
       "                                                 term  \n",
       "0                                           conferire  \n",
       "1   gestore dello spazzamento e lavaggio delle strade  \n",
       "2                    condizioni igieniche e di decoro  \n",
       "3                                gestione dei rifiuti  \n",
       "4                                           emissione  \n",
       "5                                               carta  \n",
       "6                          raccolta degli oli esausti  \n",
       "7                                           conferiti  \n",
       "8                          recupero degli oli esausti  \n",
       "9                               sacchetto trasparente  \n",
       "10                              sacchetto trasparente  \n",
       "11                              sacchetto trasparente  \n",
       "12                                 busta con legaccio  \n",
       "13                              sacchetto trasparente  \n",
       "14                                 busta con legaccio  \n",
       "15  pile portatili, batterie e accumulatori al pio...  \n",
       "16                              imballaggi in cartone  \n",
       "17                                              tarsu  \n",
       "18                                            r.a.e.e  \n",
       "19                                            r.a.e.e  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gold_entries = dev_data[\"data\"]      # gold JSON\n",
    "pred_entries = ensemble_output[\"data\"]  # merged predictions JSON\n",
    "\n",
    "fp_df, fn_df = get_fp_fn_from_listformat(gold_entries, pred_entries)\n",
    "\n",
    "print(\"False Positives:\", len(fp_df))\n",
    "print(\"False Negatives:\", len(fn_df))\n",
    "\n",
    "display(fp_df.head(20))\n",
    "display(fn_df.head(20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce02f204",
   "metadata": {},
   "source": [
    "#### Sentence-level error table\n",
    "\n",
    "To inspect model behavior at the sentence level, we build a table where each row\n",
    "corresponds to one sentence that has at least one error (FP or FN).\n",
    "\n",
    "For each sentence we store:\n",
    "- the number of gold and predicted terms,\n",
    "- how many **missing** terms (FN) and **extra** terms (FP),\n",
    "- the list of missing/extra terms (normalized),\n",
    "- the original sentence text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "422e2c0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error table shape: (129, 10)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document_id</th>\n",
       "      <th>paragraph_id</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>n_gold</th>\n",
       "      <th>n_pred</th>\n",
       "      <th>n_missing</th>\n",
       "      <th>missing_terms</th>\n",
       "      <th>n_extra</th>\n",
       "      <th>extra_terms</th>\n",
       "      <th>sentence_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>doc_caserta_06</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[disciplina dei centri di raccolta dei rifiuti...</td>\n",
       "      <td>2</td>\n",
       "      <td>[centri di raccolta dei rifiuti urbani raccolt...</td>\n",
       "      <td>Il presente disciplinare per la gestione dei c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>doc_poggiomarino_01</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[raccolta]</td>\n",
       "      <td>1</td>\n",
       "      <td>[servizio supplementare di raccolta]</td>\n",
       "      <td>È un Servizio Supplementare di raccolta, rivol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>doc_nola_05</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "      <td>[servizio di raccolta dei rifiuti derivanti da...</td>\n",
       "      <td>ll servizio di raccolta dei rifiuti derivanti ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>doc_poggiomarino_12</td>\n",
       "      <td>17</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "      <td>[carta]</td>\n",
       "      <td>- giornali; - la carta per alimenti;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>doc_capaccio_10</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[sacchetto trasparente]</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>MULTIMATERIALE; Sacchetto blu trasparente; Lun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>doc_salerno_05</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "      <td>[tessuto]</td>\n",
       "      <td>Indumenti usati, accessori, lenzuola, coperte,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>doc_caserta_06</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[gestione del centro di raccolta]</td>\n",
       "      <td>1</td>\n",
       "      <td>[centro di raccolta]</td>\n",
       "      <td>- alla vigilanza nel rispetto delle norme del ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>doc_capaccio_15</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>[pile portatili, batterie e accumulatori al pi...</td>\n",
       "      <td>2</td>\n",
       "      <td>[pile portatili, utenze domestiche]</td>\n",
       "      <td>PILE PORTATILI, BATTERIE E ACCUMULATORI AL PIO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>doc_nola_05</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>[frazione verde, ritiro]</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>RITIRO FRAZIONE VERDE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>doc_salerno_05</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>[conferiti, plastica, acciaio e alluminio]</td>\n",
       "      <td>5</td>\n",
       "      <td>[acciaio, alluminio, cartoni per liquidi, plas...</td>\n",
       "      <td>I cartoni per liquidi vanno conferiti con plas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>doc_santegidiodelmontealbino_03</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "      <td>[scarti e avanzi]</td>\n",
       "      <td>- scarti e avanzi di frutta, ortaggi e verdura;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>doc_salerno_03</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "      <td>[nuova]</td>\n",
       "      <td>\"Una raccolta di successo\": i risultati dei pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>doc_pellezzano_01</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "      <td>[delibera]</td>\n",
       "      <td>DELIBERA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>doc_nocerainferiore_06</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "      <td>[indicazioni]</td>\n",
       "      <td>Per vincere questa sfida è fondamentale il ris...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>doc_salerno_03</td>\n",
       "      <td>2</td>\n",
       "      <td>27</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>[sacchetti, spazzatura]</td>\n",
       "      <td>2</td>\n",
       "      <td>[materiali finiti, sacchetti del non differenz...</td>\n",
       "      <td>Risultati di rilievo se si considera che a set...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>doc_santegidiodelmontealbino_03</td>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "      <td>[depositare]</td>\n",
       "      <td>In caso di abitazioni condominiali depositare ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>doc_capaccio_15</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "      <td>[sacchetti]</td>\n",
       "      <td>Carta: sacchetti, giornali, riviste, libri, qu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>doc_nola_02</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[responsabile del centro di raccolta]</td>\n",
       "      <td>1</td>\n",
       "      <td>[centro di raccolta]</td>\n",
       "      <td>a)Nominare il responsabile del centro di racco...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>doc_nocerainferiore_06</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>[calendario utenze domestiche per la raccolta ...</td>\n",
       "      <td>2</td>\n",
       "      <td>[calendario, utenze domestiche per la raccolta...</td>\n",
       "      <td>CALENDARIO UTENZE DOMESTICHE PER LA RACCOLTA D...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>doc_marigliano_01</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>[abbandono di rifiuti non pericolosi e non ing...</td>\n",
       "      <td>1</td>\n",
       "      <td>[abbandono di rifiuti non pericolosi]</td>\n",
       "      <td>a) € 51,65 per l'abbandono di rifiuti non peri...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        document_id  paragraph_id  sentence_id  n_gold  \\\n",
       "0                    doc_caserta_06             3            1       2   \n",
       "1               doc_poggiomarino_01             6            1       1   \n",
       "2                       doc_nola_05             2            2       2   \n",
       "3               doc_poggiomarino_12            17            4       0   \n",
       "4                   doc_capaccio_10             3            3       2   \n",
       "5                    doc_salerno_05            11            2       1   \n",
       "6                    doc_caserta_06             6            2       1   \n",
       "7                   doc_capaccio_15             5           12       1   \n",
       "8                       doc_nola_05             2            0       2   \n",
       "9                    doc_salerno_05             7            6       2   \n",
       "10  doc_santegidiodelmontealbino_03             6            4       0   \n",
       "11                   doc_salerno_03             1            0       2   \n",
       "12                doc_pellezzano_01            28            0       0   \n",
       "13           doc_nocerainferiore_06             2            2       1   \n",
       "14                   doc_salerno_03             2           27       3   \n",
       "15  doc_santegidiodelmontealbino_03            15            4       3   \n",
       "16                  doc_capaccio_15            10            4       1   \n",
       "17                      doc_nola_02            12            2       1   \n",
       "18           doc_nocerainferiore_06            10            0       1   \n",
       "19                doc_marigliano_01             9            1       2   \n",
       "\n",
       "    n_pred  n_missing                                      missing_terms  \\\n",
       "0        2          2  [disciplina dei centri di raccolta dei rifiuti...   \n",
       "1        1          1                                         [raccolta]   \n",
       "2        3          0                                                 []   \n",
       "3        1          0                                                 []   \n",
       "4        1          1                            [sacchetto trasparente]   \n",
       "5        2          0                                                 []   \n",
       "6        1          1                  [gestione del centro di raccolta]   \n",
       "7        2          1  [pile portatili, batterie e accumulatori al pi...   \n",
       "8        0          2                           [frazione verde, ritiro]   \n",
       "9        5          2         [conferiti, plastica, acciaio e alluminio]   \n",
       "10       1          0                                                 []   \n",
       "11       3          0                                                 []   \n",
       "12       1          0                                                 []   \n",
       "13       2          0                                                 []   \n",
       "14       3          2                            [sacchetti, spazzatura]   \n",
       "15       4          0                                                 []   \n",
       "16       2          0                                                 []   \n",
       "17       1          1              [responsabile del centro di raccolta]   \n",
       "18       2          1  [calendario utenze domestiche per la raccolta ...   \n",
       "19       1          2  [abbandono di rifiuti non pericolosi e non ing...   \n",
       "\n",
       "    n_extra                                        extra_terms  \\\n",
       "0         2  [centri di raccolta dei rifiuti urbani raccolt...   \n",
       "1         1               [servizio supplementare di raccolta]   \n",
       "2         1  [servizio di raccolta dei rifiuti derivanti da...   \n",
       "3         1                                            [carta]   \n",
       "4         0                                                 []   \n",
       "5         1                                          [tessuto]   \n",
       "6         1                               [centro di raccolta]   \n",
       "7         2                [pile portatili, utenze domestiche]   \n",
       "8         0                                                 []   \n",
       "9         5  [acciaio, alluminio, cartoni per liquidi, plas...   \n",
       "10        1                                  [scarti e avanzi]   \n",
       "11        1                                            [nuova]   \n",
       "12        1                                         [delibera]   \n",
       "13        1                                      [indicazioni]   \n",
       "14        2  [materiali finiti, sacchetti del non differenz...   \n",
       "15        1                                       [depositare]   \n",
       "16        1                                        [sacchetti]   \n",
       "17        1                               [centro di raccolta]   \n",
       "18        2  [calendario, utenze domestiche per la raccolta...   \n",
       "19        1              [abbandono di rifiuti non pericolosi]   \n",
       "\n",
       "                                        sentence_text  \n",
       "0   Il presente disciplinare per la gestione dei c...  \n",
       "1   È un Servizio Supplementare di raccolta, rivol...  \n",
       "2   ll servizio di raccolta dei rifiuti derivanti ...  \n",
       "3                - giornali; - la carta per alimenti;  \n",
       "4   MULTIMATERIALE; Sacchetto blu trasparente; Lun...  \n",
       "5   Indumenti usati, accessori, lenzuola, coperte,...  \n",
       "6   - alla vigilanza nel rispetto delle norme del ...  \n",
       "7   PILE PORTATILI, BATTERIE E ACCUMULATORI AL PIO...  \n",
       "8                               RITIRO FRAZIONE VERDE  \n",
       "9   I cartoni per liquidi vanno conferiti con plas...  \n",
       "10    - scarti e avanzi di frutta, ortaggi e verdura;  \n",
       "11  \"Una raccolta di successo\": i risultati dei pr...  \n",
       "12                                           DELIBERA  \n",
       "13  Per vincere questa sfida è fondamentale il ris...  \n",
       "14  Risultati di rilievo se si considera che a set...  \n",
       "15  In caso di abitazioni condominiali depositare ...  \n",
       "16  Carta: sacchetti, giornali, riviste, libri, qu...  \n",
       "17  a)Nominare il responsabile del centro di racco...  \n",
       "18  CALENDARIO UTENZE DOMESTICHE PER LA RACCOLTA D...  \n",
       "19  a) € 51,65 per l'abbandono di rifiuti non peri...  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " \n",
    "def build_sentence_error_table(dev_data, pred_data):\n",
    "    \"\"\"\n",
    "    Build a table where each row corresponds to a sentence that has errors.\n",
    "    Coherent with:\n",
    "      - norm()\n",
    "      - collect_sentence_errors()\n",
    "      - merge structure\n",
    "      - (doc, par, sent) key\n",
    "    \"\"\"\n",
    "\n",
    "    # ---- Build gold map ----\n",
    "    gold_map = {}\n",
    "    for e in dev_data[\"data\"]:\n",
    "        key = (e[\"document_id\"], e[\"paragraph_id\"], e[\"sentence_id\"])\n",
    "        gold_map[key] = set(norm(t) for t in e.get(\"term_list\", []))\n",
    "\n",
    "    # ---- Build pred map ----\n",
    "    pred_map = {}\n",
    "    for e in pred_data[\"data\"]:\n",
    "        key = (e[\"document_id\"], e[\"paragraph_id\"], e[\"sentence_id\"])\n",
    "        pred_map[key] = set(norm(t) for t in e.get(\"term_list\", []))\n",
    "\n",
    "    # ---- Build rows ----\n",
    "    rows = []\n",
    "    for key in gold_map:\n",
    "        doc, par, sent = key\n",
    "        gold_set = gold_map[key]\n",
    "        pred_set = pred_map.get(key, set())\n",
    "\n",
    "        missing = gold_set - pred_set\n",
    "        extra   = pred_set - gold_set\n",
    "\n",
    "        if missing or extra:\n",
    "            rows.append({\n",
    "                \"document_id\": doc,\n",
    "                \"paragraph_id\": par,\n",
    "                \"sentence_id\": sent,\n",
    "                \"n_gold\": len(gold_set),\n",
    "                \"n_pred\": len(pred_set),\n",
    "                \"n_missing\": len(missing),\n",
    "                \"missing_terms\": sorted(missing),\n",
    "                \"n_extra\": len(extra),\n",
    "                \"extra_terms\": sorted(extra),\n",
    "                \"sentence_text\": next(\n",
    "                    r[\"sentence_text\"] \n",
    "                    for r in dev_data[\"data\"] \n",
    "                    if (r[\"document_id\"], r[\"paragraph_id\"], r[\"sentence_id\"]) == key\n",
    "                )\n",
    "            })\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    return df\n",
    "errors_df = build_sentence_error_table(dev_data, ensemble_output)\n",
    "print(f\"Error table shape: {errors_df.shape}\")\n",
    "errors_df.head(20)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
