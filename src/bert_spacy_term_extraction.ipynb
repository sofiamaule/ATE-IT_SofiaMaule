{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52af72d2",
   "metadata": {},
   "source": [
    "# **Term Extraction Ensemble (BERT + spaCy + Dictionary)**\n",
    "\n",
    "This notebook implements a complete post-processing pipeline for the ATE-IT Subtask A (Automatic Term Extraction).  \n",
    "It takes the raw predictions from a fine-tuned **BERT token classification model** and combines them with **spaCy noun-chunk spans** and a **gold-derived domain vocabulary** to produce a higher-quality list of domain terms for each sentence.\n",
    "\n",
    "### Pipeline Summary\n",
    "1. **Load BERT and spaCy predictions**  \n",
    "   - Import model outputs in ATE-IT JSON format.  \n",
    "   - Map predictions to sentence identifiers for easy lookup.\n",
    "\n",
    "2. **Normalize and clean BERT terms**  \n",
    "   - Remove punctuation, unify quotes, lowercase, collapse whitespace.  \n",
    "   - Filter out spurious or generic one-word candidates.\n",
    "\n",
    "3. **Build a domain vocabulary from the gold training set**  \n",
    "   - Normalize gold terms.  \n",
    "   - Track frequencies to identify strong (repeated) vs. weak (rare) terms.\n",
    "\n",
    "4. **Merge BERT + spaCy + Dictionary knowledge**  \n",
    "   - **Upgrade** short BERT terms to longer spaCy spans when they form a valid multi-word expression present in the gold vocabulary.  \n",
    "   - **Add** additional spaCy multi-word spans only if they appear in the gold vocabulary.  \n",
    "   - **Filter out** generic, meaningless, or uninformative unigrams.  \n",
    "   - **Normalize and deduplicate** final terms.\n",
    "\n",
    "5. **Generate final ensemble predictions**  \n",
    "   - For each sentence, produce an improved term list combining all signals.  \n",
    "   - Output saved in ATE-IT JSON format.\n",
    "\n",
    "### Goal\n",
    "The notebook improves recall and precision of automatic term extraction by combining:\n",
    "- contextual predictions (BERT),\n",
    "- linguistic structure (spaCy),\n",
    "- and domain consistency (gold vocabulary).\n",
    "\n",
    "This hybrid ensemble typically outperforms each component alone.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "35713571",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, os\n",
    "def load_json(path: str):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "    \n",
    "\n",
    "def save_json(obj, path: str):\n",
    "    os.makedirs(os.path.dirname(path) or \".\", exist_ok=True)\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(obj, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"✓ Saved cleaned predictions to {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "fc351393",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"../data/\"\n",
    "PRED_DIR = \"../src/predictions/\"\n",
    "\n",
    "TRAIN_FILE = os.path.join(DATA_DIR, \"subtask_a_train.json\")\n",
    "DEV_FILE = os.path.join(DATA_DIR, \"subtask_a_dev.json\")\n",
    "\n",
    "# BERT_DEV_PRED_FILE = os.path.join(\n",
    "#     PRED_DIR, \"subtask_a_dev_bert_token_classification_preds_clean.json\"\n",
    "# )\n",
    "BERT_DEV_PRED_FILE = os.path.join(\n",
    "    PRED_DIR, \"subtask_a_dev_bert_preds_2e-5_changed_cleaned.json\"\n",
    ")\n",
    "SPACY_DEV_PRED_FILE = os.path.join(\n",
    "    PRED_DIR, \"subtask_a_dev_spacy_trained_preds.json\"\n",
    ")\n",
    "\n",
    "ENSEMBLE_OUT_FILE = os.path.join(\n",
    "    PRED_DIR, \"subtask_a_dev_ensemble_bert_2e-5_changed_spacy_dictfilter_metrics_changed.json\"\n",
    ")\n",
    "\n",
    "os.makedirs(PRED_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "b91e87aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../src/predictions/subtask_a_dev_bert_preds_2e-5_changed_cleaned.json\n",
      "../src/predictions/subtask_a_dev_spacy_trained_preds.json\n"
     ]
    }
   ],
   "source": [
    "print(BERT_DEV_PRED_FILE)\n",
    "print(SPACY_DEV_PRED_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "fb6b47ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "\n",
    "def norm(t: str) -> str:\n",
    "    if not t:\n",
    "        return \"\"\n",
    "    t = t.lower()\n",
    "    t = unicodedata.normalize(\"NFKC\", t)\n",
    "    t = t.replace(\"’\", \"'\").replace(\"`\", \"'\")\n",
    "    t = t.replace(\"“\", '\"').replace(\"”\", '\"')\n",
    "    t = \" \".join(t.split())\n",
    "    # strip punteggiatura ai bordi\n",
    "    t = t.strip(\".,;:-'\\\"()[]{}\")\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "3beb4aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Train vocabulary from gold terms\n",
    "# ==============================\n",
    "\n",
    "def build_train_vocab(train_data: dict) -> set:\n",
    "    vocab = set()\n",
    "    for entry in train_data[\"data\"]:\n",
    "        for term in entry.get(\"term_list\", []):\n",
    "            n = norm(term)\n",
    "            if n:\n",
    "                vocab.add(n)\n",
    "    return vocab\n",
    "\n",
    "\n",
    "def build_term_map(pred_json: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Build a mapping:\n",
    "        (document_id, paragraph_id, sentence_id) -> list of predicted terms\n",
    "    from a prediction JSON in the ATE-IT format.\n",
    "    \"\"\"\n",
    "    m = {}\n",
    "    for e in pred_json[\"data\"]:\n",
    "        key = (e[\"document_id\"], e[\"paragraph_id\"], e[\"sentence_id\"])\n",
    "        m[key] = e.get(\"term_list\", []) or []\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "4014db25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def build_train_vocab_with_freq(train_data):\n",
    "    freq = Counter()\n",
    "    for e in train_data[\"data\"]:\n",
    "        for term in e.get(\"term_list\", []):\n",
    "            norm = norm(term)\n",
    "            if norm:\n",
    "                freq[norm] += 1\n",
    "    strong = {t for t, c in freq.items() if c >= 3}\n",
    "    weak   = {t for t, c in freq.items() if c == 1}\n",
    "    return freq, strong, weak\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75ba29d",
   "metadata": {},
   "source": [
    "For each sentence:\n",
    "\n",
    "keeps all BERT terms as baseline,\n",
    "\n",
    "adds spaCy terms only if their normalized form appears in the train vocabulary (and they’re multi-word and not duplicates)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "2683b37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm(t: str) -> str:\n",
    "    if not t:\n",
    "        return \"\"\n",
    "    t = t.lower().strip()\n",
    "    t = \" \".join(t.split())\n",
    "    t = t.replace(\"’\", \"'\")\n",
    "    t = t.strip(\".,;:-'\\\"()[]{}\")\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "1090afc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "GENERIC_HEADS = {\n",
    "    \"rifiuti\", \"materiali\", \"utenti\", \"plastica\", \"carta\",\n",
    "    \"residui\", \"tariffe\", \"gestore\", \"servizio\", \"modalità\",\n",
    "    \"conferimento\", \"costi\", \"parte\", \"quota\", \"impianto\"\n",
    "}\n",
    "def looks_like_acronym(n: str) -> bool:\n",
    "    # es: \"tmb\", \"raee\", \"r.a.e.e.\"\n",
    "    n_clean = n.replace(\".\", \"\")\n",
    "    return (len(n_clean) >= 2 and len(n_clean) <= 6 and n_clean.isalpha())\n",
    "\n",
    "def filter_generic_unigrams(terms, train_vocab_norm):\n",
    "    filtered = []\n",
    "    for t in terms:\n",
    "        n = norm(t)\n",
    "        tokens = n.split()\n",
    "        if len(tokens) == 1:\n",
    "            if n in GENERIC_HEADS and n not in train_vocab_norm and not looks_like_acronym(n):\n",
    "                continue\n",
    "            if n in GENERIC_HEADS and n not in train_vocab_norm:\n",
    "                # scarta \"quota\", \"parte\", ecc. se non compaiono mai come termini gold\n",
    "                continue\n",
    "        filtered.append(t)\n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "64f161d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "GENERIC_BAD = {\n",
    "    \"parte\", \"gestione\", \"città\", \"territorio\", \"comune\",\n",
    "    \"ore\", \"no\", \"si\", \"anno\", \"mese\", \"giorno\"\n",
    "} \n",
    "def contains_as_subspan(longer: str, shorter: str) -> bool:\n",
    "    long_tokens = longer.split()\n",
    "    short_tokens = shorter.split()\n",
    "    L, S = len(long_tokens), len(short_tokens)\n",
    "    if S > L:\n",
    "        return False\n",
    "    for i in range(L - S + 1):\n",
    "        if long_tokens[i:i+S] == short_tokens:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def upgrade_with_longer_spacy(bert_terms, spacy_terms, train_vocab_norm):\n",
    "    \"\"\"\n",
    "    Upgrade BERT terms to longer spaCy spans ONLY WHEN BENEFICIAL.\n",
    "    \"\"\"\n",
    "    final = []\n",
    "    seen = set()\n",
    "    \n",
    "    spacy_norm_map = {norm(t): t for t in spacy_terms or []}\n",
    "\n",
    "    for b in bert_terms or []:\n",
    "        b_norm = norm(b)\n",
    "        if not b_norm or b_norm in GENERIC_BAD:\n",
    "            continue\n",
    "\n",
    "        best = None\n",
    "\n",
    "        # search longest valid spaCy span containing the BERT term\n",
    "        for s_norm, s in spacy_norm_map.items():\n",
    "            if len(s_norm.split()) < 2:\n",
    "                continue\n",
    "            if s_norm not in train_vocab_norm:\n",
    "                continue\n",
    "            if contains_as_subspan(s_norm, b_norm):\n",
    "                if best is None or len(s_norm.split()) > len(norm(best).split()):\n",
    "                    best = s\n",
    "\n",
    "\n",
    "        chosen = best if best else b\n",
    "        c_norm = norm(chosen)\n",
    "\n",
    "        if c_norm not in seen and c_norm not in GENERIC_BAD:\n",
    "            final.append(chosen)\n",
    "            seen.add(c_norm)\n",
    "\n",
    "    return final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "32bb4936",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_bert_spacy_with_dict(bert_terms, spacy_terms, train_vocab_norm):\n",
    "    \"\"\"\n",
    "    BEST ensemble so far:\n",
    "    1. upgrade BERT with spaCy\n",
    "    2. add dictionary-filtered spaCy spans\n",
    "    3. skip generic or meaningless words\n",
    "    \"\"\"\n",
    "    upgraded = upgrade_with_longer_spacy(\n",
    "        bert_terms=bert_terms,\n",
    "        spacy_terms=spacy_terms,\n",
    "        train_vocab_norm=train_vocab_norm,\n",
    "    )\n",
    "\n",
    "    final = upgraded[:]\n",
    "    seen = {norm(t) for t in upgraded}\n",
    "\n",
    "    for s in spacy_terms or []:\n",
    "        s_norm = norm(s)\n",
    "\n",
    "        if len(s.split()) < 2:\n",
    "            continue\n",
    "        if s_norm not in train_vocab_norm:\n",
    "            continue\n",
    "        if s_norm in seen:\n",
    "            continue\n",
    "        if s_norm in GENERIC_BAD:\n",
    "            continue\n",
    "\n",
    "        final.append(s)\n",
    "        seen.add(s_norm)\n",
    "\n",
    "    return final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "35d36815",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_sentence(bert_terms, spacy_terms, train_vocab_norm):\n",
    "    merged = merge_bert_spacy_with_dict(\n",
    "        bert_terms=bert_terms,\n",
    "        spacy_terms=spacy_terms,\n",
    "        train_vocab_norm=train_vocab_norm\n",
    "    )\n",
    "    merged = filter_generic_unigrams(merged, train_vocab_norm)\n",
    "\n",
    "    # dedupe and normalize\n",
    "    seen = set()\n",
    "    final = []\n",
    "    for t in merged:\n",
    "        n = norm(t)\n",
    "        if n not in seen:\n",
    "            final.append(n)\n",
    "            seen.add(n)\n",
    "    return final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "47d23d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "                   \n",
    "def micro_f1_score(gold_standard, system_output):\n",
    "  \"\"\"\n",
    "  Evaluates a term extraction system's performance using Precision, Recall,\n",
    "  and F1 score based on individual term matching (micro-average).\n",
    "\n",
    "  Args:\n",
    "    gold_standard: A list of lists, where each inner list contains the\n",
    "        gold standard terms for an item.\n",
    "    system_output: A list of lists, where each inner list contains the\n",
    "                   terms extracted by the system for the corresponding item.\n",
    "\n",
    "  Returns:\n",
    "    A tuple containing the Precision, Recall, and F1 score.\n",
    "  \"\"\"\n",
    "  total_true_positives = 0\n",
    "  total_false_positives = 0\n",
    "  total_false_negatives = 0\n",
    "\n",
    "  # Iterate through each item's gold standard and system output terms\n",
    "  for gold, system in zip(gold_standard, system_output):\n",
    "    # Convert to sets for efficient comparison\n",
    "    gold_set = set(gold)\n",
    "    system_set = set(system)\n",
    "\n",
    "    # Calculate True Positives, False Positives, and False Negatives for the current item\n",
    "    true_positives = len(gold_set.intersection(system_set))\n",
    "    false_positives = len(system_set - gold_set)\n",
    "    false_negatives = len(gold_set - system_set)\n",
    "\n",
    "    # Accumulate totals across all items\n",
    "    total_true_positives += true_positives\n",
    "    total_false_positives += false_positives\n",
    "    total_false_negatives += false_negatives\n",
    "\n",
    "  # Calculate Precision, Recall, and F1 score (micro-average)\n",
    "  precision = total_true_positives / (total_true_positives + total_false_positives) if (total_true_positives + total_false_positives) > 0 else 0\n",
    "  recall = total_true_positives / (total_true_positives + total_false_negatives) if (total_true_positives + total_false_negatives) > 0 else 0\n",
    "  f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "  return precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "1536f7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def type_f1_score(gold_standard, system_output):\n",
    "  \"\"\"\n",
    "  Evaluates a term extraction system's performance using Type Precision,\n",
    "  Type Recall, and Type F1 score based on the set of unique terms extracted\n",
    "  at least once across the entire dataset.\n",
    "\n",
    "  Args:\n",
    "    gold_standard: A list of lists, where each inner list contains the\n",
    "                   gold standard terms for an item.\n",
    "    system_output: A list of lists, where each inner list contains the\n",
    "                   terms extracted by the system for the corresponding item.\n",
    "\n",
    "  Returns:\n",
    "    A tuple containing the Type Precision, Type Recall, and Type F1 score.\n",
    "  \"\"\"\n",
    "\n",
    "  # Get the set of all unique gold standard terms across the dataset\n",
    "  all_gold_terms = set()\n",
    "  for item_terms in gold_standard:\n",
    "    all_gold_terms.update(item_terms)\n",
    "\n",
    "  # Get the set of all unique system extracted terms across the dataset\n",
    "  all_system_terms = set()\n",
    "  for item_terms in system_output:\n",
    "    all_system_terms.update(item_terms)\n",
    "\n",
    "  # Calculate True Positives (terms present in both sets)\n",
    "  type_true_positives = len(all_gold_terms.intersection(all_system_terms))\n",
    "\n",
    "  # Calculate False Positives (terms in system output but not in gold standard)\n",
    "  type_false_positives = len(all_system_terms - all_gold_terms)\n",
    "\n",
    "  # Calculate False Negatives (terms in gold standard but not in system output)\n",
    "  type_false_negatives = len(all_gold_terms - all_system_terms)\n",
    "\n",
    "  # Calculate Type Precision, Type Recall, and Type F1 score\n",
    "  type_precision = type_true_positives / (type_true_positives + type_false_positives) if (type_true_positives + type_false_positives) > 0 else 0\n",
    "  type_recall = type_true_positives / (type_true_positives + type_false_negatives) if (type_true_positives + type_false_negatives) > 0 else 0\n",
    "  type_f1 = 2 * (type_precision * type_recall) / (type_precision + type_recall) if (type_precision + type_recall) > 0 else 0\n",
    "\n",
    "  return type_precision, type_recall, type_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9d3d67",
   "metadata": {},
   "source": [
    "###   BUILD BERT + SPACY ENSEMBLE USING merge_sentence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48532509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# unique normalized terms from train gold: 710\n",
      "Building improved BERT+spaCy ensemble ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 577/577 [00:00<00:00, 108219.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------------------------------\n",
      "Sentence 0\n",
      "TEXT: Non Domestica; CAMPEGGI, DISTRIBUTORI CARBURANTI, PARCHEGGI; 1,22; 4,73 \n",
      "  BERT  : []\n",
      "  SPACY : []\n",
      "  MERGED: []\n",
      "\n",
      "---------------------------------------\n",
      "Sentence 1\n",
      "TEXT: Il presente disciplinare per la gestione dei centri di raccolta comunali è stato redatto ai sensi e per effetto del DM 13/05/2009, pubblicato sulla G.U. n. 165 del 18/07/2009, con il quale sono state apportate le modifiche sostanziali al DM 08/04/2008, Disciplina dei centri di raccolta dei rifiuti urbani raccolti in modo differenziato, come previsto dall'art. 183, comma 7, lettera cc) del Dlgs 3 aprile 2006, n. 152, e ss.mm.ii.\n",
      "  BERT  : ['disciplinare per la gestione dei centri di raccolta comunali', 'centri di raccolta dei rifiuti urbani raccolti in']\n",
      "  SPACY : ['gestione dei centri di raccolta comunali', 'centri di raccolta dei rifiuti urbani raccolti']\n",
      "  MERGED: ['disciplinare per la gestione dei centri di raccolta comunali', 'centri di raccolta dei rifiuti urbani raccolti in']\n",
      "\n",
      "---------------------------------------\n",
      "Sentence 2\n",
      "TEXT: Voti astenuti: 2 (Acampora Alessandro, Gargiulo Mario)\n",
      "  BERT  : []\n",
      "  SPACY : []\n",
      "  MERGED: []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "# ---- Load train data and build vocabulary ----\n",
    "with open(TRAIN_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "    train_data = json.load(f)\n",
    "\n",
    "train_vocab_norm = build_train_vocab(train_data)\n",
    "print(f\"# unique normalized terms from train gold: {len(train_vocab_norm)}\")\n",
    "\n",
    "# ---- Load dev gold (for evaluation) ----\n",
    "with open(DEV_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "    dev_data = json.load(f)\n",
    "\n",
    "# ---- Load BERT and spaCy predictions ----\n",
    "with open(BERT_DEV_PRED_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "    bert_pred = json.load(f)\n",
    "\n",
    "with open(SPACY_DEV_PRED_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "    spacy_pred = json.load(f)\n",
    "\n",
    "# Convert JSON predictions → dict[(doc,par,sent)] → [terms...]\n",
    "bert_map = build_term_map(bert_pred)\n",
    "spacy_map = build_term_map(spacy_pred)\n",
    "\n",
    "# ---- Build ensemble predictions using merge_sentence ----\n",
    "ensemble_output = {\"data\": []}\n",
    "\n",
    "print(\"Building improved BERT+spaCy ensemble ...\")\n",
    "\n",
    "for idx, row in enumerate(tqdm(dev_data[\"data\"])):\n",
    "\n",
    "    key = (row[\"document_id\"], row[\"paragraph_id\"], row[\"sentence_id\"])\n",
    "\n",
    "    bert_terms = bert_map.get(key, []) or []\n",
    "    spacy_terms = spacy_map.get(key, []) or []\n",
    "\n",
    "    #  NEW MERGE FUNCTION \n",
    "    merged_terms = merge_sentence(\n",
    "        bert_terms=bert_terms,\n",
    "        spacy_terms=spacy_terms,\n",
    "        train_vocab_norm=train_vocab_norm\n",
    "    )\n",
    "\n",
    "    # Debug on first 3\n",
    "    if idx < 3:\n",
    "        print(\"\\n---------------------------------------\")\n",
    "        print(\"Sentence\", idx)\n",
    "        print(\"TEXT:\", row[\"sentence_text\"])\n",
    "        print(\"  BERT  :\", bert_terms)\n",
    "        print(\"  SPACY :\", spacy_terms)\n",
    "        print(\"  MERGED:\", merged_terms)\n",
    "\n",
    "    # Save\n",
    "    ensemble_output[\"data\"].append({\n",
    "        \"document_id\": row[\"document_id\"],\n",
    "        \"paragraph_id\": row[\"paragraph_id\"],\n",
    "        \"sentence_id\": row[\"sentence_id\"],\n",
    "        \"term_list\": merged_terms,\n",
    "    })\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5f2d4e",
   "metadata": {},
   "source": [
    "#### Save predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "921fb996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ensemble predictions saved to: ../src/predictions/subtask_a_dev_ensemble_bert_2e-5_changed_spacy_dictfilter_metrics_changed.json\n"
     ]
    }
   ],
   "source": [
    "# ---- Save final merged predictions ----\n",
    "with open(ENSEMBLE_OUT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(ensemble_output, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"\\nEnsemble predictions saved to: {ENSEMBLE_OUT_FILE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "7162f311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=====================================================\n",
      "    IMPROVED BERT + SPACY + DICTIONARY MERGE\n",
      "=====================================================\n",
      "\n",
      "Micro-averaged Metrics:\n",
      "  Precision: 0.7687\n",
      "  Recall:    0.7295\n",
      "  F1 Score:  0.7486\n",
      "\n",
      "Type-level Metrics:\n",
      "  Type Precision: 0.7277\n",
      "  Type Recall:    0.6736\n",
      "  Type F1 Score:  0.6996\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Extract gold + predicted lists\n",
    "dev_gold = [entry[\"term_list\"] for entry in dev_data[\"data\"]]\n",
    "ensemble_preds = [entry[\"term_list\"] for entry in ensemble_output[\"data\"]]\n",
    "\n",
    "precision, recall, f1 = micro_f1_score(dev_gold, ensemble_preds)\n",
    "type_precision, type_recall, type_f1 = type_f1_score(dev_gold, ensemble_preds)\n",
    "\n",
    "print(\"\\n=====================================================\")\n",
    "print(\"    IMPROVED BERT + SPACY + DICTIONARY MERGE\")\n",
    "print(\"=====================================================\")\n",
    "\n",
    "print(\"\\nMicro-averaged Metrics:\")\n",
    "print(f\"  Precision: {precision:.4f}\")\n",
    "print(f\"  Recall:    {recall:.4f}\")\n",
    "print(f\"  F1 Score:  {f1:.4f}\")\n",
    "\n",
    "print(\"\\nType-level Metrics:\")\n",
    "print(f\"  Type Precision: {type_precision:.4f}\")\n",
    "print(f\"  Type Recall:    {type_recall:.4f}\")\n",
    "print(f\"  Type F1 Score:  {type_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "b0d24159",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def get_fp_fn_from_listformat(gold_entries, pred_entries):\n",
    "    \"\"\"\n",
    "    gold_entries: list of rows from dev_data[\"data\"]\n",
    "    pred_entries: list of rows from ensemble_output[\"data\"]\n",
    "    \n",
    "    Each entry has:\n",
    "        - document_id\n",
    "        - paragraph_id\n",
    "        - sentence_id\n",
    "        - term_list (list of terms)\n",
    "    \n",
    "    Returns DataFrames:\n",
    "        fp_df (false positives)\n",
    "        fn_df (false negatives)\n",
    "    \"\"\"\n",
    "\n",
    "    gold_rows = []\n",
    "    pred_rows = []\n",
    "\n",
    "    # --- Expand GOLD ---\n",
    "    for e in gold_entries:\n",
    "        doc = e[\"document_id\"]\n",
    "        par = e[\"paragraph_id\"]\n",
    "        sid = e[\"sentence_id\"]\n",
    "        for t in e[\"term_list\"]:\n",
    "            t_norm = norm(t)\n",
    "            if t_norm:\n",
    "                gold_rows.append((doc, par, sid, t_norm))\n",
    "\n",
    "    # --- Expand PRED ---\n",
    "    for e in pred_entries:\n",
    "        doc = e[\"document_id\"]\n",
    "        par = e[\"paragraph_id\"]\n",
    "        sid = e[\"sentence_id\"]\n",
    "        for t in e[\"term_list\"]:\n",
    "            t_norm = norm(t)\n",
    "            if t_norm:\n",
    "                pred_rows.append((doc, par, sid, t_norm))\n",
    "\n",
    "    gold_set = set(gold_rows)\n",
    "    pred_set = set(pred_rows)\n",
    "\n",
    "    fp = pred_set - gold_set\n",
    "    fn = gold_set - pred_set\n",
    "\n",
    "    fp_df = pd.DataFrame(list(fp),\n",
    "                         columns=[\"document_id\", \"paragraph_id\", \"sentence_id\", \"term\"])\n",
    "    fn_df = pd.DataFrame(list(fn),\n",
    "                         columns=[\"document_id\", \"paragraph_id\", \"sentence_id\", \"term\"])\n",
    "\n",
    "    return fp_df, fn_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "7c99fdde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False Positives: 99\n",
      "False Negatives: 122\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document_id</th>\n",
       "      <th>paragraph_id</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>term</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>doc_sorrento_20</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>plastica mono</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>doc_santegidiodelmontealbino_03</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>sacco azzurro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>doc_sorrento_20</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>raccolta differenziata</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>doc_salerno_06</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>svuotamento</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>doc_caserta_06</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>frazioni di rifiuti</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>doc_prataprincipatodiultra_02</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>porta a porta spinto</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>doc_nocerainferiore_06</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>sistema di raccolta differenziata dei rifiuti ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>doc_sorrento_10</td>\n",
       "      <td>28</td>\n",
       "      <td>2</td>\n",
       "      <td>parte fissa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>doc_caserta_02</td>\n",
       "      <td>68</td>\n",
       "      <td>8</td>\n",
       "      <td>pannocarta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>doc_poggiomarino_12</td>\n",
       "      <td>23</td>\n",
       "      <td>2</td>\n",
       "      <td>centro di raccolta differenziata</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>doc_nola_02</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>totale annuale</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>doc_battipaglia_13</td>\n",
       "      <td>15</td>\n",
       "      <td>5</td>\n",
       "      <td>contenitori per la raccolta dei rifiuti urbani...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>doc_capaccio_15</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>sacchetti</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>doc_salerno_05</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>plastica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>doc_capaccio_28</td>\n",
       "      <td>14</td>\n",
       "      <td>5</td>\n",
       "      <td>concorrente</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>doc_praiano_02</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>metallo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>doc_salerno_05</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>cartoni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>doc_marigliano_01</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>abbandono di rifiuti non pericolosi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>doc_praiano_07</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>superficie ka</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>doc_praiano_02</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>lattine</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        document_id  paragraph_id  sentence_id  \\\n",
       "0                   doc_sorrento_20             1            3   \n",
       "1   doc_santegidiodelmontealbino_03            15            2   \n",
       "2                   doc_sorrento_20             1            3   \n",
       "3                    doc_salerno_06            27            1   \n",
       "4                    doc_caserta_06            10            4   \n",
       "5     doc_prataprincipatodiultra_02             2            0   \n",
       "6            doc_nocerainferiore_06             2            1   \n",
       "7                   doc_sorrento_10            28            2   \n",
       "8                    doc_caserta_02            68            8   \n",
       "9               doc_poggiomarino_12            23            2   \n",
       "10                      doc_nola_02             8            9   \n",
       "11               doc_battipaglia_13            15            5   \n",
       "12                  doc_capaccio_15            10            4   \n",
       "13                   doc_salerno_05             7            6   \n",
       "14                  doc_capaccio_28            14            5   \n",
       "15                   doc_praiano_02             8            1   \n",
       "16                   doc_salerno_05             7            6   \n",
       "17                doc_marigliano_01             9            1   \n",
       "18                   doc_praiano_07            21            1   \n",
       "19                   doc_praiano_02             8            1   \n",
       "\n",
       "                                                 term  \n",
       "0                                       plastica mono  \n",
       "1                                       sacco azzurro  \n",
       "2                              raccolta differenziata  \n",
       "3                                         svuotamento  \n",
       "4                                 frazioni di rifiuti  \n",
       "5                                porta a porta spinto  \n",
       "6   sistema di raccolta differenziata dei rifiuti ...  \n",
       "7                                         parte fissa  \n",
       "8                                          pannocarta  \n",
       "9                    centro di raccolta differenziata  \n",
       "10                                     totale annuale  \n",
       "11  contenitori per la raccolta dei rifiuti urbani...  \n",
       "12                                          sacchetti  \n",
       "13                                           plastica  \n",
       "14                                        concorrente  \n",
       "15                                            metallo  \n",
       "16                                            cartoni  \n",
       "17                abbandono di rifiuti non pericolosi  \n",
       "18                                      superficie ka  \n",
       "19                                            lattine  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document_id</th>\n",
       "      <th>paragraph_id</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>term</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>doc_sorrento_10</td>\n",
       "      <td>28</td>\n",
       "      <td>2</td>\n",
       "      <td>coefficienti per la determinazione della parte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>doc_capaccio_10</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>sacchetto trasparente</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>doc_salerno_03</td>\n",
       "      <td>2</td>\n",
       "      <td>27</td>\n",
       "      <td>sacchetti</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>doc_nocerainferiore_06</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>depositare i rifiuti</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>doc_sorrento_10</td>\n",
       "      <td>56</td>\n",
       "      <td>0</td>\n",
       "      <td>costi variabili</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>doc_capaccio_21</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>imballaggi in cartone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>doc_sorrento_22</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>materiali ferrosi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>doc_sorrento_15</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>materiali in plastica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>doc_poggiomarino_12</td>\n",
       "      <td>17</td>\n",
       "      <td>65</td>\n",
       "      <td>r1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>doc_nocerainferiore_06</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>carta, cartone, cartoncino</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>doc_poggiomarino_12</td>\n",
       "      <td>17</td>\n",
       "      <td>65</td>\n",
       "      <td>r4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>doc_capaccio_10</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>modalità di conferimento</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>doc_caserta_02</td>\n",
       "      <td>64</td>\n",
       "      <td>9</td>\n",
       "      <td>r.a.e.e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>doc_nola_02</td>\n",
       "      <td>19</td>\n",
       "      <td>2</td>\n",
       "      <td>rifiuti</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>doc_caserta_06</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>cernita</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>doc_battipaglia_02</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>emissione</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>doc_nocerainferiore_06</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>operatori dell'isola ecologica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>doc_capaccio_10</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>sacchetto trasparente</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>doc_prataprincipatodiultra_02</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>raccolta differenziata porta a porta spinto</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>doc_sorrento_10</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>tari</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      document_id  paragraph_id  sentence_id  \\\n",
       "0                 doc_sorrento_10            28            2   \n",
       "1                 doc_capaccio_10             6            6   \n",
       "2                  doc_salerno_03             2           27   \n",
       "3          doc_nocerainferiore_06            10            1   \n",
       "4                 doc_sorrento_10            56            0   \n",
       "5                 doc_capaccio_21            21            1   \n",
       "6                 doc_sorrento_22             2            0   \n",
       "7                 doc_sorrento_15             2            0   \n",
       "8             doc_poggiomarino_12            17           65   \n",
       "9          doc_nocerainferiore_06             4            0   \n",
       "10            doc_poggiomarino_12            17           65   \n",
       "11                doc_capaccio_10             8            1   \n",
       "12                 doc_caserta_02            64            9   \n",
       "13                    doc_nola_02            19            2   \n",
       "14                 doc_caserta_06            10            4   \n",
       "15             doc_battipaglia_02            20            0   \n",
       "16         doc_nocerainferiore_06             5            1   \n",
       "17                doc_capaccio_10             8            3   \n",
       "18  doc_prataprincipatodiultra_02             2            0   \n",
       "19                doc_sorrento_10            30            0   \n",
       "\n",
       "                                                 term  \n",
       "0   coefficienti per la determinazione della parte...  \n",
       "1                               sacchetto trasparente  \n",
       "2                                           sacchetti  \n",
       "3                                depositare i rifiuti  \n",
       "4                                     costi variabili  \n",
       "5                               imballaggi in cartone  \n",
       "6                                   materiali ferrosi  \n",
       "7                               materiali in plastica  \n",
       "8                                                  r1  \n",
       "9                          carta, cartone, cartoncino  \n",
       "10                                                 r4  \n",
       "11                           modalità di conferimento  \n",
       "12                                            r.a.e.e  \n",
       "13                                            rifiuti  \n",
       "14                                            cernita  \n",
       "15                                          emissione  \n",
       "16                     operatori dell'isola ecologica  \n",
       "17                              sacchetto trasparente  \n",
       "18        raccolta differenziata porta a porta spinto  \n",
       "19                                               tari  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gold_entries = dev_data[\"data\"]      # gold JSON\n",
    "pred_entries = ensemble_output[\"data\"]  # merged predictions JSON\n",
    "\n",
    "fp_df, fn_df = get_fp_fn_from_listformat(gold_entries, pred_entries)\n",
    "\n",
    "print(\"False Positives:\", len(fp_df))\n",
    "print(\"False Negatives:\", len(fn_df))\n",
    "\n",
    "display(fp_df.head(20))\n",
    "display(fn_df.head(20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2223e8",
   "metadata": {},
   "source": [
    "#### SENTENCE-LEVEL ERROR ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "f9d8b088",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from typing import List\n",
    "def collect_sentence_errors(pred_json, gold_json):\n",
    "\n",
    "    \"\"\"\n",
    "    Build structures:\n",
    "       errors[(doc,par,sent)] = {\n",
    "           \"gold\": [...],\n",
    "           \"pred\": [...],\n",
    "           \"fp\": [...],\n",
    "           \"fn\": [...],\n",
    "       }\n",
    "    and a list sorted by the number of errors.\n",
    "    \"\"\"\n",
    "    pred_map = {}\n",
    "    for e in pred_json[\"data\"]:\n",
    "        key = (e[\"document_id\"], e[\"paragraph_id\"], e[\"sentence_id\"])\n",
    "        pred_map[key] = set(norm(t) for t in e.get(\"term_list\", []))\n",
    "\n",
    "    gold_map = {}\n",
    "    for e in gold_json[\"data\"]:\n",
    "        key = (e[\"document_id\"], e[\"paragraph_id\"], e[\"sentence_id\"])\n",
    "        gold_map[key] = set(norm(t) for t in e.get(\"term_list\", []))\n",
    "\n",
    "    errors = {}\n",
    "    for key in gold_map:\n",
    "        gold = gold_map[key]\n",
    "        pred = pred_map.get(key, set())\n",
    "\n",
    "        fp = sorted(pred - gold)\n",
    "        fn = sorted(gold - pred)\n",
    "\n",
    "        errors[key] = {\n",
    "            \"gold\": gold,\n",
    "            \"pred\": pred,\n",
    "            \"fp\": fp,\n",
    "            \"fn\": fn,\n",
    "            \"err_count\": len(fp) + len(fn),\n",
    "        }\n",
    "\n",
    "    # sort keys by number of errors descending\n",
    "    sorted_keys = sorted(errors.keys(), key=lambda k: errors[k][\"err_count\"], reverse=True)\n",
    "    return errors, sorted_keys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "bca9fb28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top_error_sentences(errors, sorted_keys, dev_data, top_n=10):\n",
    "    \"\"\"\n",
    "    print the N worst sentences with text, gold, pred, FP, FN.\n",
    "    \"\"\"\n",
    "    # Build sentence map\n",
    "    sent_map = {}\n",
    "    for r in dev_data[\"data\"]:\n",
    "        key = (r[\"document_id\"], r[\"paragraph_id\"], r[\"sentence_id\"])\n",
    "        sent_map[key] = r[\"sentence_text\"]\n",
    "\n",
    "    print(f\"\\n=== Top {top_n} error sentences ===\")\n",
    "    for i, key in enumerate(sorted_keys[:top_n]):\n",
    "        doc, par, sent = key\n",
    "        e = errors[key]\n",
    "        txt = sent_map[key]\n",
    "\n",
    "        print(\"\\n------------------------------------------------------------\")\n",
    "        print(f\"[{i+1}] Doc: {doc}  Par: {par}  Sent: {sent}\")\n",
    "        print(\"TEXT :\", txt)\n",
    "        print(\"GOLD :\", sorted(e['gold']))\n",
    "        print(\"PRED :\", sorted(e['pred']))\n",
    "        print(\"FP   :\", e['fp'])\n",
    "        print(\"FN   :\", e['fn'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "741ec78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from typing import List\n",
    "ITALIAN_BAD_ENDINGS = {\n",
    "    \"di\", \"dei\", \"degli\", \"del\", \"della\", \"dello\", \"delle\",\n",
    "    \"e\", \"ed\",\n",
    "    \"a\", \"ai\", \"agli\", \"al\", \"alla\", \"alle\", \"allo\",\n",
    "    \"da\", \"dal\", \"dai\", \"dagli\", \"dalla\", \"dalle\",\n",
    "    \"con\", \"per\", \"su\", \"tra\", \"fra\"\n",
    "}\n",
    "def looks_truncated(term: str) -> bool:\n",
    "    \"\"\"\n",
    "    Heuristica: termini che finiscono con una stopword funzionale\n",
    "    (es. 'gestione dei', 'batterie e') sono probabilmente tagliati.\n",
    "    \"\"\"\n",
    "    tokens = term.split()\n",
    "    if len(tokens) < 2:\n",
    "        return False  # una sola parola: può essere un termine valido (es. 'multimateriale')\n",
    "    last = tokens[-1]\n",
    "    return last in ITALIAN_BAD_ENDINGS\n",
    "class Span:\n",
    "    def __init__(self, term: str, start: int, end: int):\n",
    "        self.term = term\n",
    "        self.start = start\n",
    "        self.end = end\n",
    "\n",
    "    def length(self) -> int:\n",
    "        return self.end - self.start\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Span(term={self.term!r}, start={self.start}, end={self.end})\"\n",
    "\n",
    "def find_spans(sentence: str, term: str) -> List[Span]:\n",
    "    \"\"\"\n",
    "    Trova tutte le occorrenze (span carattere) di 'term' in 'sentence' (case-insensitive).\n",
    "    \"\"\"\n",
    "    spans = []\n",
    "    sent_l = sentence.lower()\n",
    "    t = term.lower()\n",
    "    start = 0\n",
    "    while True:\n",
    "        idx = sent_l.find(t, start)\n",
    "        if idx == -1:\n",
    "            break\n",
    "        spans.append(Span(term, idx, idx + len(t)))\n",
    "        start = idx + 1\n",
    "    return spans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "ba504ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_removal_path(term, sentence_text, bert_terms_raw, spacy_terms_raw, train_vocab_norm):\n",
    "    \"\"\"\n",
    "    Show whether a term was removed because:\n",
    "      - truncated\n",
    "      - nested\n",
    "      - generic unigram filter\n",
    "      - not in train vocabulary\n",
    "      - overridden by spaCy upgrade\n",
    "      - dedup\n",
    "    \"\"\"\n",
    "    n = norm(term)\n",
    "\n",
    "    print(\"\\n=== DEBUG TERM:\", term, \"→\", n, \"===\")\n",
    "\n",
    "    # Step 1: raw BERT terms\n",
    "    print(\"1) Present in raw BERT?\", term in bert_terms_raw)\n",
    "\n",
    "    # Step 2: truncation check\n",
    "    if looks_truncated(n):\n",
    "        print(\"⚠️  Removed because it looks truncated (ending stopword).\")\n",
    "    \n",
    "    # Step 3: nested removal check\n",
    "    spans = find_spans(sentence_text, n)\n",
    "    print(\"Span(s):\", spans)\n",
    "    if len(spans) > 1:\n",
    "        print(\"Potential nested conflict → check longest-span rule.\")\n",
    "\n",
    "    # Step 4: generic unigram?\n",
    "    if len(n.split()) == 1:\n",
    "        if n in GENERIC_HEADS:\n",
    "            print(\"⚠️  Likely removed: generic unigram not in training vocab.\")\n",
    "        if n in GENERIC_BAD:\n",
    "            print(\"⚠️  Likely removed: generic BAD term (stopword-like).\")\n",
    "\n",
    "    # Step 5: train vocab\n",
    "    in_vocab = n in train_vocab_norm\n",
    "    print(\"In train vocabulary?\", in_vocab)\n",
    "\n",
    "    # Step 6: spaCy upgrade\n",
    "    up = upgrade_with_longer_spacy(bert_terms_raw, spacy_terms_raw, train_vocab_norm)\n",
    "    if n not in [norm(x) for x in up]:\n",
    "        print(\"❗ Not present after spaCy upgrade → may have been replaced by longer span.\")\n",
    "\n",
    "    print(\"Done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "4c2e4016",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Top 15 error sentences ===\n",
      "\n",
      "------------------------------------------------------------\n",
      "[1] Doc: doc_salerno_05  Par: 7  Sent: 6\n",
      "TEXT : I cartoni per liquidi vanno conferiti con plastica, acciaio e alluminio.\n",
      "GOLD : ['conferiti', 'plastica, acciaio e alluminio']\n",
      "PRED : ['acciaio', 'alluminio', 'cartoni', 'plastica', 'vanno conferiti']\n",
      "FP   : ['acciaio', 'alluminio', 'cartoni', 'plastica', 'vanno conferiti']\n",
      "FN   : ['conferiti', 'plastica, acciaio e alluminio']\n",
      "\n",
      "------------------------------------------------------------\n",
      "[2] Doc: doc_poggiomarino_12  Par: 17  Sent: 65\n",
      "TEXT : R1 frigoriferi e sistemi per il condizionamento – R2 lavatrici, lavastoviglie, cucine, scaldabagni – R3 Tv e Monitor – R4 Piccoli elettrodomestici, hardware da Information Tecnology, elettronica di consumo, apparecchi illuminanti – R5 Sorgenti luminose.\n",
      "GOLD : ['frigoriferi e sistemi per il condizionamento', 'lavatrici, lavastoviglie, cucine, scaldabagni', 'r1', 'r2', 'r3', 'r4', 'r5']\n",
      "PRED : []\n",
      "FP   : []\n",
      "FN   : ['frigoriferi e sistemi per il condizionamento', 'lavatrici, lavastoviglie, cucine, scaldabagni', 'r1', 'r2', 'r3', 'r4', 'r5']\n",
      "\n",
      "------------------------------------------------------------\n",
      "[3] Doc: doc_sorrento_22  Par: 2  Sent: 0\n",
      "TEXT : Oltre 400 chilogrammi di rifiuti raccolti, corrispondenti a circa venti buste di multimateriale, una transenna arrugginita, un fusto di ferro con altri materiali ferrosi, un bidone di plastica e tre pneumatici.\n",
      "GOLD : ['bidone', 'buste', 'materiali ferrosi', 'multimateriale', 'plastica', 'pneumatici', 'rifiuti']\n",
      "PRED : ['buste', 'ferro', 'multimateriale', 'plastica', 'rifiuti raccolti']\n",
      "FP   : ['ferro', 'rifiuti raccolti']\n",
      "FN   : ['bidone', 'materiali ferrosi', 'pneumatici', 'rifiuti']\n",
      "\n",
      "------------------------------------------------------------\n",
      "[4] Doc: doc_praiano_02  Par: 8  Sent: 1\n",
      "TEXT : METALLO-BANDA STAGNATA-ALLUMINIO: Lattine in alluminio con il simbolo AL, scatolette e lattine in banda stagnata, barattoli in metallo (pelati, tonno, ecc.), pellicola in alluminio.\n",
      "GOLD : ['lattine in alluminio con il simbolo al', 'metallo-banda stagnata-alluminio']\n",
      "PRED : ['alluminio', 'banda stagnata', 'lattine', 'metallo']\n",
      "FP   : ['alluminio', 'banda stagnata', 'lattine', 'metallo']\n",
      "FN   : ['lattine in alluminio con il simbolo al', 'metallo-banda stagnata-alluminio']\n",
      "\n",
      "------------------------------------------------------------\n",
      "[5] Doc: doc_capaccio_06  Par: 10  Sent: 1\n",
      "TEXT : Gli oli conferiti sono raccolti dalla ditta SARIM SRL che provvede poi al trasporto presso impianti autorizzati che effettuano il recupero degli oli esausti.\n",
      "GOLD : ['conferiti', 'recupero degli oli esausti']\n",
      "PRED : ['oli conferiti', 'oli esausti', 'recupero']\n",
      "FP   : ['oli conferiti', 'oli esausti', 'recupero']\n",
      "FN   : ['conferiti', 'recupero degli oli esausti']\n",
      "\n",
      "------------------------------------------------------------\n",
      "[6] Doc: doc_santegidiodelmontealbino_03  Par: 12  Sent: 1\n",
      "TEXT : Gli imballaggi in plastica vanno conferiti sciacquati e svuotati, insieme ai rifiuti in metalli [alluminio, banda stagnata, metallo e latta] nel sacco azzurro in dotazione.\n",
      "GOLD : ['alluminio', 'banda stagnata', 'imballaggi in plastica', 'latta', 'rifiuti in metalli', 'sacco', 'vanno conferiti']\n",
      "PRED : ['alluminio', 'banda stagnata', 'imballaggi in plastica', 'latta', 'metallo', 'rifiuti', 'sacco azzurro', 'vanno conferiti']\n",
      "FP   : ['metallo', 'rifiuti', 'sacco azzurro']\n",
      "FN   : ['rifiuti in metalli', 'sacco']\n",
      "\n",
      "------------------------------------------------------------\n",
      "[7] Doc: doc_poggiomarino_12  Par: 23  Sent: 2\n",
      "TEXT : Sono inoltre preposti alla verifica delle disposizioni del presente Disciplinare gli Agenti di Polizia Municipale, competenti anche alla vigilanza igienico-sanitaria, il personale di vigilanza ed ispettivo dell'Azienda ASL e dell'ARPAC, nonché il personale tecnico della Amministrazione Comunale, a tal fine incaricato dal Sindaco, e dai gestori del Centro di raccolta differenziata (solo all'interno dello stesso).\n",
      "GOLD : ['gestori del centro di raccolta differenziata', 'vigilanza igienico-sanitaria']\n",
      "PRED : ['centro di raccolta differenziata', 'gestori']\n",
      "FP   : ['centro di raccolta differenziata', 'gestori']\n",
      "FN   : ['gestori del centro di raccolta differenziata', 'vigilanza igienico-sanitaria']\n",
      "\n",
      "------------------------------------------------------------\n",
      "[8] Doc: doc_sorrento_10  Par: 28  Sent: 2\n",
      "TEXT : b} Scelta dei coefficienti per la determinazione della parte fissa e parte variabile per le utenze domestiche e non domestiche sulla base delle tabelle del D.P.R. 158/99 nel rispetto dell'attuale normativa;\n",
      "GOLD : ['coefficienti per la determinazione della parte fissa e parte variabile', 'utenze domestiche', 'utenze non domestiche']\n",
      "PRED : ['parte fissa', 'parte variabile', 'utenze domestiche']\n",
      "FP   : ['parte fissa', 'parte variabile']\n",
      "FN   : ['coefficienti per la determinazione della parte fissa e parte variabile', 'utenze non domestiche']\n",
      "\n",
      "------------------------------------------------------------\n",
      "[9] Doc: doc_santegidiodelmontealbino_03  Par: 50  Sent: 11\n",
      "TEXT : - metalli, acciaio, alluminio e banda stagnata;\n",
      "GOLD : ['metalli, acciaio, alluminio e banda stagnata']\n",
      "PRED : ['acciaio', 'alluminio', 'banda stagnata']\n",
      "FP   : ['acciaio', 'alluminio', 'banda stagnata']\n",
      "FN   : ['metalli, acciaio, alluminio e banda stagnata']\n",
      "\n",
      "------------------------------------------------------------\n",
      "[10] Doc: doc_salerno_03  Par: 2  Sent: 27\n",
      "TEXT : Risultati di rilievo se si considera che a settembre il 66% dei materiali finiti nei sacchetti del non differenziabile non avrebbe proprio dovuto esserci e che si buttavano letteralmente nella spazzatura centinaia di migliaia di euro.\n",
      "GOLD : ['non differenziabile', 'sacchetti', 'spazzatura']\n",
      "PRED : ['materiali finiti', 'non differenziabile']\n",
      "FP   : ['materiali finiti']\n",
      "FN   : ['sacchetti', 'spazzatura']\n",
      "\n",
      "------------------------------------------------------------\n",
      "[11] Doc: doc_marigliano_01  Par: 9  Sent: 1\n",
      "TEXT : a) € 51,65 per l'abbandono di rifiuti non pericolosi e non ingombranti;\n",
      "GOLD : ['abbandono di rifiuti non pericolosi e non ingombranti', 'rifiuti non ingombranti']\n",
      "PRED : ['abbandono di rifiuti non pericolosi']\n",
      "FP   : ['abbandono di rifiuti non pericolosi']\n",
      "FN   : ['abbandono di rifiuti non pericolosi e non ingombranti', 'rifiuti non ingombranti']\n",
      "\n",
      "------------------------------------------------------------\n",
      "[12] Doc: doc_nocerainferiore_06  Par: 4  Sent: 0\n",
      "TEXT : ORGANICO Dalle ore 20.00 alle 24.00 a piè di fabbricato; SECCO INDIFFERENZIATO Dalle ore 20.00 alle 24.00 a piè di fabbricato; PLASTICA E METALLI Dalle ore 20.00 alle 24.00 a piè di fabbricato; CARTA, CARTONE, CARTONCINO Dalle ore 20.00 alle 24.00 a piè di fabbricato; VETRO A qualsiasi ora nella campana\n",
      "GOLD : ['campana', 'carta, cartone, cartoncino', 'organico', 'plastica e metalli', 'secco indifferenziato', 'vetro']\n",
      "PRED : ['carta, cartone', 'organico', 'plastica e metalli', 'secco indifferenziato', 'vetro']\n",
      "FP   : ['carta, cartone']\n",
      "FN   : ['campana', 'carta, cartone, cartoncino']\n",
      "\n",
      "------------------------------------------------------------\n",
      "[13] Doc: doc_nocerainferiore_06  Par: 5  Sent: 1\n",
      "TEXT : Gli operatori dell'isola ecologica osservano i seguenti orari e giorni di apertura (determina dir. prot. 703 del 15/05/2015) Lunedì - Martedì - Mercoledì - Giovedì - Venerdì dalle 8:30 alle 13:00 Martedi e Giovedìi dalle 15:30 alle 17:30.\n",
      "GOLD : [\"operatori dell'isola ecologica\"]\n",
      "PRED : ['isola ecologica', 'operatori']\n",
      "FP   : ['isola ecologica', 'operatori']\n",
      "FN   : [\"operatori dell'isola ecologica\"]\n",
      "\n",
      "------------------------------------------------------------\n",
      "[14] Doc: doc_salerno_06  Par: 27  Sent: 1\n",
      "TEXT : Presso le utenze domestiche è previsto lo svuotamento dei carrellati condominiali a piè di portone tre volte a settimana, e precisamente nei giorni di lunedì, mercoledì e sabato.\n",
      "GOLD : ['svuotamento dei carrellati condominiali', 'utenze domestiche']\n",
      "PRED : ['carrellati condomini', 'svuotamento', 'utenze domestiche']\n",
      "FP   : ['carrellati condomini', 'svuotamento']\n",
      "FN   : ['svuotamento dei carrellati condominiali']\n",
      "\n",
      "------------------------------------------------------------\n",
      "[15] Doc: doc_caserta_06  Par: 10  Sent: 4\n",
      "TEXT : - effettuare qualsiasi forma di prelievo e/o cernita di materiale conferito e/o comunque introdursi nei sito nei contenitori adibiti alla raccolta delle frazioni di rifiuti;\n",
      "GOLD : ['cernita', 'contenitori adibiti alla raccolta delle frazioni di rifiuti', 'materiale conferito']\n",
      "PRED : ['contenitori adibiti alla raccolta delle frazioni di rifiuti', 'frazioni di rifiuti', 'materiale conferito', 'sito']\n",
      "FP   : ['frazioni di rifiuti', 'sito']\n",
      "FN   : ['cernita']\n",
      "\n",
      "=== DEBUG TERM: gestione dei rifiuti urbani → gestione dei rifiuti urbani ===\n",
      "1) Present in raw BERT? False\n",
      "Span(s): []\n",
      "In train vocabulary? True\n",
      "❗ Not present after spaCy upgrade → may have been replaced by longer span.\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "errors, sorted_keys = collect_sentence_errors(ensemble_output, dev_data)\n",
    "\n",
    "# Print 10 worst sentences\n",
    "print_top_error_sentences(errors, sorted_keys, dev_data, top_n=15)\n",
    "\n",
    "# Debug a specific term in a bad sentence\n",
    "bad_key = sorted_keys[0]  # worst sentence\n",
    "doc, par, sent = bad_key\n",
    "row = next(r for r in dev_data[\"data\"] if (r[\"document_id\"],r[\"paragraph_id\"],r[\"sentence_id\"]) == bad_key)\n",
    "\n",
    "bert_raw = bert_map.get(bad_key, [])\n",
    "spacy_raw = spacy_map.get(bad_key, [])\n",
    "sentence_text = row[\"sentence_text\"]\n",
    "\n",
    "explain_removal_path(\"gestione dei rifiuti urbani\", sentence_text, bert_raw, spacy_raw, train_vocab_norm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
