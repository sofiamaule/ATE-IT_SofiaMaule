{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35713571",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, os\n",
    "def load_json(path: str):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "    \n",
    "\n",
    "def save_json(obj, path: str):\n",
    "    os.makedirs(os.path.dirname(path) or \".\", exist_ok=True)\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(obj, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"✓ Saved cleaned predictions to {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc351393",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"../data\"\n",
    "PRED_DIR = \"../src/predictions\"\n",
    "\n",
    "TRAIN_FILE = os.path.join(DATA_DIR, \"subtask_a_train.json\")\n",
    "DEV_FILE = os.path.join(DATA_DIR, \"subtask_a_dev.json\")\n",
    "\n",
    "# BERT_DEV_PRED_FILE = os.path.join(\n",
    "#     PRED_DIR, \"subtask_a_dev_bert_token_classification_preds_clean.json\"\n",
    "# )\n",
    "BERT_DEV_PRED_FILE = os.path.join(\n",
    "    PRED_DIR, \"subtask_a_dev_bert_token_classification_preds_extended_clean.json\"\n",
    ")\n",
    "SPACY_DEV_PRED_FILE = os.path.join(\n",
    "    PRED_DIR, \"subtask_a_dev_spacy_trained_preds.json\"\n",
    ")\n",
    "\n",
    "ENSEMBLE_OUT_FILE = os.path.join(\n",
    "    PRED_DIR, \"subtask_a_dev_ensemble_bert_spacy_dictfilter.json\"\n",
    ")\n",
    "\n",
    "os.makedirs(PRED_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb6b47ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "\n",
    "\n",
    "def normalize_term(t: str) -> str:\n",
    "    t = t.lower().strip()\n",
    "    t = \" \".join(t.split())\n",
    "    return t\n",
    "\n",
    "def normalize_text(s: str) -> str:\n",
    "    \"\"\"\n",
    "    Normalize a term or sentence:\n",
    "      - lowercase\n",
    "      - Unicode normalization (NFKC)\n",
    "      - normalize quotes/apostrophes\n",
    "      - collapse multiple spaces\n",
    "      - strip leading/trailing spaces\n",
    "    \"\"\"\n",
    "    s = s.lower()\n",
    "    s = unicodedata.normalize(\"NFKC\", s)\n",
    "    s = s.replace(\"’\", \"'\").replace(\"`\", \"'\").replace(\"“\", '\"').replace(\"”\", '\"')\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    return s.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3beb4aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Train vocabulary from gold terms\n",
    "# ==============================\n",
    "\n",
    "def build_train_vocab(train_data: dict) -> set:\n",
    "    \"\"\"\n",
    "    Build a normalized vocabulary of gold terms from the training set.\n",
    "    Each term is normalized with `normalize_text`.\n",
    "    \"\"\"\n",
    "    vocab = set()\n",
    "    for entry in train_data[\"data\"]:\n",
    "        for term in entry.get(\"term_list\", []):\n",
    "            norm_term = normalize_text(term)\n",
    "            if norm_term:\n",
    "                vocab.add(norm_term)\n",
    "    return vocab\n",
    "\n",
    "def build_term_map(pred_json: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Build a mapping:\n",
    "        (document_id, paragraph_id, sentence_id) -> list of predicted terms\n",
    "    from a prediction JSON in the ATE-IT format.\n",
    "    \"\"\"\n",
    "    m = {}\n",
    "    for e in pred_json[\"data\"]:\n",
    "        key = (e[\"document_id\"], e[\"paragraph_id\"], e[\"sentence_id\"])\n",
    "        m[key] = e.get(\"term_list\", []) or []\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4014db25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def build_train_vocab_with_freq(train_data):\n",
    "    freq = Counter()\n",
    "    for e in train_data[\"data\"]:\n",
    "        for term in e.get(\"term_list\", []):\n",
    "            norm = normalize_text(term)\n",
    "            if norm:\n",
    "                freq[norm] += 1\n",
    "    strong = {t for t, c in freq.items() if c >= 3}\n",
    "    weak   = {t for t, c in freq.items() if c == 1}\n",
    "    return freq, strong, weak\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75ba29d",
   "metadata": {},
   "source": [
    "For each sentence:\n",
    "\n",
    "keeps all BERT terms as baseline,\n",
    "\n",
    "adds spaCy terms only if their normalized form appears in the train vocabulary (and they’re multi-word and not duplicates)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2683b37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm(t: str) -> str:\n",
    "    if not t:\n",
    "        return \"\"\n",
    "    t = t.lower().strip()\n",
    "    t = \" \".join(t.split())\n",
    "    t = t.replace(\"’\", \"'\")\n",
    "    t = t.strip(\".,;:-'\\\"()[]{}\")\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1090afc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "GENERIC_HEADS = {\n",
    "    \"rifiuti\", \"materiali\", \"utenti\", \"plastica\", \"carta\",\n",
    "    \"residui\", \"tariffe\", \"gestore\", \"servizio\", \"modalità\",\n",
    "    \"conferimento\", \"costi\", \"parte\", \"quota\", \"impianto\"\n",
    "}\n",
    "\n",
    "def filter_generic_unigrams(terms, train_vocab_norm):\n",
    "    filtered = []\n",
    "    for t in terms:\n",
    "        tokens = t.split()\n",
    "        if len(tokens) == 1:\n",
    "            # tienilo solo se:\n",
    "            # 1) è nel vocabolario di train (compare come termine vero)\n",
    "            #    oppure\n",
    "            # 2) è una sigla tipo \"tmb\", \"r.a.e.e.\"\n",
    "            if normalize_text(t) not in train_vocab_norm and normalize_text(t) in GENERIC_HEADS:\n",
    "                continue\n",
    "        filtered.append(t)\n",
    "    return filtered\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64f161d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "GENERIC_BAD = {\n",
    "    \"parte\", \"gestione\", \"città\", \"territorio\", \"comune\",\n",
    "    \"ore\", \"no\", \"si\", \"anno\", \"mese\", \"giorno\"\n",
    "} \n",
    "\n",
    "\n",
    "def upgrade_with_longer_spacy(bert_terms, spacy_terms, train_vocab_norm):\n",
    "    \"\"\"\n",
    "    Upgrade BERT terms to longer spaCy spans ONLY WHEN BENEFICIAL.\n",
    "    \"\"\"\n",
    "    final = []\n",
    "    seen = set()\n",
    "    \n",
    "    spacy_norm_map = {norm(t): t for t in spacy_terms or []}\n",
    "\n",
    "    for b in bert_terms or []:\n",
    "        b_norm = norm(b)\n",
    "        if not b_norm or b_norm in GENERIC_BAD:\n",
    "            continue\n",
    "\n",
    "        best = None\n",
    "\n",
    "        # search longest valid spaCy span containing the BERT term\n",
    "        for s_norm, s in spacy_norm_map.items():\n",
    "            if len(s.split()) < 2:\n",
    "                continue\n",
    "            if b_norm in s_norm and s_norm in train_vocab_norm:\n",
    "                if best is None or len(s_norm.split()) > len(norm(best).split()):\n",
    "                    best = s\n",
    "\n",
    "        chosen = best if best else b\n",
    "        c_norm = norm(chosen)\n",
    "\n",
    "        if c_norm not in seen and c_norm not in GENERIC_BAD:\n",
    "            final.append(chosen)\n",
    "            seen.add(c_norm)\n",
    "\n",
    "    return final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32bb4936",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_bert_spacy_with_dict(bert_terms, spacy_terms, train_vocab_norm):\n",
    "    \"\"\"\n",
    "    BEST ensemble so far:\n",
    "    1. upgrade BERT with spaCy\n",
    "    2. add dictionary-filtered spaCy spans\n",
    "    3. skip generic or meaningless words\n",
    "    \"\"\"\n",
    "    upgraded = upgrade_with_longer_spacy(\n",
    "        bert_terms=bert_terms,\n",
    "        spacy_terms=spacy_terms,\n",
    "        train_vocab_norm=train_vocab_norm,\n",
    "    )\n",
    "\n",
    "    final = upgraded[:]\n",
    "    seen = {norm(t) for t in upgraded}\n",
    "\n",
    "    for s in spacy_terms or []:\n",
    "        s_norm = norm(s)\n",
    "\n",
    "        if len(s.split()) < 2:\n",
    "            continue\n",
    "        if s_norm not in train_vocab_norm:\n",
    "            continue\n",
    "        if s_norm in seen:\n",
    "            continue\n",
    "        if s_norm in GENERIC_BAD:\n",
    "            continue\n",
    "\n",
    "        final.append(s)\n",
    "        seen.add(s_norm)\n",
    "\n",
    "    return final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "35d36815",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_sentence(bert_terms, spacy_terms, train_vocab_norm):\n",
    "    merged = merge_bert_spacy_with_dict(\n",
    "        bert_terms=bert_terms,\n",
    "        spacy_terms=spacy_terms,\n",
    "        train_vocab_norm=train_vocab_norm\n",
    "    )\n",
    "    # dedupe and normalize\n",
    "    seen = set()\n",
    "    final = []\n",
    "    for t in merged:\n",
    "        n = norm(t)\n",
    "        if n not in seen:\n",
    "            final.append(n)\n",
    "            seen.add(n)\n",
    "    return final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "47d23d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "                   \n",
    "def micro_f1_score(gold_standard, system_output):\n",
    "  \"\"\"\n",
    "  Evaluates a term extraction system's performance using Precision, Recall,\n",
    "  and F1 score based on individual term matching (micro-average).\n",
    "\n",
    "  Args:\n",
    "    gold_standard: A list of lists, where each inner list contains the\n",
    "        gold standard terms for an item.\n",
    "    system_output: A list of lists, where each inner list contains the\n",
    "                   terms extracted by the system for the corresponding item.\n",
    "\n",
    "  Returns:\n",
    "    A tuple containing the Precision, Recall, and F1 score.\n",
    "  \"\"\"\n",
    "  total_true_positives = 0\n",
    "  total_false_positives = 0\n",
    "  total_false_negatives = 0\n",
    "\n",
    "  # Iterate through each item's gold standard and system output terms\n",
    "  for gold, system in zip(gold_standard, system_output):\n",
    "    # Convert to sets for efficient comparison\n",
    "    gold_set = set(gold)\n",
    "    system_set = set(system)\n",
    "\n",
    "    # Calculate True Positives, False Positives, and False Negatives for the current item\n",
    "    true_positives = len(gold_set.intersection(system_set))\n",
    "    false_positives = len(system_set - gold_set)\n",
    "    false_negatives = len(gold_set - system_set)\n",
    "\n",
    "    # Accumulate totals across all items\n",
    "    total_true_positives += true_positives\n",
    "    total_false_positives += false_positives\n",
    "    total_false_negatives += false_negatives\n",
    "\n",
    "  # Calculate Precision, Recall, and F1 score (micro-average)\n",
    "  precision = total_true_positives / (total_true_positives + total_false_positives) if (total_true_positives + total_false_positives) > 0 else 0\n",
    "  recall = total_true_positives / (total_true_positives + total_false_negatives) if (total_true_positives + total_false_negatives) > 0 else 0\n",
    "  f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "  return precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1536f7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def type_f1_score(gold_standard, system_output):\n",
    "  \"\"\"\n",
    "  Evaluates a term extraction system's performance using Type Precision,\n",
    "  Type Recall, and Type F1 score based on the set of unique terms extracted\n",
    "  at least once across the entire dataset.\n",
    "\n",
    "  Args:\n",
    "    gold_standard: A list of lists, where each inner list contains the\n",
    "                   gold standard terms for an item.\n",
    "    system_output: A list of lists, where each inner list contains the\n",
    "                   terms extracted by the system for the corresponding item.\n",
    "\n",
    "  Returns:\n",
    "    A tuple containing the Type Precision, Type Recall, and Type F1 score.\n",
    "  \"\"\"\n",
    "\n",
    "  # Get the set of all unique gold standard terms across the dataset\n",
    "  all_gold_terms = set()\n",
    "  for item_terms in gold_standard:\n",
    "    all_gold_terms.update(item_terms)\n",
    "\n",
    "  # Get the set of all unique system extracted terms across the dataset\n",
    "  all_system_terms = set()\n",
    "  for item_terms in system_output:\n",
    "    all_system_terms.update(item_terms)\n",
    "\n",
    "  # Calculate True Positives (terms present in both sets)\n",
    "  type_true_positives = len(all_gold_terms.intersection(all_system_terms))\n",
    "\n",
    "  # Calculate False Positives (terms in system output but not in gold standard)\n",
    "  type_false_positives = len(all_system_terms - all_gold_terms)\n",
    "\n",
    "  # Calculate False Negatives (terms in gold standard but not in system output)\n",
    "  type_false_negatives = len(all_gold_terms - all_system_terms)\n",
    "\n",
    "  # Calculate Type Precision, Type Recall, and Type F1 score\n",
    "  type_precision = type_true_positives / (type_true_positives + type_false_positives) if (type_true_positives + type_false_positives) > 0 else 0\n",
    "  type_recall = type_true_positives / (type_true_positives + type_false_negatives) if (type_true_positives + type_false_negatives) > 0 else 0\n",
    "  type_f1 = 2 * (type_precision * type_recall) / (type_precision + type_recall) if (type_precision + type_recall) > 0 else 0\n",
    "\n",
    "  return type_precision, type_recall, type_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9d3d67",
   "metadata": {},
   "source": [
    "###   BUILD BERT + SPACY ENSEMBLE USING merge_sentence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "48532509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# unique normalized terms from train gold: 713\n",
      "Building improved BERT+spaCy ensemble ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 577/577 [00:00<00:00, 110401.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------------------------------\n",
      "Sentence 0\n",
      "TEXT: Non Domestica; CAMPEGGI, DISTRIBUTORI CARBURANTI, PARCHEGGI; 1,22; 4,73 \n",
      "  BERT  : []\n",
      "  SPACY : []\n",
      "  MERGED: []\n",
      "\n",
      "---------------------------------------\n",
      "Sentence 1\n",
      "TEXT: Il presente disciplinare per la gestione dei centri di raccolta comunali è stato redatto ai sensi e per effetto del DM 13/05/2009, pubblicato sulla G.U. n. 165 del 18/07/2009, con il quale sono state apportate le modifiche sostanziali al DM 08/04/2008, Disciplina dei centri di raccolta dei rifiuti urbani raccolti in modo differenziato, come previsto dall'art. 183, comma 7, lettera cc) del Dlgs 3 aprile 2006, n. 152, e ss.mm.ii.\n",
      "  BERT  : ['disciplinare', 'gestione', 'centri di raccolta comunali', 'disciplina', 'centri di raccolta dei rifiuti urbani raccolti']\n",
      "  SPACY : ['gestione dei centri di raccolta comunali', 'centri di raccolta dei rifiuti urbani raccolti']\n",
      "  MERGED: ['disciplinare', 'centri di raccolta comunali', 'disciplina', 'centri di raccolta dei rifiuti urbani raccolti']\n",
      "\n",
      "---------------------------------------\n",
      "Sentence 2\n",
      "TEXT: Voti astenuti: 2 (Acampora Alessandro, Gargiulo Mario)\n",
      "  BERT  : []\n",
      "  SPACY : []\n",
      "  MERGED: []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "# ---- Load train data and build vocabulary ----\n",
    "with open(TRAIN_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "    train_data = json.load(f)\n",
    "\n",
    "train_vocab_norm = build_train_vocab(train_data)\n",
    "print(f\"# unique normalized terms from train gold: {len(train_vocab_norm)}\")\n",
    "\n",
    "# ---- Load dev gold (for evaluation) ----\n",
    "with open(DEV_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "    dev_data = json.load(f)\n",
    "\n",
    "# ---- Load BERT and spaCy predictions ----\n",
    "with open(BERT_DEV_PRED_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "    bert_pred = json.load(f)\n",
    "\n",
    "with open(SPACY_DEV_PRED_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "    spacy_pred = json.load(f)\n",
    "\n",
    "# Convert JSON predictions → dict[(doc,par,sent)] → [terms...]\n",
    "bert_map = build_term_map(bert_pred)\n",
    "spacy_map = build_term_map(spacy_pred)\n",
    "\n",
    "# ---- Build ensemble predictions using merge_sentence ----\n",
    "ensemble_output = {\"data\": []}\n",
    "\n",
    "print(\"Building improved BERT+spaCy ensemble ...\")\n",
    "\n",
    "for idx, row in enumerate(tqdm(dev_data[\"data\"])):\n",
    "\n",
    "    key = (row[\"document_id\"], row[\"paragraph_id\"], row[\"sentence_id\"])\n",
    "\n",
    "    bert_terms = bert_map.get(key, []) or []\n",
    "    spacy_terms = spacy_map.get(key, []) or []\n",
    "\n",
    "    #  NEW MERGE FUNCTION \n",
    "    merged_terms = merge_sentence(\n",
    "        bert_terms=bert_terms,\n",
    "        spacy_terms=spacy_terms,\n",
    "        train_vocab_norm=train_vocab_norm\n",
    "    )\n",
    "\n",
    "    # Debug on first 3\n",
    "    if idx < 3:\n",
    "        print(\"\\n---------------------------------------\")\n",
    "        print(\"Sentence\", idx)\n",
    "        print(\"TEXT:\", row[\"sentence_text\"])\n",
    "        print(\"  BERT  :\", bert_terms)\n",
    "        print(\"  SPACY :\", spacy_terms)\n",
    "        print(\"  MERGED:\", merged_terms)\n",
    "\n",
    "    # Save\n",
    "    ensemble_output[\"data\"].append({\n",
    "        \"document_id\": row[\"document_id\"],\n",
    "        \"paragraph_id\": row[\"paragraph_id\"],\n",
    "        \"sentence_id\": row[\"sentence_id\"],\n",
    "        \"term_list\": merged_terms,\n",
    "    })\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5f2d4e",
   "metadata": {},
   "source": [
    "#### Save predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "921fb996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ensemble predictions saved to: ../src/predictions\\subtask_a_dev_ensemble_bert_spacy_dictfilter.json\n"
     ]
    }
   ],
   "source": [
    "# ---- Save final merged predictions ----\n",
    "with open(ENSEMBLE_OUT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(ensemble_output, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"\\nEnsemble predictions saved to: {ENSEMBLE_OUT_FILE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b751000c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "def normalize_term(t: str) -> str:\n",
    "    t = t.lower().strip()\n",
    "\n",
    "    # spazi multipli\n",
    "    t = \" \".join(t.split())\n",
    "\n",
    "    # normalizza separatori tipo \"-\", \"/\", \",\" quando servono come lista\n",
    "    t = t.replace(\" - \", \" \").replace(\"-\", \" - \")\n",
    "    t = t.replace(\"/\", \" / \")\n",
    "\n",
    "    # togli punteggiatura ai bordi\n",
    "    t = t.strip(string.punctuation + \"«»“”'\\\"[]()\")\n",
    "\n",
    "    # normalizza sigle r.a.e.e. -> raee\n",
    "    t = re.sub(r\"\\.\", \"\", t)\n",
    "\n",
    "    return t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7162f311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=====================================================\n",
      "    IMPROVED BERT + SPACY + DICTIONARY MERGE\n",
      "=====================================================\n",
      "\n",
      "Micro-averaged Metrics:\n",
      "  Precision: 0.7388\n",
      "  Recall:    0.6962\n",
      "  F1 Score:  0.7169\n",
      "\n",
      "Type-level Metrics:\n",
      "  Type Precision: 0.6881\n",
      "  Type Recall:    0.6198\n",
      "  Type F1 Score:  0.6522\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Extract gold + predicted lists\n",
    "dev_gold = [entry[\"term_list\"] for entry in dev_data[\"data\"]]\n",
    "ensemble_preds = [entry[\"term_list\"] for entry in ensemble_output[\"data\"]]\n",
    "\n",
    "precision, recall, f1 = micro_f1_score(dev_gold, ensemble_preds)\n",
    "type_precision, type_recall, type_f1 = type_f1_score(dev_gold, ensemble_preds)\n",
    "\n",
    "print(\"\\n=====================================================\")\n",
    "print(\"    IMPROVED BERT + SPACY + DICTIONARY MERGE\")\n",
    "print(\"=====================================================\")\n",
    "\n",
    "print(\"\\nMicro-averaged Metrics:\")\n",
    "print(f\"  Precision: {precision:.4f}\")\n",
    "print(f\"  Recall:    {recall:.4f}\")\n",
    "print(f\"  F1 Score:  {f1:.4f}\")\n",
    "\n",
    "print(\"\\nType-level Metrics:\")\n",
    "print(f\"  Type Precision: {type_precision:.4f}\")\n",
    "print(f\"  Type Recall:    {type_recall:.4f}\")\n",
    "print(f\"  Type F1 Score:  {type_f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b0d24159",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def get_fp_fn_from_listformat(gold_entries, pred_entries):\n",
    "    \"\"\"\n",
    "    gold_entries: list of rows from dev_data[\"data\"]\n",
    "    pred_entries: list of rows from ensemble_output[\"data\"]\n",
    "    \n",
    "    Each entry has:\n",
    "        - document_id\n",
    "        - paragraph_id\n",
    "        - sentence_id\n",
    "        - term_list (list of terms)\n",
    "    \n",
    "    Returns DataFrames:\n",
    "        fp_df (false positives)\n",
    "        fn_df (false negatives)\n",
    "    \"\"\"\n",
    "\n",
    "    gold_rows = []\n",
    "    pred_rows = []\n",
    "\n",
    "    # --- Expand GOLD ---\n",
    "    for e in gold_entries:\n",
    "        doc = e[\"document_id\"]\n",
    "        par = e[\"paragraph_id\"]\n",
    "        sid = e[\"sentence_id\"]\n",
    "        for t in e[\"term_list\"]:\n",
    "            t_norm = t.lower().strip()\n",
    "            if t_norm:\n",
    "                gold_rows.append((doc, par, sid, t_norm))\n",
    "\n",
    "    # --- Expand PRED ---\n",
    "    for e in pred_entries:\n",
    "        doc = e[\"document_id\"]\n",
    "        par = e[\"paragraph_id\"]\n",
    "        sid = e[\"sentence_id\"]\n",
    "        for t in e[\"term_list\"]:\n",
    "            t_norm = t.lower().strip()\n",
    "            if t_norm:\n",
    "                pred_rows.append((doc, par, sid, t_norm))\n",
    "\n",
    "    gold_set = set(gold_rows)\n",
    "    pred_set = set(pred_rows)\n",
    "\n",
    "    fp = pred_set - gold_set\n",
    "    fn = gold_set - pred_set\n",
    "\n",
    "    fp_df = pd.DataFrame(list(fp),\n",
    "                         columns=[\"document_id\", \"paragraph_id\", \"sentence_id\", \"term\"])\n",
    "    fn_df = pd.DataFrame(list(fn),\n",
    "                         columns=[\"document_id\", \"paragraph_id\", \"sentence_id\", \"term\"])\n",
    "\n",
    "    return fp_df, fn_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7c99fdde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False Positives: 111\n",
      "False Negatives: 137\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document_id</th>\n",
       "      <th>paragraph_id</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>term</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>doc_sorrento_20</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>raccolta differenziata</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>doc_sorrento_15</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>plastica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>doc_praiano_05</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>forme di gestione dei rifiuti</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>doc_santegidiodelmontealbino_03</td>\n",
       "      <td>50</td>\n",
       "      <td>11</td>\n",
       "      <td>banda stagnata</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>doc_caserta_06</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>obbligo di conferimento separato dei rifiuti</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>doc_salerno_03</td>\n",
       "      <td>2</td>\n",
       "      <td>27</td>\n",
       "      <td>materiali</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>doc_capaccio_06</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>oli conferiti</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>doc_gragnano_03</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>servizio di raccolta rsu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>doc_nocerainferiore_06</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>porta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>doc_sorrento_20</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>plastica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>doc_poggiomarino_12</td>\n",
       "      <td>23</td>\n",
       "      <td>2</td>\n",
       "      <td>centro di raccolta differenziata</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>doc_prataprincipatodiultra_02</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>porta a porta spinto</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>doc_praiano_07</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "      <td>delibera arera</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>doc_santegidiodelmontealbino_03</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>rifiuti</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>doc_praiano_03</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>metalli</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>doc_nocerainferiore_06</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>sistema di raccolta differenziata dei rifiuti</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>doc_santegidiodelmontealbino_03</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>metalli</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>doc_nocerainferiore_06</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>isola ecologica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>doc_battipaglia_13</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>manutenzione verde</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>doc_salerno_05</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>alluminio</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        document_id  paragraph_id  sentence_id  \\\n",
       "0                   doc_sorrento_20             1            3   \n",
       "1                   doc_sorrento_15             2            0   \n",
       "2                    doc_praiano_05            13            2   \n",
       "3   doc_santegidiodelmontealbino_03            50           11   \n",
       "4                    doc_caserta_06            16            1   \n",
       "5                    doc_salerno_03             2           27   \n",
       "6                   doc_capaccio_06            10            1   \n",
       "7                   doc_gragnano_03             5            0   \n",
       "8            doc_nocerainferiore_06             2            1   \n",
       "9                   doc_sorrento_20             1            3   \n",
       "10              doc_poggiomarino_12            23            2   \n",
       "11    doc_prataprincipatodiultra_02             2            0   \n",
       "12                   doc_praiano_07            14            4   \n",
       "13  doc_santegidiodelmontealbino_03            12            1   \n",
       "14                   doc_praiano_03             3            1   \n",
       "15           doc_nocerainferiore_06             2            1   \n",
       "16  doc_santegidiodelmontealbino_03            15            2   \n",
       "17           doc_nocerainferiore_06             5            1   \n",
       "18               doc_battipaglia_13            20            0   \n",
       "19                   doc_salerno_05             7            6   \n",
       "\n",
       "                                             term  \n",
       "0                          raccolta differenziata  \n",
       "1                                        plastica  \n",
       "2                   forme di gestione dei rifiuti  \n",
       "3                                  banda stagnata  \n",
       "4    obbligo di conferimento separato dei rifiuti  \n",
       "5                                       materiali  \n",
       "6                                   oli conferiti  \n",
       "7                        servizio di raccolta rsu  \n",
       "8                                           porta  \n",
       "9                                        plastica  \n",
       "10               centro di raccolta differenziata  \n",
       "11                           porta a porta spinto  \n",
       "12                                 delibera arera  \n",
       "13                                        rifiuti  \n",
       "14                                        metalli  \n",
       "15  sistema di raccolta differenziata dei rifiuti  \n",
       "16                                        metalli  \n",
       "17                                isola ecologica  \n",
       "18                             manutenzione verde  \n",
       "19                                      alluminio  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document_id</th>\n",
       "      <th>paragraph_id</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>term</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>doc_nocerainferiore_06</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>calendario utenze domestiche per la raccolta d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>doc_poggiomarino_12</td>\n",
       "      <td>17</td>\n",
       "      <td>65</td>\n",
       "      <td>r1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>doc_sorrento_22</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>bidone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>doc_salerno_05</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>plastica, acciaio e alluminio</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>doc_capaccio_10</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>busta con legaccio</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>doc_praiano_05</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>raccolta/ritiro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>doc_capaccio_10</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>sacchetto trasparente</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>doc_francavillais_09</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>ruote gommate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>doc_santegidiodelmontealbino_03</td>\n",
       "      <td>50</td>\n",
       "      <td>11</td>\n",
       "      <td>metalli, acciaio, alluminio e banda stagnata</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>doc_sorrento_22</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>plastica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>doc_caserta_06</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>gestione del centro di raccolta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>doc_sarno_15</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>fauna</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>doc_capaccio_10</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>sacchetto trasparente</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>doc_sorrento_20</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>percentuale di raccolta differenziata</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>doc_gragnano_02</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>servizio di svuotamento</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>doc_santegidiodelmontealbino_03</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>rifiuti in metalli</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>doc_santegidiodelmontealbino_03</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>latta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>doc_sorrento_22</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>buste</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>doc_capaccio_28</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>servizio integrato gestione rifiuti – raccolta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>doc_sorrento_22</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>rifiuti</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        document_id  paragraph_id  sentence_id  \\\n",
       "0            doc_nocerainferiore_06            10            0   \n",
       "1               doc_poggiomarino_12            17           65   \n",
       "2                   doc_sorrento_22             2            0   \n",
       "3                    doc_salerno_05             7            6   \n",
       "4                   doc_capaccio_10             9            4   \n",
       "5                    doc_praiano_05            10            0   \n",
       "6                   doc_capaccio_10             3            3   \n",
       "7              doc_francavillais_09            19            1   \n",
       "8   doc_santegidiodelmontealbino_03            50           11   \n",
       "9                   doc_sorrento_22             2            0   \n",
       "10                   doc_caserta_06             6            2   \n",
       "11                     doc_sarno_15             5            5   \n",
       "12                  doc_capaccio_10             9            6   \n",
       "13                  doc_sorrento_20             1            3   \n",
       "14                  doc_gragnano_02             3            0   \n",
       "15  doc_santegidiodelmontealbino_03            12            1   \n",
       "16  doc_santegidiodelmontealbino_03            12            1   \n",
       "17                  doc_sorrento_22             2            0   \n",
       "18                  doc_capaccio_28             3            0   \n",
       "19                  doc_sorrento_22             2            0   \n",
       "\n",
       "                                                 term  \n",
       "0   calendario utenze domestiche per la raccolta d...  \n",
       "1                                                  r1  \n",
       "2                                              bidone  \n",
       "3                       plastica, acciaio e alluminio  \n",
       "4                                  busta con legaccio  \n",
       "5                                     raccolta/ritiro  \n",
       "6                               sacchetto trasparente  \n",
       "7                                       ruote gommate  \n",
       "8        metalli, acciaio, alluminio e banda stagnata  \n",
       "9                                            plastica  \n",
       "10                    gestione del centro di raccolta  \n",
       "11                                              fauna  \n",
       "12                              sacchetto trasparente  \n",
       "13              percentuale di raccolta differenziata  \n",
       "14                            servizio di svuotamento  \n",
       "15                                 rifiuti in metalli  \n",
       "16                                              latta  \n",
       "17                                              buste  \n",
       "18  servizio integrato gestione rifiuti – raccolta...  \n",
       "19                                            rifiuti  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gold_entries = dev_data[\"data\"]      # gold JSON\n",
    "pred_entries = ensemble_output[\"data\"]  # merged predictions JSON\n",
    "\n",
    "fp_df, fn_df = get_fp_fn_from_listformat(gold_entries, pred_entries)\n",
    "\n",
    "print(\"False Positives:\", len(fp_df))\n",
    "print(\"False Negatives:\", len(fn_df))\n",
    "\n",
    "display(fp_df.head(20))\n",
    "display(fn_df.head(20))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
