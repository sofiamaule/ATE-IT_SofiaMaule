{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de5755f1",
   "metadata": {},
   "source": [
    "# POST PROCESSING of Extracted Terms\n",
    "In this notebook we apply a post-processing pipeline to the raw term predictions.\n",
    "\n",
    "Goal of this step:\n",
    "- clean and normalize predicted terms\n",
    "- remove clearly bad or truncated terms\n",
    "- remove nested / redundant terms inside the same sentence\n",
    "\n",
    "The final output is a cleaner list of candidate terms, better aligned with the gold standard.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "739f9ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, os\n",
    "# --- 1. UTILITIES DI BASE ----------------------------------------------------\n",
    "\n",
    "\n",
    "def load_json(path: str):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "\n",
    "def save_json(obj, path: str):\n",
    "    os.makedirs(os.path.dirname(path) or \".\", exist_ok=True)\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(obj, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"✓ Saved cleaned predictions to {path}\")\n",
    "\n",
    "\n",
    "def normalize_term(term: str) -> str:\n",
    "    \"\"\"\n",
    "    Normalizza un termine:\n",
    "    - lowercase\n",
    "    - strip\n",
    "    - normalizza spazi multipli\n",
    "    \"\"\"\n",
    "    t = term.strip().lower()\n",
    "    # normalizza spazi multipli in singolo spazio\n",
    "    t = \" \".join(t.split())\n",
    "    return t\n",
    "\n",
    "\n",
    "# Stopword finali che indicano spesso termini \"tagliati\" (es. 'gestione dei')\n",
    "ITALIAN_BAD_ENDINGS = {\n",
    "    \"di\", \"dei\", \"degli\", \"del\", \"della\", \"dello\", \"delle\",\n",
    "    \"e\", \"ed\",\n",
    "    \"a\", \"ai\", \"agli\", \"al\", \"alla\", \"alle\", \"allo\",\n",
    "    \"da\", \"dal\", \"dai\", \"dagli\", \"dalla\", \"dalle\",\n",
    "    \"con\", \"per\", \"su\", \"tra\", \"fra\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132e0cb5",
   "metadata": {},
   "source": [
    "### 2. Detecting truncated terms\n",
    "\n",
    "Sometimes the extractor returns incomplete terms such as:\n",
    "\n",
    "- *\"gestione dei\"*\n",
    "- *\"batterie e\"*\n",
    "\n",
    "These usually end with a function word (preposition, conjunction, etc.) and are unlikely to be valid domain terms.\n",
    "\n",
    "The function `looks_truncated(term)` implements a simple heuristic:\n",
    "\n",
    "- if the term has only one token → it is **not** considered truncated\n",
    "- otherwise, if the **last** token is in `ITALIAN_BAD_ENDINGS` → the term is marked as **truncated**\n",
    "\n",
    "This signal can be used later to filter out bad candidates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4920bb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def looks_truncated(term: str) -> bool:\n",
    "    \"\"\"\n",
    "    Heuristica: termini che finiscono con una stopword funzionale\n",
    "    (es. 'gestione dei', 'batterie e') sono probabilmente tagliati.\n",
    "    \"\"\"\n",
    "    tokens = term.split()\n",
    "    if len(tokens) < 2:\n",
    "        return False  # una sola parola: può essere un termine valido (es. 'multimateriale')\n",
    "    last = tokens[-1]\n",
    "    return last in ITALIAN_BAD_ENDINGS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4701c6",
   "metadata": {},
   "source": [
    "### Removing nested / redundant terms\n",
    "\n",
    "Extractors often produce nested terms for the same sentence.\n",
    "\n",
    "If a **short term** is fully contained inside a **longer term** (same span), we usually want to keep only the longer, more informative one.\n",
    "\n",
    "The function `remove_nested_terms(sentence_text, terms)` does:\n",
    "\n",
    "1. For each term, it finds all character spans in the sentence using `find_spans`.\n",
    "2. It collects all spans and sorts them by **decreasing length** (longer terms first).\n",
    "3. Greedy selection:  it keeps a span if it is **not fully contained** inside any already selected span\n",
    "4. It then sorts the selected spans by `start` offset, to preserve sentence order.\n",
    "5. Finally, it returns the list of unique terms corresponding to the selected spans.\n",
    "\n",
    "Result: we remove terms that are strictly nested inside other terms, reducing redundancy and making evaluation fairer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "182182ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Span:\n",
    "    def __init__(self, term: str, start: int, end: int):\n",
    "        self.term = term\n",
    "        self.start = start\n",
    "        self.end = end\n",
    "\n",
    "    def length(self) -> int:\n",
    "        return self.end - self.start\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Span(term={self.term!r}, start={self.start}, end={self.end})\"\n",
    "\n",
    "\n",
    "def find_spans(sentence: str, term: str) -> List[Span]:\n",
    "    \"\"\"\n",
    "    Trova tutte le occorrenze (span carattere) di 'term' in 'sentence' (case-insensitive).\n",
    "    \"\"\"\n",
    "    spans = []\n",
    "    sent_l = sentence.lower()\n",
    "    t = term.lower()\n",
    "    start = 0\n",
    "    while True:\n",
    "        idx = sent_l.find(t, start)\n",
    "        if idx == -1:\n",
    "            break\n",
    "        spans.append(Span(term, idx, idx + len(t)))\n",
    "        start = idx + 1\n",
    "    return spans\n",
    "\n",
    "\n",
    "def remove_nested_terms(sentence_text: str, terms: List[str]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Rimuove termini nidificati: se un termine è completamente contenuto in un altro,\n",
    "    manteniamo solo lo span più lungo.\n",
    "\n",
    "    Strategia:\n",
    "    - Troviamo tutti gli span (start, end) dei termini nella frase.\n",
    "    - Ordiniamo per lunghezza decrescente.\n",
    "    - Greedy: teniamo uno span se non è completamente contenuto in uno già tenuto.\n",
    "    \"\"\"\n",
    "    spans: List[Span] = []\n",
    "\n",
    "    for term in terms:\n",
    "        term_spans = find_spans(sentence_text, term)\n",
    "        # se un termine compare più volte, consideriamo comunque tutti gli span\n",
    "        spans.extend(term_spans)\n",
    "\n",
    "    if not spans:\n",
    "        return terms  # niente match → nessun filtraggio\n",
    "\n",
    "    # ordina per lunghezza decrescente (prima i termini più lunghi)\n",
    "    spans.sort(key=lambda s: s.length(), reverse=True)\n",
    "\n",
    "    selected: List[Span] = []\n",
    "    for cand in spans:\n",
    "        contained = False\n",
    "        for s in selected:\n",
    "            if cand.start >= s.start and cand.end <= s.end:\n",
    "                contained = True\n",
    "                break\n",
    "        if not contained:\n",
    "            selected.append(cand)\n",
    "\n",
    "    # ordina gli span selezionati in ordine di apparizione nella frase\n",
    "    selected.sort(key=lambda s: s.start)\n",
    "\n",
    "    # termini finali nell'ordine in cui appaiono nella frase\n",
    "    final_terms = []\n",
    "    seen = set()\n",
    "    for s in selected:\n",
    "        if s.term not in seen:\n",
    "            final_terms.append(s.term)\n",
    "            seen.add(s.term)\n",
    "\n",
    "    return final_terms\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "767a8692",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. PIPELINE DI PULIZIA PER UNA SINGOLA FRASE ---------------------------\n",
    "\n",
    "def clean_terms_for_sentence(sentence_text: str, raw_terms: List[str]) -> Tuple[List[str], Dict[str, int]]:\n",
    "    \"\"\"\n",
    "    Pulisce i termini per una singola frase applicando:\n",
    "      - normalizzazione (lowercase, strip, spazi)\n",
    "      - rimozione duplicati\n",
    "      - rimozione termini \"tagliati\" (es. 'gestione dei')\n",
    "      - rimozione termini nidificati (nested)\n",
    "    Ritorna:\n",
    "      - lista di termini puliti\n",
    "      - stats locali (contatori)\n",
    "    \"\"\"\n",
    "    stats = Counter()\n",
    "\n",
    "    # 1. normalizza e rimuovi vuoti\n",
    "    normalized = []\n",
    "    for t in raw_terms:\n",
    "        nt = normalize_term(t)\n",
    "        if nt:\n",
    "            normalized.append(nt)\n",
    "        else:\n",
    "            stats[\"removed_empty_after_norm\"] += 1\n",
    "\n",
    "    stats[\"after_normalization\"] = len(normalized)\n",
    "\n",
    "    # 2. rimuovi duplicati preservando l'ordine\n",
    "    deduped = []\n",
    "    seen = set()\n",
    "    for t in normalized:\n",
    "        if t not in seen:\n",
    "            deduped.append(t)\n",
    "            seen.add(t)\n",
    "        else:\n",
    "            stats[\"removed_duplicates\"] += 1\n",
    "\n",
    "    # 3. rimuovi termini \"tagliati\" (ending stopword)\n",
    "    not_truncated = []\n",
    "    for t in deduped:\n",
    "        if looks_truncated(t):\n",
    "            stats[\"removed_truncated_heuristic\"] += 1\n",
    "        else:\n",
    "            not_truncated.append(t)\n",
    "\n",
    "    # 4. rimuovi termini nidificati usando il testo originale\n",
    "    final_terms = remove_nested_terms(sentence_text, not_truncated)\n",
    "    stats[\"after_nested_filter\"] = len(final_terms)\n",
    "\n",
    "    return final_terms, stats\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7fb34c5",
   "metadata": {},
   "source": [
    "## Span Reconstruction (must-have) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "cc00d545",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_spans(tokens, labels):\n",
    "    spans = []\n",
    "    current = []\n",
    "    for tok, lab in zip(tokens, labels):\n",
    "        if lab.startswith(\"B-\"):\n",
    "            if current:\n",
    "                spans.append(\" \".join(current))\n",
    "            current = [tok]\n",
    "        elif lab.startswith(\"I-\") and current:\n",
    "            current.append(tok)\n",
    "        else:\n",
    "            if current:\n",
    "                spans.append(\" \".join(current))\n",
    "                current = []\n",
    "    if current:\n",
    "        spans.append(\" \".join(current))\n",
    "    return spans\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef018520",
   "metadata": {},
   "source": [
    "#### Sentence-level debugging\n",
    "\n",
    "To better understand what the post-processing is doing, we define a helper:\n",
    "\n",
    "`debug_cleaning_for_sentence(sentence_text, raw_terms)`\n",
    "\n",
    "This function prints, for a single sentence:\n",
    "\n",
    "1. **RAW TERMS**: original predicted terms.\n",
    "2. **NORMALIZED TERMS**: after lowercasing and space normalization.\n",
    "3. **DEDUPED TERMS**: duplicates removed while preserving order.\n",
    "4. **TRUNCATED**: terms detected as truncated by `looks_truncated()` and removed.\n",
    "5. **NOT TRUNCATED**: remaining terms that are passed to the nested-term filter.\n",
    "6. **FINAL TERMS**: result after `remove_nested_terms()`.\n",
    "\n",
    "This is useful to check if our heuristics are too aggressive (removing good terms) or too weak.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "32b3a3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def debug_cleaning_for_sentence(sentence_text: str, raw_terms: List[str]):\n",
    "    \"\"\"\n",
    "    Stampa passo-passo cosa succede ai termini di una singola frase.\n",
    "    Utile per capire se l'euristica sta rimuovendo cose buone.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(\"SENTENCE:\")\n",
    "    print(sentence_text)\n",
    "    print(\"\\nRAW TERMS:\")\n",
    "    print(raw_terms)\n",
    "\n",
    "    # 1) normalizzazione\n",
    "    normalized = [normalize_term(t) for t in raw_terms if normalize_term(t)]\n",
    "    print(\"\\nNORMALIZED TERMS:\")\n",
    "    print(normalized)\n",
    "\n",
    "    # 2) deduplica\n",
    "    deduped = []\n",
    "    seen = set()\n",
    "    for t in normalized:\n",
    "        if t not in seen:\n",
    "            deduped.append(t)\n",
    "            seen.add(t)\n",
    "    print(\"\\nDEDUPED TERMS:\")\n",
    "    print(deduped)\n",
    "\n",
    "    # 3) troncati\n",
    "    not_truncated = []\n",
    "    truncated = []\n",
    "    for t in deduped:\n",
    "        if looks_truncated(t):\n",
    "            truncated.append(t)\n",
    "        else:\n",
    "            not_truncated.append(t)\n",
    "\n",
    "    print(\"\\nTRUNCATED (rimossi dalla euristica):\")\n",
    "    print(truncated)\n",
    "    print(\"\\nNOT TRUNCATED (che passano allo step nested):\")\n",
    "    print(not_truncated)\n",
    "\n",
    "    # 4) nested\n",
    "    final_terms = remove_nested_terms(sentence_text, not_truncated)\n",
    "    print(\"\\nFINAL TERMS AFTER NESTED FILTER:\")\n",
    "    print(final_terms)\n",
    "\n",
    "    print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a86b6a",
   "metadata": {},
   "source": [
    "### Cleaning BERT predictions on the full dev set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "76e4c60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clean_bert_predictions(\n",
    "    bert_pred_path: str,\n",
    "    dev_data_path: str,\n",
    "    output_path: str,\n",
    "    debug_sample_n: int = 0,\n",
    "):\n",
    "    \"\"\"\n",
    "    bert_pred_path: JSON con struttura:\n",
    "      {\"data\": [... {document_id, paragraph_id, sentence_id, term_list} ...]}\n",
    "\n",
    "    dev_data_path: JSON ufficiale ATE-IT (con sentence_text) per ricostruire le frasi.\n",
    "\n",
    "    output_path: dove salvare il JSON pulito.\n",
    "\n",
    "    debug_sample_n: se > 0, stampa il debug della pulizia per le prime N frasi.\n",
    "    \"\"\"\n",
    "    print(f\"Loading dev data from {dev_data_path}...\")\n",
    "    dev_data = load_json(dev_data_path)\n",
    "\n",
    "    # Costruisci mappa (doc, par, sent) -> sentence_text\n",
    "    sentence_map: Dict[Tuple[str, int, int], str] = {}\n",
    "    rows = dev_data[\"data\"] if isinstance(dev_data, dict) and \"data\" in dev_data else dev_data\n",
    "    for r in rows:\n",
    "        key = (r[\"document_id\"], r[\"paragraph_id\"], r[\"sentence_id\"])\n",
    "        sentence_map[key] = r[\"sentence_text\"]\n",
    "\n",
    "    print(f\"✓ Loaded {len(sentence_map)} sentences from dev\")\n",
    "\n",
    "    print(f\"Loading BERT predictions from {bert_pred_path}...\")\n",
    "    bert_pred = load_json(bert_pred_path)\n",
    "    pred_rows = bert_pred[\"data\"] if isinstance(bert_pred, dict) and \"data\" in bert_pred else bert_pred\n",
    "    print(f\"✓ Loaded {len(pred_rows)} prediction entries\")\n",
    "\n",
    "    global_stats: Counter = Counter()\n",
    "    cleaned_data = {\"data\": []}\n",
    "\n",
    "    for idx, entry in enumerate(pred_rows):\n",
    "        key = (entry[\"document_id\"], entry[\"paragraph_id\"], entry[\"sentence_id\"])\n",
    "        sent_text = sentence_map.get(key, \"\")\n",
    "\n",
    "        raw_terms: List[str] = entry.get(\"term_list\", []) or []\n",
    "\n",
    "        cleaned_terms, stats = clean_terms_for_sentence(sent_text, raw_terms)\n",
    "\n",
    "        # debug opzionale sulle prime N frasi\n",
    "        if debug_sample_n > 0 and idx < debug_sample_n:\n",
    "            debug_cleaning_for_sentence(sent_text, raw_terms)\n",
    "\n",
    "        # aggiorna stats globali\n",
    "        global_stats.update(stats)\n",
    "        global_stats[\"total_sentences\"] += 1\n",
    "        global_stats[\"total_terms_before\"] += len(raw_terms)\n",
    "        global_stats[\"total_terms_after\"] += len(cleaned_terms)\n",
    "\n",
    "        cleaned_data[\"data\"].append({\n",
    "            \"document_id\": entry[\"document_id\"],\n",
    "            \"paragraph_id\": entry[\"paragraph_id\"],\n",
    "            \"sentence_id\": entry[\"sentence_id\"],\n",
    "            \"term_list\": cleaned_terms,\n",
    "        })\n",
    "    \n",
    "        # salva output\n",
    "    save_json(cleaned_data, output_path)\n",
    "\n",
    "    # stampa qualche statistica riassuntiva\n",
    "    print(\"\\n=== Cleaning statistics ===\")\n",
    "    for k, v in sorted(global_stats.items()):\n",
    "        print(f\"{k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7d3a6a68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dev data from ../../data/subtask_a_dev.json...\n",
      "✓ Loaded 577 sentences from dev\n",
      "Loading BERT predictions from ../predictions/subtask_a_dev_bert_token_classification_preds_extended_2e-5_changed.json...\n",
      "✓ Loaded 577 prediction entries\n",
      "================================================================================\n",
      "SENTENCE:\n",
      "Non Domestica; CAMPEGGI, DISTRIBUTORI CARBURANTI, PARCHEGGI; 1,22; 4,73 \n",
      "\n",
      "RAW TERMS:\n",
      "[]\n",
      "\n",
      "NORMALIZED TERMS:\n",
      "[]\n",
      "\n",
      "DEDUPED TERMS:\n",
      "[]\n",
      "\n",
      "TRUNCATED (rimossi dalla euristica):\n",
      "[]\n",
      "\n",
      "NOT TRUNCATED (che passano allo step nested):\n",
      "[]\n",
      "\n",
      "FINAL TERMS AFTER NESTED FILTER:\n",
      "[]\n",
      "================================================================================\n",
      "================================================================================\n",
      "SENTENCE:\n",
      "Il presente disciplinare per la gestione dei centri di raccolta comunali è stato redatto ai sensi e per effetto del DM 13/05/2009, pubblicato sulla G.U. n. 165 del 18/07/2009, con il quale sono state apportate le modifiche sostanziali al DM 08/04/2008, Disciplina dei centri di raccolta dei rifiuti urbani raccolti in modo differenziato, come previsto dall'art. 183, comma 7, lettera cc) del Dlgs 3 aprile 2006, n. 152, e ss.mm.ii.\n",
      "\n",
      "RAW TERMS:\n",
      "['disciplinare per la gestione dei centri di raccolta comunali', 'centri di raccolta dei rifiuti urbani raccolti in']\n",
      "\n",
      "NORMALIZED TERMS:\n",
      "['disciplinare per la gestione dei centri di raccolta comunali', 'centri di raccolta dei rifiuti urbani raccolti in']\n",
      "\n",
      "DEDUPED TERMS:\n",
      "['disciplinare per la gestione dei centri di raccolta comunali', 'centri di raccolta dei rifiuti urbani raccolti in']\n",
      "\n",
      "TRUNCATED (rimossi dalla euristica):\n",
      "[]\n",
      "\n",
      "NOT TRUNCATED (che passano allo step nested):\n",
      "['disciplinare per la gestione dei centri di raccolta comunali', 'centri di raccolta dei rifiuti urbani raccolti in']\n",
      "\n",
      "FINAL TERMS AFTER NESTED FILTER:\n",
      "['disciplinare per la gestione dei centri di raccolta comunali', 'centri di raccolta dei rifiuti urbani raccolti in']\n",
      "================================================================================\n",
      "================================================================================\n",
      "SENTENCE:\n",
      "Voti astenuti: 2 (Acampora Alessandro, Gargiulo Mario)\n",
      "\n",
      "RAW TERMS:\n",
      "[]\n",
      "\n",
      "NORMALIZED TERMS:\n",
      "[]\n",
      "\n",
      "DEDUPED TERMS:\n",
      "[]\n",
      "\n",
      "TRUNCATED (rimossi dalla euristica):\n",
      "[]\n",
      "\n",
      "NOT TRUNCATED (che passano allo step nested):\n",
      "[]\n",
      "\n",
      "FINAL TERMS AFTER NESTED FILTER:\n",
      "[]\n",
      "================================================================================\n",
      "================================================================================\n",
      "SENTENCE:\n",
      "Alba srl\n",
      "\n",
      "RAW TERMS:\n",
      "[]\n",
      "\n",
      "NORMALIZED TERMS:\n",
      "[]\n",
      "\n",
      "DEDUPED TERMS:\n",
      "[]\n",
      "\n",
      "TRUNCATED (rimossi dalla euristica):\n",
      "[]\n",
      "\n",
      "NOT TRUNCATED (che passano allo step nested):\n",
      "[]\n",
      "\n",
      "FINAL TERMS AFTER NESTED FILTER:\n",
      "[]\n",
      "================================================================================\n",
      "================================================================================\n",
      "SENTENCE:\n",
      "È un Servizio Supplementare di raccolta, rivolto a famiglie con bambini al di sotto dei 3 anni o con componenti affetti da malattie di lunga degenza.\n",
      "\n",
      "RAW TERMS:\n",
      "['servizio']\n",
      "\n",
      "NORMALIZED TERMS:\n",
      "['servizio']\n",
      "\n",
      "DEDUPED TERMS:\n",
      "['servizio']\n",
      "\n",
      "TRUNCATED (rimossi dalla euristica):\n",
      "[]\n",
      "\n",
      "NOT TRUNCATED (che passano allo step nested):\n",
      "['servizio']\n",
      "\n",
      "FINAL TERMS AFTER NESTED FILTER:\n",
      "['servizio']\n",
      "================================================================================\n",
      "✓ Saved cleaned predictions to ../predictions/subtask_a_dev_bert_preds_extended_3e-5_cleaned.json\n",
      "\n",
      "=== Cleaning statistics ===\n",
      "after_nested_filter: 422\n",
      "after_normalization: 439\n",
      "removed_duplicates: 2\n",
      "removed_truncated_heuristic: 10\n",
      "total_sentences: 577\n",
      "total_terms_after: 422\n",
      "total_terms_before: 439\n"
     ]
    }
   ],
   "source": [
    "clean_bert_predictions(\n",
    "    bert_pred_path=\"../predictions/subtask_a_dev_bert_token_classification_preds_extended_2e-5_changed.json\",\n",
    "    dev_data_path=\"../../data/subtask_a_dev.json\",\n",
    "    output_path=\"../predictions/subtask_a_dev_bert_preds_extended_3e-5_cleaned.json\",\n",
    "    debug_sample_n=5,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9a5b5c7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' #BERT_PRED_PATH = \"../predictions/subtask_a_dev_bert_token_classification_preds.json\"\\nBERT_PRED_PATH = \"../predictions/subtask_a_dev_bert_token_classification_preds_extended.json\" \\nDEV_DATA_PATH = \"../../data/subtask_a_dev.json\"\\n#OUTPUT_PATH = \"../predictions/subtask_a_dev_bert_token_classification_preds_clean.json\"\\nOUTPUT_PATH = \"../predictions/subtask_a_dev_bert_token_classification_preds_extended_clean.json\"\\n\\nclean_bert_predictions(BERT_PRED_PATH, DEV_DATA_PATH, OUTPUT_PATH) '"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" #BERT_PRED_PATH = \"../predictions/subtask_a_dev_bert_token_classification_preds.json\"\n",
    "BERT_PRED_PATH = \"../predictions/subtask_a_dev_bert_token_classification_preds_extended.json\" \n",
    "DEV_DATA_PATH = \"../../data/subtask_a_dev.json\"\n",
    "#OUTPUT_PATH = \"../predictions/subtask_a_dev_bert_token_classification_preds_clean.json\"\n",
    "OUTPUT_PATH = \"../predictions/subtask_a_dev_bert_token_classification_preds_extended_clean.json\"\n",
    "\n",
    "clean_bert_predictions(BERT_PRED_PATH, DEV_DATA_PATH, OUTPUT_PATH) \"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
