{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f192efa",
   "metadata": {},
   "source": [
    "# SpaCy Term Extraction for Italian Text\n",
    "\n",
    "This notebook demonstrates two approaches to term extraction using SpaCy:\n",
    "1. **Baseline**: Rule-based extraction with EntityRuler\n",
    "2. **Trained**: Custom NER model fine-tuned for term extraction\n",
    "\n",
    "Dataset: EvalITA 2025 ATE-IT (Automatic Term Extraction - Italian Testbed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5800b0f8",
   "metadata": {},
   "source": [
    "## Setup and Imports\n",
    "- For complete information about the Italian SpaCy models available: https://spacy.io/models/it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "82b6c4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m spacy download it_core_news_sm\n",
    "#!python -m spacy download it_core_news_md\n",
    "#!python -m spacy download it_core_news_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cfe3a8ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Italian model loaded successfully\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "import spacy\n",
    "from spacy.tokens import DocBin, Doc\n",
    "from spacy.training import Example\n",
    "from spacy.util import minibatch, compounding\n",
    "from spacy.pipeline import EntityRuler\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load Italian model\n",
    "try:\n",
    "    nlp = spacy.load('it_core_news_md')\n",
    "    print(\"✓ Italian model loaded successfully\")\n",
    "except:\n",
    "    print(\"Model not found. Install with: python -m spacy download it_core_news_md\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff0de76",
   "metadata": {},
   "source": [
    "## Data Loading and Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f7d62edf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Data loading functions work correctly\n"
     ]
    }
   ],
   "source": [
    "def load_jsonl(path: str) -> List[Dict]:\n",
    "    \"\"\"Load a JSON lines file or JSON array file.\"\"\"\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        text = f.read().strip()\n",
    "    if not text:\n",
    "        return []\n",
    "    try:\n",
    "        # Try parsing as single JSON object/array\n",
    "        data = json.loads(text)\n",
    "    except json.JSONDecodeError:\n",
    "        # Fall back to JSONL (one JSON per line)\n",
    "        data = []\n",
    "        for line in text.splitlines():\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                data.append(json.loads(line))\n",
    "    return data\n",
    "\n",
    "\n",
    "def build_sentence_gold_map(records: List[Dict]) -> List[Dict]:\n",
    "    \"\"\"Convert dataset rows into list of sentences with aggregated terms.\n",
    "    \n",
    "    Handles both formats:\n",
    "    - Records with 'term_list' field (list of terms) for input files in json format\n",
    "    - Records with individual 'term' field (one term per row) for input files in csv format\n",
    "    \"\"\"\n",
    "    out = {}\n",
    "    \n",
    "    # Support both dict with 'data' key and plain list\n",
    "    if isinstance(records, dict) and 'data' in records:\n",
    "        rows = records['data']\n",
    "    else:\n",
    "        rows = records\n",
    "    \n",
    "    for r in rows:\n",
    "        key = (r.get('document_id'), r.get('paragraph_id'), r.get('sentence_id'))\n",
    "        if key not in out:\n",
    "            out[key] = {\n",
    "                'document_id': r.get('document_id'),\n",
    "                'paragraph_id': r.get('paragraph_id'),\n",
    "                'sentence_id': r.get('sentence_id'),\n",
    "                'sentence_text': r.get('sentence_text', ''),\n",
    "                'terms': []\n",
    "            }\n",
    "        \n",
    "        # Support both 'term_list' (list) and 'term' (single value)\n",
    "        if isinstance(r.get('term_list'), list):\n",
    "            for t in r.get('term_list'):\n",
    "                if t and t not in out[key]['terms']:\n",
    "                    out[key]['terms'].append(t)\n",
    "        else:\n",
    "            term = r.get('term')\n",
    "            if term and term not in out[key]['terms']:\n",
    "                out[key]['terms'].append(term)\n",
    "    \n",
    "    return list(out.values())\n",
    "\n",
    "\n",
    "# Test: Load a small sample\n",
    "test_data = {\n",
    "    'data': [\n",
    "        {\n",
    "            'document_id': 'doc1',\n",
    "            'paragraph_id': 'p1',\n",
    "            'sentence_id': 's1',\n",
    "            'sentence_text': 'La tassa di successione è un tributo.',\n",
    "            'term_list': ['tassa di successione', 'tributo']\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "test_sentences = build_sentence_gold_map(test_data)\n",
    "assert len(test_sentences) == 1\n",
    "assert test_sentences[0]['terms'] == ['tassa di successione', 'tributo']\n",
    "print(\"✓ Data loading functions work correctly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1e62011d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training sentences: 2308\n",
      "Dev sentences: 577\n",
      "\n",
      "Example sentence:\n",
      "  Text: AFFIDAMENTO DEL “SERVIZIO DI SPAZZAMENTO, RACCOLTA, TRASPORTO E SMALTIMENTO/RECUPERO DEI RIFIUTI URBANI ED ASSIMILATI E SERVIZI COMPLEMENTARI DELLA CITTA' DI AGROPOLI” VALEVOLE PER UN QUINQUENNIO\n",
      "  Terms: ['raccolta', 'recupero', 'servizio di raccolta', 'servizio di spazzamento', 'smaltimento', 'trasporto']\n"
     ]
    }
   ],
   "source": [
    "# Load actual training and dev data\n",
    "train_data = load_jsonl('../data/subtask_a_train.json')\n",
    "dev_data = load_jsonl('../data/subtask_a_dev.json')\n",
    "\n",
    "train_sentences = build_sentence_gold_map(train_data)\n",
    "dev_sentences = build_sentence_gold_map(dev_data)\n",
    "\n",
    "print(f\"Training sentences: {len(train_sentences)}\")\n",
    "print(f\"Dev sentences: {len(dev_sentences)}\")\n",
    "print(f\"\\nExample sentence:\")\n",
    "print(f\"  Text: {train_sentences[6]['sentence_text']}\")\n",
    "print(f\"  Terms: {train_sentences[6]['terms']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11fcded5",
   "metadata": {},
   "source": [
    "## Evaluation Metrics\n",
    "\n",
    "Using the official evaluation metrics from the competition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9b1f2479",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Evaluation functions work correctly\n",
      "  Test metrics: P=0.67, R=0.67, F1=0.67\n",
      "  Type metrics: P=0.67, R=0.67, F1=0.67\n"
     ]
    }
   ],
   "source": [
    "def micro_f1_score(gold_standard, system_output):\n",
    "    \"\"\"\n",
    "    Evaluates performance using Precision, Recall, and F1 score \n",
    "    based on individual term matching (micro-average).\n",
    "    \n",
    "    Args:\n",
    "        gold_standard: List of lists, where each inner list contains gold standard terms\n",
    "        system_output: List of lists, where each inner list contains extracted terms\n",
    "    \n",
    "    Returns:\n",
    "        Tuple containing (precision, recall, f1, tp, fp, fn)\n",
    "    \"\"\"\n",
    "    total_true_positives = 0\n",
    "    total_false_positives = 0\n",
    "    total_false_negatives = 0\n",
    "    \n",
    "    # Iterate through each item's gold standard and system output terms\n",
    "    for gold, system in zip(gold_standard, system_output):\n",
    "        # Convert to sets for efficient comparison\n",
    "        gold_set = set(gold)\n",
    "        system_set = set(system)\n",
    "        \n",
    "        # Calculate TP, FP, FN for the current item\n",
    "        true_positives = len(gold_set.intersection(system_set))\n",
    "        false_positives = len(system_set - gold_set)\n",
    "        false_negatives = len(gold_set - system_set)\n",
    "        \n",
    "        # Accumulate totals across all items\n",
    "        total_true_positives += true_positives\n",
    "        total_false_positives += false_positives\n",
    "        total_false_negatives += false_negatives\n",
    "    \n",
    "    # Calculate Precision, Recall, and F1 score (micro-average)\n",
    "    precision = total_true_positives / (total_true_positives + total_false_positives) if (total_true_positives + total_false_positives) > 0 else 0\n",
    "    recall = total_true_positives / (total_true_positives + total_false_negatives) if (total_true_positives + total_false_negatives) > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    return precision, recall, f1, total_true_positives, total_false_positives, total_false_negatives\n",
    "\n",
    "\n",
    "def type_f1_score(gold_standard, system_output):\n",
    "    \"\"\"\n",
    "    Evaluates performance using Type Precision, Type Recall, and Type F1 score\n",
    "    based on the set of unique terms extracted at least once across the entire dataset.\n",
    "    \n",
    "    Args:\n",
    "        gold_standard: List of lists, where each inner list contains gold standard terms\n",
    "        system_output: List of lists, where each inner list contains extracted terms\n",
    "    \n",
    "    Returns:\n",
    "        Tuple containing (type_precision, type_recall, type_f1)\n",
    "    \"\"\"\n",
    "    # Get the set of all unique gold standard terms across the dataset\n",
    "    all_gold_terms = set()\n",
    "    for item_terms in gold_standard:\n",
    "        all_gold_terms.update(item_terms)\n",
    "    \n",
    "    # Get the set of all unique system extracted terms across the dataset\n",
    "    all_system_terms = set()\n",
    "    for item_terms in system_output:\n",
    "        all_system_terms.update(item_terms)\n",
    "    \n",
    "    # Calculate True Positives (terms present in both sets)\n",
    "    type_true_positives = len(all_gold_terms.intersection(all_system_terms))\n",
    "    \n",
    "    # Calculate False Positives (terms in system output but not in gold standard)\n",
    "    type_false_positives = len(all_system_terms - all_gold_terms)\n",
    "    \n",
    "    # Calculate False Negatives (terms in gold standard but not in system output)\n",
    "    type_false_negatives = len(all_gold_terms - all_system_terms)\n",
    "    \n",
    "    # Calculate Type Precision, Type Recall, and Type F1 score\n",
    "    type_precision = type_true_positives / (type_true_positives + type_false_positives) if (type_true_positives + type_false_positives) > 0 else 0\n",
    "    type_recall = type_true_positives / (type_true_positives + type_false_negatives) if (type_true_positives + type_false_negatives) > 0 else 0\n",
    "    type_f1 = 2 * (type_precision * type_recall) / (type_precision + type_recall) if (type_precision + type_recall) > 0 else 0\n",
    "    \n",
    "    return type_precision, type_recall, type_f1\n",
    "\n",
    "\n",
    "# Test: Simple case\n",
    "gold_test = [['term1', 'term2'], ['term3']]\n",
    "pred_test = [['term1', 'term4'], ['term3']]\n",
    "precision, recall, f1, tp, fp, fn = micro_f1_score(gold_test, pred_test)\n",
    "assert tp == 2  # term1 and term3\n",
    "assert fp == 1  # term4\n",
    "assert fn == 1  # term2\n",
    "print(\"✓ Evaluation functions work correctly\")\n",
    "print(f\"  Test metrics: P={precision:.2f}, R={recall:.2f}, F1={f1:.2f}\")\n",
    "\n",
    "# Test type-level metrics\n",
    "type_p, type_r, type_f1 = type_f1_score(gold_test, pred_test)\n",
    "print(f\"  Type metrics: P={type_p:.2f}, R={type_r:.2f}, F1={type_f1:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c26a65e",
   "metadata": {},
   "source": [
    "## Baseline Model: Rule-Based EntityRuler\n",
    "\n",
    "Simple approach using SpaCy's EntityRuler:\n",
    "- Creates exact match patterns from training terms\n",
    "- Fast and deterministic\n",
    "- No generalization to unseen terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b53d42b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "\n",
    "def norm(t: str) -> str:\n",
    "    if not t:\n",
    "        return \"\"\n",
    "    t = t.lower()\n",
    "    t = unicodedata.normalize(\"NFKC\", t)\n",
    "    t = t.replace(\"’\", \"'\").replace(\"`\", \"'\")\n",
    "    t = t.replace(\"“\", '\"').replace(\"”\", '\"')\n",
    "    t = \" \".join(t.split())\n",
    "    # strip punteggiatura ai bordi\n",
    "    t = t.strip(\".,;:-'\\\"()[]{}\")\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2a23bc1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpacyRuleBaseline:\n",
    "    \"\"\"Rule-based extractor using EntityRuler.\"\"\"\n",
    "\n",
    "    def __init__(self, model: str = \"it_core_news_sm\"):\n",
    "        \"\"\"\n",
    "        Se ti serve solo l'EntityRuler potresti anche usare spacy.blank(\"it\")\n",
    "        per essere più leggera. Per ora mantengo il modello pre-addestrato.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.nlp = spacy.load(model)\n",
    "            print(f\"✓ Loaded spaCy model: {model}\")\n",
    "        except Exception:\n",
    "            print(f\" Could not load {model}, using blank('it') instead\")\n",
    "            self.nlp = spacy.blank(\"it\")\n",
    "\n",
    "        self.ruler = None\n",
    "\n",
    "    def build(self, terms: List[str]):\n",
    "        \"\"\"Build EntityRuler patterns from term list (token-based, LOWER, normalizzati).\"\"\"\n",
    "\n",
    "        # 1) normalizza e deduplica i termini\n",
    "        norm_terms = set()\n",
    "        for t in terms:\n",
    "            if not t:\n",
    "                continue\n",
    "            n = norm(t)  # usa la tua funzione norm(t)\n",
    "            if not n:\n",
    "                continue\n",
    "            norm_terms.add(n)\n",
    "\n",
    "        # 2) crea pattern token-based in LOWER, ordinati per lunghezza desc\n",
    "        patterns = []\n",
    "        # ordino per numero di token e lunghezza, per favorire i multiword\n",
    "        def sort_key(s: str):\n",
    "            toks = s.split()\n",
    "            return (len(toks), len(s))\n",
    "\n",
    "        for t_norm in sorted(norm_terms, key=sort_key, reverse=True):\n",
    "            tokens = t_norm.split()\n",
    "            # se vuoi, qui puoi filtrare: solo multiword, ecc.\n",
    "            # if len(tokens) < 2:  # solo multiword\n",
    "            #     continue\n",
    "\n",
    "            token_pattern = [{\"LOWER\": tok} for tok in tokens]\n",
    "            patterns.append({\"label\": \"TERM\", \"pattern\": token_pattern})\n",
    "\n",
    "        # 3) rimpiazza eventuale entity_ruler esistente\n",
    "        if \"entity_ruler\" in self.nlp.pipe_names:\n",
    "            self.nlp.remove_pipe(\"entity_ruler\")\n",
    "\n",
    "        self.nlp.add_pipe(\"entity_ruler\", name=\"entity_ruler\", first=True)\n",
    "        self.ruler = self.nlp.get_pipe(\"entity_ruler\")\n",
    "        self.ruler.add_patterns(patterns)\n",
    "\n",
    "        print(f\"Built EntityRuler with {len(patterns)} patterns\")\n",
    "\n",
    "    def predict(self, texts: List[str]) -> List[List[str]]:\n",
    "        \"\"\"Extract terms from texts.\"\"\"\n",
    "        if self.ruler is None:\n",
    "            raise RuntimeError(\"Model not built. Call build() first.\")\n",
    "\n",
    "        results = []\n",
    "        for doc in tqdm(self.nlp.pipe(texts, batch_size=32),\n",
    "                        desc=\"Predicting\", total=len(texts)):\n",
    "            # prendo tutte le entità TERM, così come sono nel testo\n",
    "            terms = [ent.text for ent in doc.ents if ent.label_ == \"TERM\"]\n",
    "            results.append(terms)\n",
    "        return results\n",
    "\n",
    "    def save(self, path: str):\n",
    "        \"\"\"Save EntityRuler patterns.\"\"\"\n",
    "        os.makedirs(os.path.dirname(path) or \".\", exist_ok=True)\n",
    "        if self.ruler:\n",
    "            self.ruler.to_disk(path)\n",
    "            print(f\"Model saved to {path}\")\n",
    "\n",
    "    def load(self, path: str):\n",
    "        \"\"\"Load EntityRuler patterns.\"\"\"\n",
    "        if \"entity_ruler\" in self.nlp.pipe_names:\n",
    "            self.nlp.remove_pipe(\"entity_ruler\")\n",
    "\n",
    "        self.nlp.add_pipe(\"entity_ruler\", name=\"entity_ruler\", first=True)\n",
    "        self.ruler = self.nlp.get_pipe(\"entity_ruler\")\n",
    "        self.ruler.from_disk(path)\n",
    "        print(f\"Model loaded from {path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "27eefaaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loaded spaCy model: it_core_news_sm\n",
      "Built EntityRuler with 2 patterns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|██████████| 1/1 [00:00<00:00, 117.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Baseline model works correctly\n",
      "  Test predictions: ['tributo', 'tassa di successione']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Test: Simple predictions\n",
    "baseline = SpacyRuleBaseline()\n",
    "baseline.build(['tributo', 'tassa di successione'])\n",
    "test_preds = baseline.predict(['Il tributo è una tassa di successione.'])\n",
    "assert 'tributo' in test_preds[0]\n",
    "assert 'tassa di successione' in test_preds[0]\n",
    "print(\"✓ Baseline model works correctly\")\n",
    "print(f\"  Test predictions: {test_preds[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf81511",
   "metadata": {},
   "source": [
    "### Run and Evaluate Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "87c8524c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique training terms: 713\n",
      "✓ Loaded spaCy model: it_core_news_sm\n",
      "Built EntityRuler with 710 patterns\n",
      "\n",
      "Running baseline predictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|██████████| 577/577 [00:05<00:00, 105.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Baseline Model Results:\n",
      "Micro-averaged metrics:\n",
      "  Precision: 0.2889\n",
      "  Recall:    0.3415\n",
      "  F1 Score:  0.3130\n",
      "  TP=154, FP=379, FN=297\n",
      "\n",
      "Type-level metrics:\n",
      "  Type Precision: 0.3494\n",
      "  Type Recall:    0.3595\n",
      "  Type F1 Score:  0.3544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Extract unique terms from training data\n",
    "train_terms = set()\n",
    "for s in train_sentences:\n",
    "    train_terms.update(t for t in s['terms'] if t)\n",
    "\n",
    "print(f\"Unique training terms: {len(train_terms)}\")\n",
    "\n",
    "# Build baseline model\n",
    "baseline_model = SpacyRuleBaseline()\n",
    "baseline_model.build(sorted(train_terms))\n",
    "\n",
    "# Predict on dev set\n",
    "dev_texts = [s['sentence_text'] for s in dev_sentences]\n",
    "dev_gold = [s['terms'] for s in dev_sentences]\n",
    "\n",
    "print(\"\\nRunning baseline predictions...\")\n",
    "baseline_preds = baseline_model.predict(dev_texts)\n",
    "\n",
    "# Evaluate\n",
    "precision, recall, f1, tp, fp, fn = micro_f1_score(dev_gold, baseline_preds)\n",
    "type_precision, type_recall, type_f1 = type_f1_score(dev_gold, baseline_preds)\n",
    "\n",
    "print(\"\\nBaseline Model Results:\")\n",
    "print(\"Micro-averaged metrics:\")\n",
    "print(f\"  Precision: {precision:.4f}\")\n",
    "print(f\"  Recall:    {recall:.4f}\")\n",
    "print(f\"  F1 Score:  {f1:.4f}\")\n",
    "print(f\"  TP={tp}, FP={fp}, FN={fn}\")\n",
    "print(\"\\nType-level metrics:\")\n",
    "print(f\"  Type Precision: {type_precision:.4f}\")\n",
    "print(f\"  Type Recall:    {type_recall:.4f}\")\n",
    "print(f\"  Type F1 Score:  {type_f1:.4f}\")\n",
    "\n",
    "# Store metrics for later comparison\n",
    "baseline_metrics = {\n",
    "    'precision': precision,\n",
    "    'recall': recall,\n",
    "    'f1': f1,\n",
    "    'type_precision': type_precision,\n",
    "    'type_recall': type_recall,\n",
    "    'type_f1': type_f1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "05c65527",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/spacy_baseline_refined\n"
     ]
    }
   ],
   "source": [
    "# Save baseline model\n",
    "baseline_model.save('models/spacy_baseline_refined')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019c25b2",
   "metadata": {},
   "source": [
    "## Trained Model: Custom NER\n",
    "\n",
    "Neural approach that learns from examples:\n",
    "- Fine-tunes SpaCy's NER model on term extraction task\n",
    "- Learns patterns and context from labeled data\n",
    "- Can generalize to similar terms not seen during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e8ec48c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing model: it_core_news_sm\n",
      "Preparing training examples...\n",
      "Training on 2 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  15%|█▌        | 3/20 [00:00<00:00, 21.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Iteration 0: Loss = 5.953\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  45%|████▌     | 9/20 [00:00<00:00, 22.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Iteration 5: Loss = 4.633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  75%|███████▌  | 15/20 [00:00<00:00, 24.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Iteration 10: Loss = 3.874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 20/20 [00:00<00:00, 23.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Iteration 15: Loss = 2.896\n",
      "Training complete!\n",
      "✓ Trained model works correctly\n",
      "  Test prediction: ['tributo']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "class SpacyTrainedModel:\n",
    "    \"\"\"Trainable NER model for term extraction.\"\"\"\n",
    "    \n",
    "    def __init__(self, model: str = 'it_core_news_sm'):\n",
    "        self.model_name = model\n",
    "        self.nlp = None\n",
    "    \n",
    "    def _prepare_training_data(self, sentences: List[str], term_lists: List[List[str]]) -> List[Example]:\n",
    "        \"\"\"Convert to SpaCy training format with character-span annotations.\"\"\"\n",
    "        training_data = []\n",
    "        \n",
    "        for sent_text, terms in zip(sentences, term_lists):\n",
    "            doc = self.nlp.make_doc(sent_text)\n",
    "            entities = []\n",
    "            \n",
    "            # Find character spans for each term\n",
    "            for term in terms:\n",
    "                if not term:\n",
    "                    continue\n",
    "                \n",
    "                # Find all occurrences\n",
    "                start_idx = 0\n",
    "                while True:\n",
    "                    start_idx = sent_text.find(term, start_idx)\n",
    "                    if start_idx == -1:\n",
    "                        break\n",
    "                    \n",
    "                    end_idx = start_idx + len(term)\n",
    "                    span = doc.char_span(start_idx, end_idx, label='TERM', alignment_mode='expand')\n",
    "                    if span is not None:\n",
    "                        entities.append((start_idx, end_idx, 'TERM'))\n",
    "                    \n",
    "                    start_idx = end_idx\n",
    "            \n",
    "            # Remove overlapping entities\n",
    "            # It might be that overlapping occurrences exist and should be captured\n",
    "            # For example, if \"servizio di raccolta\" is found, we might want to also capture \"raccolta\" \n",
    "            # In this case, we need to ensure that both spans are included\n",
    "            entities = self._remove_overlapping(entities)\n",
    "            example = Example.from_dict(doc, {'entities': entities})\n",
    "            training_data.append(example)\n",
    "        \n",
    "        return training_data\n",
    "    \n",
    "    def _remove_overlapping(self, entities: List[Tuple[int, int, str]]) -> List[Tuple[int, int, str]]:\n",
    "        \"\"\"Keep longer spans when entities overlap.\"\"\"\n",
    "        if not entities:\n",
    "            return []\n",
    "        \n",
    "        # Sort by start, then by length (descending)\n",
    "        entities = sorted(entities, key=lambda x: (x[0], -(x[1] - x[0])))\n",
    "        \n",
    "        non_overlapping = []\n",
    "        for start, end, label in entities:\n",
    "            overlaps = False\n",
    "            for prev_start, prev_end, _ in non_overlapping:\n",
    "                if not (end <= prev_start or start >= prev_end):\n",
    "                    overlaps = True\n",
    "                    break\n",
    "            if not overlaps:\n",
    "                non_overlapping.append((start, end, label))\n",
    "        \n",
    "        return non_overlapping\n",
    "    \n",
    "    def train(self, sentences: List[str], term_lists: List[List[str]], \n",
    "              n_iter: int = 30, dropout: float = 0.2, batch_size: int = 8):\n",
    "        \"\"\"Train NER model on labeled data.\"\"\"\n",
    "        print(f\"Initializing model: {self.model_name}\")\n",
    "        \n",
    "        # Load base model\n",
    "        try:\n",
    "            self.nlp = spacy.load(self.model_name)\n",
    "        except:\n",
    "            print(f\"Model not found, using blank Italian model\")\n",
    "            self.nlp = spacy.blank('it')\n",
    "        \n",
    "        # Setup NER\n",
    "        if 'ner' not in self.nlp.pipe_names:\n",
    "            ner = self.nlp.add_pipe('ner')\n",
    "        else:\n",
    "            ner = self.nlp.get_pipe('ner')\n",
    "        ner.add_label('TERM')\n",
    "        \n",
    "        # Prepare training data\n",
    "        print(\"Preparing training examples...\")\n",
    "        train_examples = self._prepare_training_data(sentences, term_lists)\n",
    "        train_examples = [ex for ex in train_examples if len(ex.reference.ents) > 0] # Keep only examples with entities\n",
    "        print(f\"Training on {len(train_examples)} examples\")\n",
    "        \n",
    "        # Train\n",
    "        other_pipes = [pipe for pipe in self.nlp.pipe_names if pipe != 'ner']\n",
    "        with self.nlp.disable_pipes(*other_pipes):\n",
    "            #optimizer = self.nlp.begin_training()\n",
    "            if self.model_name == 'it_core_news_sm':\n",
    "                optimizer = self.nlp.resume_training()\n",
    "            else:\n",
    "                optimizer = self.nlp.begin_training()\n",
    "\n",
    "            for iteration in tqdm(range(n_iter), desc=\"Training\", total=n_iter):\n",
    "                random.shuffle(train_examples)\n",
    "                losses = {}\n",
    "                batches = minibatch(train_examples, size=compounding(4.0, batch_size, 1.001))\n",
    "                \n",
    "                for batch in batches:\n",
    "                    self.nlp.update(batch, drop=dropout, losses=losses)\n",
    "                \n",
    "                if iteration % 5 == 0:\n",
    "                    print(f\"  Iteration {iteration}: Loss = {losses.get('ner', 0):.3f}\")\n",
    "        \n",
    "        print(\"Training complete!\")\n",
    "    \n",
    "    def predict(self, sentences: List[str]) -> List[List[str]]:\n",
    "        \"\"\"Extract terms from sentences.\"\"\"\n",
    "        if self.nlp is None:\n",
    "            raise RuntimeError(\"Model not trained. Call train() or load() first.\")\n",
    "        \n",
    "        results = []\n",
    "        for doc in self.nlp.pipe(sentences, batch_size=32):\n",
    "            terms = [ent.text for ent in doc.ents if ent.label_ == 'TERM']\n",
    "            results.append(terms)\n",
    "        return results\n",
    "    \n",
    "    def save(self, path: str):\n",
    "        \"\"\"Save trained model.\"\"\"\n",
    "        if self.nlp is None:\n",
    "            raise RuntimeError(\"No model to save\")\n",
    "        \n",
    "        output_dir = Path(path)\n",
    "        output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        self.nlp.to_disk(output_dir)\n",
    "        print(f\"Model saved to {output_dir}\")\n",
    "    \n",
    "    def load(self, path: str):\n",
    "        \"\"\"Load trained model.\"\"\"\n",
    "        self.nlp = spacy.load(path)\n",
    "        if 'ner' not in self.nlp.pipe_names:\n",
    "            raise ValueError(\"Loaded model doesn't have NER component\")\n",
    "        print(f\"Model loaded from {path}\")\n",
    "\n",
    "\n",
    "# Test: Simple training\n",
    "test_model = SpacyTrainedModel()\n",
    "test_sents = ['Il tributo è importante.', 'La tassa di successione è un tributo.']\n",
    "test_terms = [['tributo'], ['tassa di successione', 'tributo']]\n",
    "test_model.train(test_sents, test_terms, n_iter=20)\n",
    "test_pred = test_model.predict(['Il tributo è fondamentale.'])\n",
    "assert 'tributo' in test_pred[0]\n",
    "print(f\"✓ Trained model works correctly\")\n",
    "print(f\"  Test prediction: {test_pred[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c444e273",
   "metadata": {},
   "source": [
    "### Train and Evaluate Trained Model\n",
    "\n",
    "Note: This cell might take several minutes to run.\n",
    "\n",
    "**Additional configurations to test**\n",
    "- Keep overlapping entities\n",
    "- Keep documents with 0 entities in the training set\n",
    "- Change hyperparameters (*n_iter*, *dropout*, *batch_size*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e9cdbf93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing model: it_core_news_md\n",
      "Preparing training examples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\super\\Documents\\UniPd\\ATA\\ATE-IT_SofiaMaule\\.venv\\Lib\\site-packages\\spacy\\training\\iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"Valore econ unit frazione= valore economico unitar...\" with entities \"[(17, 25, 'TERM'), (68, 76, 'TERM'), (132, 158, 'T...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "c:\\Users\\super\\Documents\\UniPd\\ATA\\ATE-IT_SofiaMaule\\.venv\\Lib\\site-packages\\spacy\\training\\iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"Scarti di cucina, di frutta e di verdura, avanzi d...\" with entities \"[(72, 74, 'TERM'), (113, 115, 'TERM'), (164, 166, ...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 623 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   3%|▎         | 1/30 [00:15<07:30, 15.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Iteration 0: Loss = 2764.503\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|██        | 6/30 [01:32<06:09, 15.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Iteration 5: Loss = 569.971\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  37%|███▋      | 11/30 [02:50<04:53, 15.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Iteration 10: Loss = 332.613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  53%|█████▎    | 16/30 [04:06<03:35, 15.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Iteration 15: Loss = 205.661\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  70%|███████   | 21/30 [05:24<02:19, 15.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Iteration 20: Loss = 174.807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  87%|████████▋ | 26/30 [06:38<00:59, 14.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Iteration 25: Loss = 114.507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 30/30 [07:38<00:00, 15.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Prepare training data\n",
    "train_texts = [s['sentence_text'] for s in train_sentences]\n",
    "train_term_lists = [s['terms'] for s in train_sentences]\n",
    "\n",
    "# Initialize and train model\n",
    "trained_model = SpacyTrainedModel(model='it_core_news_md') #CHANGED\n",
    "\n",
    "trained_model.train(\n",
    "    train_texts, \n",
    "    train_term_lists,\n",
    "    n_iter=30,\n",
    "    dropout=0.1, #changed\n",
    "    batch_size=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a97e1161",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_texts = [s['sentence_text'] for s in dev_sentences]\n",
    "dev_gold = [s['terms'] for s in dev_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8fd629db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running trained model predictions...\n",
      "\n",
      "Trained Model Results:\n",
      "Micro-averaged metrics:\n",
      "  Precision: 0.5894\n",
      "  Recall:    0.3215\n",
      "  F1 Score:  0.4161\n",
      "  TP=145, FP=101, FN=306\n",
      "\n",
      "Type-level metrics:\n",
      "  Type Precision: 0.5597\n",
      "  Type Recall:    0.3678\n",
      "  Type F1 Score:  0.4439\n"
     ]
    }
   ],
   "source": [
    "# Predict on dev set\n",
    "print(\"Running trained model predictions...\")\n",
    "trained_preds = trained_model.predict(dev_texts)\n",
    "\n",
    "# Evaluate\n",
    "precision, recall, f1, tp, fp, fn = micro_f1_score(dev_gold, trained_preds)\n",
    "type_precision, type_recall, type_f1 = type_f1_score(dev_gold, trained_preds)\n",
    "\n",
    "print(\"\\nTrained Model Results:\")\n",
    "print(\"Micro-averaged metrics:\")\n",
    "print(f\"  Precision: {precision:.4f}\")\n",
    "print(f\"  Recall:    {recall:.4f}\")\n",
    "print(f\"  F1 Score:  {f1:.4f}\")\n",
    "print(f\"  TP={tp}, FP={fp}, FN={fn}\")\n",
    "print(\"\\nType-level metrics:\")\n",
    "print(f\"  Type Precision: {type_precision:.4f}\")\n",
    "print(f\"  Type Recall:    {type_recall:.4f}\")\n",
    "print(f\"  Type F1 Score:  {type_f1:.4f}\")\n",
    "\n",
    "# Store metrics for later comparison\n",
    "trained_metrics = {\n",
    "    'precision': precision,\n",
    "    'recall': recall,\n",
    "    'f1': f1,\n",
    "    'type_precision': type_precision,\n",
    "    'type_recall': type_recall,\n",
    "    'type_f1': type_f1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4dc5fb1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models\\spacy_trained_new_md\n"
     ]
    }
   ],
   "source": [
    "# Save trained model\n",
    "trained_model.save('models/spacy_trained_new_md')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5299f2c1",
   "metadata": {},
   "source": [
    "## Results Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69c5983",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro-averaged Metrics:\n",
      "| Model         |   Precision |   Recall |       F1 |\n",
      "|:--------------|------------:|---------:|---------:|\n",
      "| Trained (NER) |    0.589431 | 0.321508 | 0.416069 |\n",
      "\n",
      "\n",
      "Type-level Metrics:\n",
      "| Model         |   Type Precision |   Type Recall |   Type F1 |\n",
      "|:--------------|-----------------:|--------------:|----------:|\n",
      "| Trained (NER) |         0.559748 |      0.367769 |   0.44389 |\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' f1_improvement = (trained_metrics[\\'f1\\'] - baseline_metrics[\\'f1\\']) / baseline_metrics[\\'f1\\'] * 100\\ntype_f1_improvement = (trained_metrics[\\'type_f1\\'] - baseline_metrics[\\'type_f1\\']) / baseline_metrics[\\'type_f1\\'] * 100\\nprint(f\"\\n\\nMicro F1 Score improvement: {f1_improvement:+.1f}%\")\\nprint(f\"Type F1 Score improvement: {type_f1_improvement:+.1f}%\") '"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\"\"\"{\n",
    "        'Model': 'Baseline (Rules)',\n",
    "        'Precision': baseline_metrics['precision'],\n",
    "        'Recall': baseline_metrics['recall'],\n",
    "        'F1': baseline_metrics['f1']\n",
    "    },\"\"\"\n",
    "# Micro-averaged comparison\n",
    "results_df = pd.DataFrame([\n",
    "    {\n",
    "        'Model': 'Trained (NER)',\n",
    "        'Precision': trained_metrics['precision'],\n",
    "        'Recall': trained_metrics['recall'],\n",
    "        'F1': trained_metrics['f1']\n",
    "    },\n",
    "])\n",
    "\n",
    "print(\"Micro-averaged Metrics:\")\n",
    "print(results_df.to_markdown(index=False))\n",
    "\"\"\"{\n",
    "    'Model': 'Baseline (Rules)',\n",
    "    'Type Precision': baseline_metrics['type_precision'],\n",
    "    'Type Recall': baseline_metrics['type_recall'],\n",
    "    'Type F1': baseline_metrics['type_f1']\n",
    "},\"\"\"\n",
    "# Type-level comparison\n",
    "type_results_df = pd.DataFrame([\n",
    "   \n",
    "    {\n",
    "        'Model': 'Trained (NER)',\n",
    "        'Type Precision': trained_metrics['type_precision'],\n",
    "        'Type Recall': trained_metrics['type_recall'],\n",
    "        'Type F1': trained_metrics['type_f1']\n",
    "    }\n",
    "])\n",
    "\n",
    "print(\"\\n\\nType-level Metrics:\")\n",
    "print(type_results_df.to_markdown(index=False))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95db8f7d",
   "metadata": {},
   "source": [
    "## Save Predictions to Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "67116b0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 577 predictions to predictions/new/subtask_a_dev_spacy_trained_MD_preds_refined.json\n"
     ]
    }
   ],
   "source": [
    "def save_predictions(predictions: List[List[str]], \n",
    "                     sentences: List[Dict], \n",
    "                     output_path: str):\n",
    "    \"\"\"Save predictions in competition format.\"\"\"\n",
    "    output = {'data': []}\n",
    "    for pred, sent in zip(predictions, sentences):\n",
    "        output['data'].append({\n",
    "            'document_id': sent['document_id'],\n",
    "            'paragraph_id': sent['paragraph_id'],\n",
    "            'sentence_id': sent['sentence_id'],\n",
    "            'term_list': pred\n",
    "        })\n",
    "    \n",
    "    os.makedirs(os.path.dirname(output_path) or '.', exist_ok=True)\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(output, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"Saved {len(predictions)} predictions to {output_path}\")\n",
    "\n",
    "\n",
    "# Save both sets of predictions\n",
    "#save_predictions(baseline_preds, dev_sentences, 'predictions/new_subtask_a_dev_spacy_baseline_preds.json')\n",
    "save_predictions(trained_preds, dev_sentences, 'predictions/new/subtask_a_dev_spacy_trained_MD_preds_refined.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1187f06d",
   "metadata": {},
   "source": [
    "## Load and Test Saved Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "186390b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loaded spaCy model: it_core_news_sm\n",
      "Model loaded from models/spacy_baseline\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|██████████| 1/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Baseline model saved and loaded correctly\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from models/spacy_trained\n",
      "✓ Trained model saved and loaded correctly\n",
      "\n",
      "All models successfully saved and can be reloaded!\n"
     ]
    }
   ],
   "source": [
    "# Test loading baseline\n",
    "loaded_baseline = SpacyRuleBaseline()\n",
    "loaded_baseline.load('models/spacy_baseline')\n",
    "test_preds_baseline = loaded_baseline.predict([dev_texts[0]])\n",
    "assert test_preds_baseline[0] == baseline_preds[0]\n",
    "print(\"✓ Baseline model saved and loaded correctly\")\n",
    "\n",
    "# Test loading trained model\n",
    "loaded_trained = SpacyTrainedModel()\n",
    "loaded_trained.load('models/spacy_trained')\n",
    "test_preds_trained = loaded_trained.predict([dev_texts[0]])\n",
    "assert test_preds_trained[0] == trained_preds[0]\n",
    "print(\"✓ Trained model saved and loaded correctly\")\n",
    "\n",
    "print(\"\\nAll models successfully saved and can be reloaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d49ec6c",
   "metadata": {},
   "source": [
    "## Example Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "578dad17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: a. per incendi dei rifiuti nei contenitori € 2.000\n",
      "\n",
      "Gold terms: ['rifiuti']\n",
      "\n",
      "Baseline predictions: ['rifiuti']\n",
      "Trained predictions: ['rifiuti']\n",
      "\n",
      "Baseline correct: {'rifiuti'}\n",
      "Trained correct: {'rifiuti'}\n"
     ]
    }
   ],
   "source": [
    "# Show example predictions from both models\n",
    "example_idx = 120\n",
    "example_text = dev_texts[example_idx]\n",
    "example_gold = dev_gold[example_idx]\n",
    "example_baseline = baseline_preds[example_idx]\n",
    "example_trained = trained_preds[example_idx]\n",
    "\n",
    "print(f\"Sentence: {example_text}\\n\")\n",
    "print(f\"Gold terms: {example_gold}\\n\")\n",
    "print(f\"Baseline predictions: {example_baseline}\")\n",
    "print(f\"Trained predictions: {example_trained}\\n\")\n",
    "\n",
    "# Show what each model got right/wrong\n",
    "baseline_correct = set(example_baseline) & set(example_gold)\n",
    "trained_correct = set(example_trained) & set(example_gold)\n",
    "\n",
    "print(f\"Baseline correct: {baseline_correct}\")\n",
    "print(f\"Trained correct: {trained_correct}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ATE-IT venv)",
   "language": "python",
   "name": "ate-it-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
