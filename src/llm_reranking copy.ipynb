{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a0f18a7",
   "metadata": {},
   "source": [
    "# LLM-based Reranking of Candidate Terms for ATE-IT (Subtask A)\n",
    "\n",
    "This notebook shows how to:\n",
    "\n",
    "1. Load the ensemble predictions from **BERT + spaCy + dictionary filter**  \n",
    "   (`subtask_a_dev_ensemble_bert_spacy_dictfilter.json`).\n",
    "2. (Optionally) Load the original **dev sentences** (to give context to the LLM).\n",
    "3. Call a **Gemini LLM** to **rerank and filter** candidate terms for each sentence.\n",
    "4. Save the new predictions in the **same competition format** (JSON with `data` → `term_list`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b15a797",
   "metadata": {},
   "source": [
    "### Imports and basic setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1db6172b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Imports loaded\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Tuple, Optional\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import google.generativeai as genai\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(\"✓ Imports loaded\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "473a6078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working dir: c:\\Users\\super\\Documents\\UniPd\\ATA\\ATE-IT_SofiaMaule\\src\n",
      "Repo root          : c:\\Users\\super\\Documents\\UniPd\\ATA\\ATE-IT_SofiaMaule\n",
      "Predictions path   : c:\\Users\\super\\Documents\\UniPd\\ATA\\ATE-IT_SofiaMaule\\src\\predictions\\subtask_a_dev_ensemble_bert_spacy_dictfilter.json\n",
      "Dev sentences path : c:\\Users\\super\\Documents\\UniPd\\ATA\\ATE-IT_SofiaMaule\\data\\subtask_a_dev.json\n",
      "Output path        : c:\\Users\\super\\Documents\\UniPd\\ATA\\ATE-IT_SofiaMaule\\src\\predictions\\subtask_a_dev_reranked_llm_2.json\n"
     ]
    }
   ],
   "source": [
    "# Current working dir = .../ATE-IT_SofiaMaule/src\n",
    "CWD = Path.cwd()\n",
    "REPO_ROOT = CWD.parent  # .../ATE-IT_SofiaMaule\n",
    "\n",
    "PREDICTIONS_PATH = REPO_ROOT / \"src\" / \"predictions\" / \"subtask_a_dev_ensemble_bert_spacy_dictfilter.json\"\n",
    "DEV_SENTENCES_PATH = REPO_ROOT / \"data\" / \"subtask_a_dev.json\"\n",
    "OUTPUT_PATH = REPO_ROOT / \"src\" / \"predictions\" / \"subtask_a_dev_reranked_llm_2.json\"\n",
    "\n",
    "print(\"Current working dir:\", CWD)\n",
    "print(\"Repo root          :\", REPO_ROOT)\n",
    "print(\"Predictions path   :\", PREDICTIONS_PATH)\n",
    "print(\"Dev sentences path :\", DEV_SENTENCES_PATH)\n",
    "print(\"Output path        :\", OUTPUT_PATH)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c8e03f",
   "metadata": {},
   "source": [
    "### 3. Initialize GROQ  model\n",
    " with an API key stored in `.env` as `GROQI_API_KEY`.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ce68e400",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Groq client initialized\n"
     ]
    }
   ],
   "source": [
    "from groq import Groq\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load env\n",
    "load_dotenv()\n",
    "\n",
    "api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "if not api_key:\n",
    "    raise RuntimeError(\"GROQ_API_KEY not found in .env\")\n",
    "\n",
    "client = Groq(api_key=api_key)\n",
    "\n",
    "print(\"✓ Groq client initialized\")\n",
    "\n",
    "# Choose a Groq-supported model\n",
    "#model_name = \"openai/gpt-oss-20b\"\n",
    "model_name = \"qwen/qwen3-32b\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "3d541283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 577 prediction entries\n",
      "Loaded 577 dev sentences\n"
     ]
    }
   ],
   "source": [
    "# Load predictions\n",
    "\n",
    "with open(PREDICTIONS_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    ensemble_pred = json.load(f)\n",
    "\n",
    "# Sanity check that the top-level key is \"data\"\n",
    "assert \"data\" in ensemble_pred, \"Unexpected format: 'data' key not found in predictions JSON\"\n",
    "\n",
    "print(f\"Loaded {len(ensemble_pred['data'])} prediction entries\")\n",
    "\n",
    "#  Load dev sentences\n",
    "\n",
    "if DEV_SENTENCES_PATH.exists():\n",
    "    with open(DEV_SENTENCES_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "        dev_sentences = json.load(f)\n",
    "    assert \"data\" in dev_sentences, \"Unexpected format: 'data' key not found in dev sentences JSON\"\n",
    "    print(f\"Loaded {len(dev_sentences['data'])} dev sentences\")\n",
    "else:\n",
    "    dev_sentences = None\n",
    "    print(\"Dev sentences file not found. Reranking will use only candidate terms, without sentence context.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "9b83f9ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed 577 sentence texts\n"
     ]
    }
   ],
   "source": [
    "# Build index: (doc, par, sent) -> sentence_text\n",
    "\n",
    "sentence_index: Dict[Tuple[str, int, int], str] = {}\n",
    "\n",
    "if dev_sentences is not None:\n",
    "    for entry in dev_sentences[\"data\"]:\n",
    "        key = (entry[\"document_id\"], entry[\"paragraph_id\"], entry[\"sentence_id\"])\n",
    "        sentence_index[key] = entry[\"sentence_text\"]\n",
    "\n",
    "    print(f\"Indexed {len(sentence_index)} sentence texts\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff8ab8b",
   "metadata": {},
   "source": [
    "### Define the LLM prompt for reranking\n",
    "\n",
    "We now define a **prompt template**. For each sentence, we will provide:\n",
    "\n",
    "- The **sentence text** (if available).\n",
    "- The list of **candidate terms** from BERT+spaCy.\n",
    "\n",
    "We ask the LLM to:\n",
    "\n",
    "1. Decide which candidates are **real domain-relevant terms** in context.\n",
    "2. Assign a **score** in `[0, 1]` (e.g., 0.0–1.0) representing term quality.\n",
    "3. Return a **strict JSON** object with this structure:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"reranked_terms\": [\n",
    "    {\"term\": \"centri di raccolta\", \"score\": 0.95, \"keep\": true},\n",
    "    {\"term\": \"disciplina\", \"score\": 0.80, \"keep\": true},\n",
    "    ...\n",
    "  ]\n",
    "}\n",
    "Then we will:\n",
    "- Keep only items where keep == true.\n",
    "- Sort them by score descending.\n",
    "- Use the sorted term strings as the new term_list for that sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40defbda",
   "metadata": {},
   "source": [
    "## 1. Domain-aware reranking prompt\n",
    "\n",
    "We enrich the system prompt with **explicit domain knowledge** and tell the model that its scores will be combined with a rule-based domain scorer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "13b9b6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt_rerank = \"\"\"\n",
    "You are an automatic term extraction *reranking* agent for Italian municipal waste management texts.\n",
    "\n",
    "You will receive:\n",
    "- one sentence (in Italian), and\n",
    "- a list of candidate terms extracted by a baseline system.\n",
    "\n",
    "Your task:\n",
    "- For each candidate term, decide if it is a good domain-relevant term in the context of the sentence.\n",
    "- Assign a relevance score between 0.0 and 1.0 (higher = better).\n",
    "- Decide whether to keep or discard each candidate.\n",
    "\n",
    "A valid \"term\" in this task is:\n",
    "- a single- or multi-word expression\n",
    "- that refers to a concept in the municipal waste management domain\n",
    "- typically nouns or noun phrases (sometimes adjectives or verbs as part of a phrase)\n",
    "- examples: \"tassa rifiuti\", \"tari\", \"isola ecologica comunale\", \"impianto di trattamento rifiuti urbani\"\n",
    "\n",
    "Non-terms are:\n",
    "- generic function words (e.g., \"e\", \"di\", \"per\", \"che\")\n",
    "- pure numbers or dates not part of a waste term\n",
    "- person names, city names, street names (unless part of an official name of a waste service)\n",
    "\n",
    "IMPORTANT DOMAIN-SPECIFIC RULES:\n",
    "- DO NOT output generic single materials as terms (e.g., \"plastica\", \"carta\", \"metalli\", \"metallo\", \"alluminio\", \"vetro\"),\n",
    "  unless they are part of a multi-word term (e.g., \"raccolta plastica\", \"plastica, acciaio e alluminio\").\n",
    "- Prefer complete multi-word terms over shorter fragments.\n",
    "  For example, prefer \"modalità di conferimento\" over \"modalità\" alone.\n",
    "- Single-word terms are usually NOT valid unless they refer to well-defined waste concepts\n",
    "  such as \"TARI\", \"TARES\", or \"disciplinare\" when clearly used as the waste regulation.\n",
    "- DO NOT discard relevant acronyms commonly used in the waste domain, such as:\n",
    "  \"CCR\", \"RUP\", \"RAEE\", \"R.A.E.E.\", \"PAP\". These acronyms should normally be kept as valid terms.\n",
    "\n",
    "OUTPUT FORMAT (STRICT JSON):\n",
    "\n",
    "You MUST output ONLY a JSON object with this exact structure:\n",
    "\n",
    "{\n",
    "  \"reranked_terms\": [\n",
    "    {\"term\": \"...\", \"score\": 0.0, \"keep\": true or false},\n",
    "    ...\n",
    "  ]\n",
    "}\n",
    "\n",
    "Rules:\n",
    "- Do not add new terms that are not in the candidate list.\n",
    "- Do not modify the spelling of the candidates.\n",
    "- If a candidate looks truncated or not a full concept, set \"keep\": false and give it a low score.\n",
    "- If a candidate matches a valid domain concept, do NOT remove it just because it is short.\n",
    "- If no candidate is good, you may return an empty list: \"reranked_terms\": [].\n",
    "- The JSON must be valid and parseable by Python's json.loads().\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "93043d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_llm_input(\n",
    "    sentence_text: Optional[str],\n",
    "    candidates: List[str]\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Build the USER part of the prompt sent to Gemini, consistent with the 'System: ... / User: ...' pattern.\n",
    "    \"\"\"\n",
    "    lines = []\n",
    "\n",
    "    if sentence_text is not None:\n",
    "        lines.append(f\"Sentence (Italian): {sentence_text}\")\n",
    "    else:\n",
    "        lines.append(\"Sentence: [NOT AVAILABLE]\")\n",
    "\n",
    "    lines.append(\"Candidate terms:\")\n",
    "    for t in candidates:\n",
    "        lines.append(f\"- {t}\")\n",
    "\n",
    "    lines.append(\n",
    "        \"\\nNow produce ONLY the JSON object with the structure described in the system instructions.\"\n",
    "    )\n",
    "\n",
    "    return \"\\n\".join(lines)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e703d5",
   "metadata": {},
   "source": [
    "##  Rule-based domain scorer (hybrid with LLM)\n",
    "\n",
    "We now define a small **domain vocabulary** and a function that adjusts the LLM scores:\n",
    "\n",
    "- boost terms that match domain patterns (TARI, RAEE, centro di raccolta, conferire, ecc.)\n",
    "- penalize generic or clearly bad terms\n",
    "- detect truncated phrases\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d3222a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Domain vocab + helpers for hybrid scoring\n",
    "\n",
    "import math\n",
    "ACRONYMS = {\n",
    "    \"ccr\",\n",
    "    \"rup\",\n",
    "    \"raee\",\n",
    "    \"r.a.e.e.\",\n",
    "    \"pap\",\n",
    "}\n",
    "\n",
    "BAD_SINGLE_WORDS = {\n",
    "    \"plastica\",\n",
    "    \"carta\",\n",
    "    \"cartone\",\n",
    "    \"metallo\",\n",
    "    \"metalli\",\n",
    "    \"alluminio\",\n",
    "    \"vetro\",\n",
    "}\n",
    "# You can refine / expand this over time\n",
    "DOMAIN_STRONG_TERMS = {\n",
    "    \"rifiuti urbani\",\n",
    "    \"rifiuti ingombranti\",\n",
    "    \"rifiuti pericolosi\",\n",
    "    \"raccolta differenziata\",\n",
    "    \"raccolta porta a porta\",\n",
    "    \"servizio di raccolta\",\n",
    "    \"servizio di igiene urbana\",\n",
    "    \"centro di raccolta\",\n",
    "    \"centri di raccolta comunali\",\n",
    "    \"isola ecologica\",\n",
    "    \"ecocentro\",\n",
    "    \"piattaforma ecologica\",\n",
    "    \"impianto di trattamento rifiuti\",\n",
    "    \"impianto di smaltimento\",\n",
    "    \"tassa rifiuti\",\n",
    "    \"tari\",\n",
    "    \"disciplinare\",\n",
    "    \"regolamento\",\n",
    "    \"utenze domestiche\",\n",
    "    \"utenze non domestiche\",\n",
    "    \"modalità di conferimento\",\n",
    "    \"modalità di raccolta\",\n",
    "    \"conferimento\",\n",
    "    \"conferire\",\n",
    "    \"conferiti\",\n",
    "    \"vanno conferiti\",\n",
    "}\n",
    "\n",
    "# Substring-based signals (if term contains one of these, it's likely relevant)\n",
    "DOMAIN_KEYWORD_SUBSTRINGS = [\n",
    "    \"rifiuti\",\n",
    "    \"raccolta\",\n",
    "    \"confer\",\n",
    "    \"ecologic\",\n",
    "    \"centro di raccolta\",\n",
    "    \"impianto\",\n",
    "    \"tariff\",\n",
    "    \"tassa\",\n",
    "    \"tari\",\n",
    "    \"raee\",\n",
    "    \"isola ecologica\",\n",
    "]\n",
    "\n",
    "# Terms that are often too generic if used alone\n",
    "GENERIC_WEAK_TERMS = {\n",
    "    \"rifiuti\",\n",
    "    \"plastica\",\n",
    "    \"carta\",\n",
    "    \"vetro\",\n",
    "    \"metalli\",\n",
    "    \"alluminio\",\n",
    "    \"legno\",\n",
    "}\n",
    "\n",
    "FUNCTION_ENDINGS = {\"di\", \"dei\", \"degli\", \"delle\", \"del\", \"e\", \"o\", \"ed\", \"al\", \"allo\", \"alla\", \"ai\", \"agli\", \"alle\"}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c1d65ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def normalize_term_text(t: str) -> str:\n",
    "    return \" \".join(t.lower().strip().split())\n",
    "\n",
    "\n",
    "def looks_truncated(term: str) -> bool:\n",
    "    \"\"\"\n",
    "    Heuristic: terms that end with a function word or are extremely short / odd.\n",
    "    \"\"\"\n",
    "    t = normalize_term_text(term)\n",
    "    tokens = t.split()\n",
    "    if len(tokens) == 0:\n",
    "        return True\n",
    "    if len(tokens) == 1 and tokens[0] in {\"r\", \"oo\"}:\n",
    "        return True\n",
    "    if tokens[-1] in FUNCTION_ENDINGS:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def contains_domain_substring(term: str) -> bool:\n",
    "    t = normalize_term_text(term)\n",
    "    return any(sub in t for sub in DOMAIN_KEYWORD_SUBSTRINGS)\n",
    "\n",
    "\n",
    "# Hyperparameters for hybrid scoring\n",
    "LLM_WEIGHT = 0.7       # how much we trust the LLM score\n",
    "RULE_WEIGHT = 0.3      # how much we trust the rules\n",
    "KEEP_THRESHOLD = 0.35  # final score threshold to keep a term\n",
    "\n",
    "\n",
    "def hybrid_score_term(term: str, llm_score: float, sentence_text: str | None = None) -> tuple[float, bool]:\n",
    "    \"\"\"\n",
    "    Combine LLM score with domain-driven rule-based score.\n",
    "\n",
    "    Returns:\n",
    "      final_score, keep_flag\n",
    "    \"\"\"\n",
    "    t_norm = normalize_term_text(term)\n",
    "    base_rule_score = 0.5  # neutral baseline\n",
    "\n",
    "    # Strong domain terms → boost\n",
    "    if t_norm in {normalize_term_text(x) for x in DOMAIN_STRONG_TERMS}:\n",
    "        base_rule_score += 0.3\n",
    "\n",
    "    # Contains domain-ish substring → slight boost\n",
    "    if contains_domain_substring(term):\n",
    "        base_rule_score += 0.15\n",
    "\n",
    "    # Weak generic terms used alone → penalty\n",
    "    if t_norm in GENERIC_WEAK_TERMS and len(t_norm.split()) == 1:\n",
    "        base_rule_score -= 0.2\n",
    "\n",
    "    # Truncated or clearly bad-looking → strong penalty\n",
    "    if looks_truncated(term):\n",
    "        base_rule_score -= 0.4\n",
    "\n",
    "    # Clip rule score to [0, 1]\n",
    "    rule_score = max(0.0, min(1.0, base_rule_score))\n",
    "\n",
    "    # Combine LLM + rules\n",
    "    llm_score_clipped = max(0.0, min(1.0, llm_score))\n",
    "    final_score = LLM_WEIGHT * llm_score_clipped + RULE_WEIGHT * rule_score\n",
    "\n",
    "    # Decision to keep\n",
    "    \n",
    "    keep_flag = final_score >= KEEP_THRESHOLD\n",
    "\n",
    "    # Safety: if LLM says keep AND rule_score > 0.7, force keep even if threshold borderline\n",
    "    if llm_score_clipped >= 0.6 and rule_score >= 0.7:\n",
    "        keep_flag = True\n",
    "\n",
    "    return final_score, keep_flag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5aff66c",
   "metadata": {},
   "source": [
    "###  LLM call helper\n",
    "\n",
    "We now define a helper function that:\n",
    "\n",
    "1. Builds the prompt text.\n",
    "2. Calls `model.generate_content(...)`.\n",
    "3. Extracts and parses the JSON part.\n",
    "4. Returns a **list of dicts** (`{\"term\", \"score\", \"keep\"}`) or an empty list if something fails.\n",
    "\n",
    "\n",
    "We call Groq as before, but then **post-process each term** with `hybrid_score_term(...)` \n",
    "and overwrite both `score` and `keep` according to the hybrid logic.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "6dc2a651",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ Groq-based LLM reranking with simple domain heuristics\n",
    "\n",
    "def call_llm_rerank(\n",
    "    sentence_text: Optional[str],\n",
    "    candidates: List[str],\n",
    "    dry_run: bool = False\n",
    ") -> List[Dict[str, Any]]:\n",
    "\n",
    "    if not candidates:\n",
    "        return []\n",
    "\n",
    "    user_prompt = build_llm_input(sentence_text, candidates)\n",
    "\n",
    "    # Dry-run option to skip API call\n",
    "    if dry_run:\n",
    "        return [{\"term\": c, \"score\": 0.5, \"keep\": True} for c in candidates]\n",
    "\n",
    "    try:\n",
    "        # Call Groq LLM — ChatCompletion style\n",
    "        response = client.chat.completions.create(\n",
    "            model=model_name,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt_rerank},\n",
    "                {\"role\": \"user\", \"content\": user_prompt},\n",
    "            ],\n",
    "            temperature=0.15,\n",
    "        )\n",
    "\n",
    "        text = response.choices[0].message.content.strip()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"⚠️ LLM call failed:\", e)\n",
    "        # Fallback: keep all as-is\n",
    "        return [{\"term\": c, \"score\": 0.5, \"keep\": True} for c in candidates]\n",
    "\n",
    "    # Try to parse JSON from the model output\n",
    "    try:\n",
    "        data = json.loads(text)\n",
    "    except json.JSONDecodeError:\n",
    "        # Try to extract substring containing JSON\n",
    "        try:\n",
    "            start = text.index(\"{\")\n",
    "            end = text.rindex(\"}\") + 1\n",
    "            data = json.loads(text[start:end])\n",
    "        except Exception as e2:\n",
    "            print(\"⚠️ JSON parsing failed, using fallback:\", e2)\n",
    "            return [{\"term\": c, \"score\": 0.5, \"keep\": True} for c in candidates]\n",
    "\n",
    "    reranked = data.get(\"reranked_terms\", [])\n",
    "    cleaned: List[Dict[str, Any]] = []\n",
    "\n",
    "    # --- Base cleaning from JSON ---\n",
    "    for item in reranked:\n",
    "        term = item.get(\"term\")\n",
    "        if term is None or term not in candidates:\n",
    "            continue\n",
    "\n",
    "        # Safe parsing\n",
    "        try:\n",
    "            score = float(item.get(\"score\", 0.0))\n",
    "        except Exception:\n",
    "            score = 0.0\n",
    "\n",
    "        keep = bool(item.get(\"keep\", True))\n",
    "\n",
    "        cleaned.append({\"term\": term, \"score\": score, \"keep\": keep})\n",
    "\n",
    "    # --- Domain heuristics post-processing ---\n",
    "\n",
    "    # 1) Always keep acronyms (if present among candidates)\n",
    "    for t in candidates:\n",
    "        if t.lower() in ACRONYMS:\n",
    "            # Check if already present\n",
    "            found = next((x for x in cleaned if x[\"term\"] == t), None)\n",
    "            if found:\n",
    "                found[\"keep\"] = True\n",
    "                # boost score a bit\n",
    "                found[\"score\"] = max(found[\"score\"], 0.8)\n",
    "            else:\n",
    "                cleaned.append({\"term\": t, \"score\": 0.8, \"keep\": True})\n",
    "\n",
    "    # 2) Remove generic single-material words if the model kept them\n",
    "    for item in cleaned:\n",
    "        term_lower = item[\"term\"].lower().strip()\n",
    "        # if it is exactly one token and in BAD_SINGLE_WORDS → force drop\n",
    "        if len(term_lower.split()) == 1 and term_lower in BAD_SINGLE_WORDS:\n",
    "            item[\"keep\"] = False\n",
    "            # optionally lower score\n",
    "            item[\"score\"] = min(item[\"score\"], 0.1)\n",
    "\n",
    "    # If everything got filtered out, fall back to keeping all candidates\n",
    "    if not cleaned:\n",
    "        cleaned = [{\"term\": c, \"score\": 0.5, \"keep\": True} for c in candidates]\n",
    "\n",
    "    return cleaned\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74cec5f5",
   "metadata": {},
   "source": [
    "### Apply reranking to all sentences\n",
    "\n",
    "We now:\n",
    "\n",
    "1. Iterate over each entry in `ensemble_pred[\"data\"]`.\n",
    "2. Extract:\n",
    "   - `document_id`, `paragraph_id`, `sentence_id`\n",
    "   - `term_list` (candidate terms)\n",
    "3. Look up the **sentence text** using `sentence_index` (if available).\n",
    "4. Call `call_llm_rerank(...)`.\n",
    "5. Filter and sort terms:\n",
    "   - Keep only `keep == True`.\n",
    "   - Sort by `score` descending.\n",
    "6. Build a new `data` list with the **same structure** as the original predictions, but with reranked `term_list`.\n",
    "\n",
    "We also add a `dry_run` option for debugging without making real API calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "c5010b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Apply reranking\n",
    "\n",
    "def rerank_all_entries(\n",
    "    predictions: Dict[str, Any],\n",
    "    sentence_index: Dict[Tuple[str, int, int], str],\n",
    "    use_sentence_context: bool = True,\n",
    "    dry_run: bool = False,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Apply LLM reranking to all entries in predictions[\"data\"].\n",
    "\n",
    "    Returns a new dict with the same structure, but reranked term_list.\n",
    "    \"\"\"\n",
    "    new_data = []\n",
    "\n",
    "    for entry in tqdm(predictions[\"data\"], desc=\"Reranking terms\"):\n",
    "        doc_id = entry[\"document_id\"]\n",
    "        par_id = entry[\"paragraph_id\"]\n",
    "        sent_id = entry[\"sentence_id\"]\n",
    "        candidates = entry.get(\"term_list\", [])\n",
    "\n",
    "        key = (doc_id, par_id, sent_id)\n",
    "        sentence_text = sentence_index.get(key) if (use_sentence_context and sentence_index) else None\n",
    "\n",
    "        reranked_items = call_llm_rerank(\n",
    "            sentence_text=sentence_text,\n",
    "            candidates=candidates,\n",
    "            dry_run=dry_run,\n",
    "        )\n",
    "\n",
    "        # Filter by keep==True and sort by score (descending)\n",
    "        kept = [item for item in reranked_items if item[\"keep\"]]\n",
    "        kept_sorted = sorted(kept, key=lambda x: x[\"score\"], reverse=True)\n",
    "\n",
    "        new_term_list = [item[\"term\"] for item in kept_sorted]\n",
    "\n",
    "        new_entry = {\n",
    "            \"document_id\": doc_id,\n",
    "            \"paragraph_id\": par_id,\n",
    "            \"sentence_id\": sent_id,\n",
    "            \"term_list\": new_term_list,\n",
    "        }\n",
    "        new_data.append(new_entry)\n",
    "\n",
    "    return {\"data\": new_data}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab626749",
   "metadata": {},
   "source": [
    "## 8. Quick dry-run (no real LLM calls)\n",
    "\n",
    "Before spending tokens, we can test the pipeline in **dry_run** mode, which:\n",
    "\n",
    "- Skips real LLM calls.\n",
    "- Assigns a dummy score of 0.5 to every candidate.\n",
    "- Keeps all terms, but goes through the whole structure.\n",
    "\n",
    "This is useful to detect path / format issues.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "63a8157b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reranking terms: 100%|██████████| 5/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"document_id\": \"doc_praiano_07\",\n",
      "    \"paragraph_id\": 32,\n",
      "    \"sentence_id\": 7,\n",
      "    \"term_list\": []\n",
      "  },\n",
      "  {\n",
      "    \"document_id\": \"doc_caserta_06\",\n",
      "    \"paragraph_id\": 3,\n",
      "    \"sentence_id\": 1,\n",
      "    \"term_list\": [\n",
      "      \"disciplinare\",\n",
      "      \"centri di raccolta comunali\",\n",
      "      \"disciplina\",\n",
      "      \"centri di raccolta dei rifiuti urbani raccolti\"\n",
      "    ]\n",
      "  }\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 8. Dry run test on a small subset\n",
    "\n",
    "test_predictions = {\n",
    "    \"data\": ensemble_pred[\"data\"][:5]  # only first 5 entries for a quick test\n",
    "}\n",
    "\n",
    "reranked_test = rerank_all_entries(\n",
    "    predictions=test_predictions,\n",
    "    sentence_index=sentence_index,\n",
    "    use_sentence_context=True,\n",
    "    dry_run=True,   # <-- no real API calls\n",
    ")\n",
    "\n",
    "print(json.dumps(reranked_test[\"data\"][:2], indent=2, ensure_ascii=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "fd386aaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reranking terms: 100%|██████████| 577/577 [30:14<00:00,  3.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Completed LLM reranking\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 9. Full reranking with real LLM calls\n",
    "\n",
    "USE_SENTENCE_CONTEXT = True    # set to False if you don't have dev sentences\n",
    "DRY_RUN = False                # set to True if you want to test without API calls\n",
    "\n",
    "reranked_full = rerank_all_entries(\n",
    "    predictions=ensemble_pred,\n",
    "    sentence_index=sentence_index,\n",
    "    use_sentence_context=USE_SENTENCE_CONTEXT,\n",
    "    dry_run=DRY_RUN,\n",
    ")\n",
    "\n",
    "print(\"✓ Completed LLM reranking\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "cad5e436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Saved reranked predictions to: c:\\Users\\super\\Documents\\UniPd\\ATA\\ATE-IT_SofiaMaule\\src\\predictions\\subtask_a_dev_reranked_llm_2.json\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 10. Save output\n",
    "\n",
    "OUTPUT_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with open(OUTPUT_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(reranked_full, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"✓ Saved reranked predictions to: {OUTPUT_PATH}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049c5459",
   "metadata": {},
   "source": [
    "## 11. Evaluate reranking with Micro / Type F1\n",
    "\n",
    "We now evaluate the **reranked LLM output** against the **gold dev annotations**, using the usual:\n",
    "\n",
    "- `micro_f1_score(...)`\n",
    "- `type_f1_score(...)`\n",
    "\n",
    "We assume those functions are already defined in the notebook (as you pasted).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "9bf96cac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Evaluation functions defined\n"
     ]
    }
   ],
   "source": [
    "def micro_f1_score(gold_standard, system_output):\n",
    "    \"\"\"\n",
    "    Evaluates performance using Precision, Recall, and F1 score \n",
    "    based on individual term matching (micro-average).\n",
    "    \"\"\"\n",
    "    total_true_positives = 0\n",
    "    total_false_positives = 0\n",
    "    total_false_negatives = 0\n",
    "    \n",
    "    for gold, system in zip(gold_standard, system_output):\n",
    "        gold_set = set(gold)\n",
    "        system_set = set(system)\n",
    "        \n",
    "        true_positives = len(gold_set.intersection(system_set))\n",
    "        false_positives = len(system_set - gold_set)\n",
    "        false_negatives = len(gold_set - system_set)\n",
    "        \n",
    "        total_true_positives += true_positives\n",
    "        total_false_positives += false_positives\n",
    "        total_false_negatives += false_negatives\n",
    "    \n",
    "    precision = total_true_positives / (total_true_positives + total_false_positives) if (total_true_positives + total_false_positives) > 0 else 0\n",
    "    recall = total_true_positives / (total_true_positives + total_false_negatives) if (total_true_positives + total_false_negatives) > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    return precision, recall, f1, total_true_positives, total_false_positives, total_false_negatives\n",
    "\n",
    "\n",
    "def type_f1_score(gold_standard, system_output):\n",
    "    \"\"\"\n",
    "    Evaluates performance using Type Precision, Type Recall, and Type F1 score\n",
    "    based on the set of unique terms extracted at least once across the entire dataset.\n",
    "    \"\"\"\n",
    "    all_gold_terms = set()\n",
    "    for item_terms in gold_standard:\n",
    "        all_gold_terms.update(item_terms)\n",
    "    \n",
    "    all_system_terms = set()\n",
    "    for item_terms in system_output:\n",
    "        all_system_terms.update(item_terms)\n",
    "    \n",
    "    type_true_positives = len(all_gold_terms.intersection(all_system_terms))\n",
    "    type_false_positives = len(all_system_terms - all_gold_terms)\n",
    "    type_false_negatives = len(all_gold_terms - all_system_terms)\n",
    "    \n",
    "    type_precision = type_true_positives / (type_true_positives + type_false_positives) if (type_true_positives + type_false_positives) > 0 else 0\n",
    "    type_recall = type_true_positives / (type_true_positives + type_false_negatives) if (type_true_positives + type_false_negatives) > 0 else 0\n",
    "    type_f1 = 2 * (type_precision * type_recall) / (type_precision + type_recall) if (type_precision + type_recall) > 0 else 0\n",
    "    \n",
    "    return type_precision, type_recall, type_f1\n",
    "\n",
    "\n",
    "print(\"✓ Evaluation functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "d95fe044",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(DEV_SENTENCES_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    gold_json = json.load(f)\n",
    "\n",
    "with open(OUTPUT_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    reranked_json = json.load(f)\n",
    "\n",
    "with open(PREDICTIONS_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    ensemble_json = json.load(f)\n",
    "\n",
    "gold_data = gold_json[\"data\"]\n",
    "reranked_data = reranked_json[\"data\"]\n",
    "ensemble_data = ensemble_json[\"data\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "bc1465aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Alignment OK for Ensemble vs Gold\n",
      "✓ Alignment OK for Reranked vs Gold\n"
     ]
    }
   ],
   "source": [
    "def check_alignment(gold, system, name: str):\n",
    "    if len(gold) != len(system):\n",
    "        raise ValueError(f\"[{name}] Length mismatch: gold={len(gold)}, system={len(system)}\")\n",
    "\n",
    "    for i, (g, s) in enumerate(zip(gold, system)):\n",
    "        g_key = (g[\"document_id\"], g[\"paragraph_id\"], g[\"sentence_id\"])\n",
    "        s_key = (s[\"document_id\"], s[\"paragraph_id\"], s[\"sentence_id\"])\n",
    "        if g_key != s_key:\n",
    "            raise ValueError(\n",
    "                f\"[{name}] ID mismatch at index {i}:\\n\"\n",
    "                f\"  gold   = {g_key}\\n\"\n",
    "                f\"  system = {s_key}\"\n",
    "            )\n",
    "\n",
    "    print(f\"✓ Alignment OK for {name}\")\n",
    "\n",
    "check_alignment(gold_data, ensemble_data, name=\"Ensemble vs Gold\")\n",
    "check_alignment(gold_data, reranked_data, name=\"Reranked vs Gold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "c1360975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example gold terms     : ['disciplina dei centri di raccolta dei rifiuti urbani raccolti in modo differenziato', 'disciplinare per la gestione dei centri di raccolta comunali']\n",
      "Example ensemble terms : ['disciplinare', 'centri di raccolta comunali', 'disciplina', 'centri di raccolta dei rifiuti urbani raccolti']\n",
      "Example reranked terms : ['disciplinare', 'centri di raccolta comunali']\n"
     ]
    }
   ],
   "source": [
    "# Extract term lists (gold, ensemble, reranked)\n",
    "\n",
    "gold_terms = [entry.get(\"term_list\", []) for entry in gold_data]\n",
    "ensemble_terms = [entry.get(\"term_list\", []) for entry in ensemble_data]\n",
    "reranked_terms = [entry.get(\"term_list\", []) for entry in reranked_data]\n",
    "\n",
    "print(\"Example gold terms     :\", gold_terms[1][:5])\n",
    "print(\"Example ensemble terms :\", ensemble_terms[1][:5])\n",
    "print(\"Example reranked terms :\", reranked_terms[1][:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "39d217d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Ensemble (BERT + spaCy + dict) ===\n",
      "Micro Precision : 0.739\n",
      "Micro Recall    : 0.696\n",
      "Micro F1        : 0.717\n",
      "  TP / FP / FN  : 314 / 111 / 137\n",
      "\n",
      "Type Precision  : 0.688\n",
      "Type Recall     : 0.620\n",
      "Type F1         : 0.652\n"
     ]
    }
   ],
   "source": [
    "# Compute metrics for original ensemble baseline\n",
    "\n",
    "ens_p, ens_r, ens_f1, ens_tp, ens_fp, ens_fn = micro_f1_score(gold_terms, ensemble_terms)\n",
    "ens_tp_p, ens_tp_r, ens_tp_f1 = type_f1_score(gold_terms, ensemble_terms)\n",
    "\n",
    "print(\"=== Ensemble (BERT + spaCy + dict) ===\")\n",
    "print(f\"Micro Precision : {ens_p:.3f}\")\n",
    "print(f\"Micro Recall    : {ens_r:.3f}\")\n",
    "print(f\"Micro F1        : {ens_f1:.3f}\")\n",
    "print(f\"  TP / FP / FN  : {ens_tp} / {ens_fp} / {ens_fn}\")\n",
    "print()\n",
    "print(f\"Type Precision  : {ens_tp_p:.3f}\")\n",
    "print(f\"Type Recall     : {ens_tp_r:.3f}\")\n",
    "print(f\"Type F1         : {ens_tp_f1:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "e16e1b1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== LLM Reranked (Groq) ===\n",
      "Micro Precision : 0.813\n",
      "Micro Recall    : 0.539\n",
      "Micro F1        : 0.648\n",
      "  TP / FP / FN  : 243 / 56 / 208\n",
      "\n",
      "Type Precision  : 0.750\n",
      "Type Recall     : 0.521\n",
      "Type F1         : 0.615\n"
     ]
    }
   ],
   "source": [
    "#  Compute metrics for LLM-reranked system\n",
    "\n",
    "llm_p, llm_r, llm_f1, llm_tp, llm_fp, llm_fn = micro_f1_score(gold_terms, reranked_terms)\n",
    "llm_tp_p, llm_tp_r, llm_tp_f1 = type_f1_score(gold_terms, reranked_terms)\n",
    "\n",
    "print(\"=== LLM Reranked (Groq) ===\")\n",
    "print(f\"Micro Precision : {llm_p:.3f}\")\n",
    "print(f\"Micro Recall    : {llm_r:.3f}\")\n",
    "print(f\"Micro F1        : {llm_f1:.3f}\")\n",
    "print(f\"  TP / FP / FN  : {llm_tp} / {llm_fp} / {llm_fn}\")\n",
    "print()\n",
    "print(f\"Type Precision  : {llm_tp_p:.3f}\")\n",
    "print(f\"Type Recall     : {llm_tp_r:.3f}\")\n",
    "print(f\"Type F1         : {llm_tp_f1:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913c1232",
   "metadata": {},
   "source": [
    "=== LLM Reranked (Groq) ===\n",
    "Micro Precision : 0.739\n",
    "Micro Recall    : 0.696\n",
    "Micro F1        : 0.717\n",
    "  TP / FP / FN  : 314 / 111 / 137\n",
    "\n",
    "Type Precision  : 0.688\n",
    "Type Recall     : 0.620\n",
    "Type F1         : 0.652"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "32d341da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>micro_precision</th>\n",
       "      <th>micro_recall</th>\n",
       "      <th>micro_f1</th>\n",
       "      <th>type_precision</th>\n",
       "      <th>type_recall</th>\n",
       "      <th>type_f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ensemble</td>\n",
       "      <td>0.738824</td>\n",
       "      <td>0.696231</td>\n",
       "      <td>0.716895</td>\n",
       "      <td>0.688073</td>\n",
       "      <td>0.619835</td>\n",
       "      <td>0.652174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>llm_reranked</td>\n",
       "      <td>0.812709</td>\n",
       "      <td>0.538803</td>\n",
       "      <td>0.648000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.520661</td>\n",
       "      <td>0.614634</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          model  micro_precision  micro_recall  micro_f1  type_precision  \\\n",
       "0      ensemble         0.738824      0.696231  0.716895        0.688073   \n",
       "1  llm_reranked         0.812709      0.538803  0.648000        0.750000   \n",
       "\n",
       "   type_recall   type_f1  \n",
       "0     0.619835  0.652174  \n",
       "1     0.520661  0.614634  "
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compact comparison summary\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "summary = pd.DataFrame(\n",
    "    [\n",
    "        [\"ensemble\", ens_p, ens_r, ens_f1, ens_tp_p, ens_tp_r, ens_tp_f1],\n",
    "        [\"llm_reranked\", llm_p, llm_r, llm_f1, llm_tp_p, llm_tp_r, llm_tp_f1],\n",
    "    ],\n",
    "    columns=[\n",
    "        \"model\",\n",
    "        \"micro_precision\",\n",
    "        \"micro_recall\",\n",
    "        \"micro_f1\",\n",
    "        \"type_precision\",\n",
    "        \"type_recall\",\n",
    "        \"type_f1\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e80086",
   "metadata": {},
   "source": [
    "## 11. Next steps and tuning ideas\n",
    "\n",
    "Some ideas to improve the reranking quality:\n",
    "\n",
    "1. **Adjust the prompt**:\n",
    "   - Be more strict or more permissive in the instructions.\n",
    "   - Emphasize multi-word terms or certain POS patterns.\n",
    "\n",
    "2. **Control filtering**:\n",
    "   - After reranking, you can:\n",
    "     - Keep only the top-K terms per sentence (e.g., top 3 or top 5).\n",
    "     - Discard terms with score `< 0.4` (or another threshold).\n",
    "\n",
    "3. **Domain-specific heuristics**:\n",
    "   - Penalize candidates that end with stopwords like *\"di\"*, *\"dei\"*, *\"e\"*, etc.\n",
    "   - Boost terms that include frequent domain keywords (*\"rifiuti\"*, *\"raccolta\"*, *\"centro di raccolta\"*, etc.).\n",
    "\n",
    "You can implement these as a post-processing step on `reranked_full[\"data\"]` before saving, or directly instruct the LLM in the prompt.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b942da",
   "metadata": {},
   "source": [
    "## ADVANCED DEBUG ANALYSIS FOR RERANKING QUALITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "969f1f8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DEBUG ANALYSIS START ===\n",
      "\n",
      "✓ Debug dataframe created\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter, defaultdict\n",
    "import pandas as pd\n",
    "\n",
    "print(\"=== DEBUG ANALYSIS START ===\\n\")\n",
    "\n",
    "debug_rows = []\n",
    "\n",
    "for i, (gold, ens, rer) in enumerate(zip(gold_terms, ensemble_terms, reranked_terms)):\n",
    "\n",
    "    gold_set = set(gold)\n",
    "    ens_set = set(ens)\n",
    "    rer_set = set(rer)\n",
    "\n",
    "    lost_terms = list(ens_set - rer_set)\n",
    "    added_terms = list(rer_set - ens_set)\n",
    "\n",
    "    false_positives = list(rer_set - gold_set)\n",
    "    true_positives = list(rer_set & gold_set)\n",
    "    new_true_positives = list((rer_set - ens_set) & gold_set)\n",
    "\n",
    "    hard_missed = list(gold_set - ens_set - rer_set)\n",
    "\n",
    "    debug_rows.append({\n",
    "        \"index\": i,\n",
    "        \"gold\": gold,\n",
    "        \"ensemble\": ens,\n",
    "        \"reranked\": rer,\n",
    "        \"lost_terms\": lost_terms,\n",
    "        \"added_terms\": added_terms,\n",
    "        \"remaining_false_positives\": false_positives,\n",
    "        \"new_true_positives\": new_true_positives,\n",
    "        \"hard_missed_terms\": hard_missed,\n",
    "    })\n",
    "\n",
    "df_debug = pd.DataFrame(debug_rows)\n",
    "print(\"✓ Debug dataframe created\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290989ec",
   "metadata": {},
   "source": [
    "### Lost terms during reranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "402b9e3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== TOP LOST TERMS (reranking removed them but they were in ensemble) ===\n",
      "vetro                                     →  removed 13 times\n",
      "plastica                                  →  removed 10 times\n",
      "conferire                                 →  removed 8 times\n",
      "raccolta                                  →  removed 5 times\n",
      "alluminio                                 →  removed 5 times\n",
      "carta                                     →  removed 5 times\n",
      "busta                                     →  removed 3 times\n",
      "sacchetti                                 →  removed 3 times\n",
      "rifiuti                                   →  removed 3 times\n",
      "essere conferiti                          →  removed 3 times\n",
      "sacco                                     →  removed 3 times\n",
      "utente                                    →  removed 3 times\n",
      "materiali                                 →  removed 2 times\n",
      "depositare                                →  removed 2 times\n",
      "secchiello                                →  removed 2 times\n",
      "conferiti                                 →  removed 2 times\n",
      "r                                         →  removed 2 times\n",
      "utenti                                    →  removed 2 times\n",
      "differenzia                               →  removed 2 times\n",
      "metalli                                   →  removed 2 times\n"
     ]
    }
   ],
   "source": [
    "lost_counter = Counter(\n",
    "    t for row in debug_rows for t in row[\"lost_terms\"]\n",
    ")\n",
    "\n",
    "print(\"\\n=== TOP LOST TERMS (reranking removed them but they were in ensemble) ===\")\n",
    "for term, count in lost_counter.most_common(20):\n",
    "    print(f\"{term:40s}  →  removed {count} times\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24737584",
   "metadata": {},
   "source": [
    "### added terms with reranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "7b117070",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== TERMS ADDED BY RERANKING (not in ensemble) ===\n"
     ]
    }
   ],
   "source": [
    "added_counter = Counter(\n",
    "    t for row in debug_rows for t in row[\"added_terms\"]\n",
    ")\n",
    "\n",
    "print(\"\\n=== TERMS ADDED BY RERANKING (not in ensemble) ===\")\n",
    "for term, count in added_counter.most_common(20):\n",
    "    print(f\"{term:40s}  →  added {count} times\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a100c9",
   "metadata": {},
   "source": [
    "### False positives rimasti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "a4bf2b7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== REMAINING FALSE POSITIVES (still wrong after reranking) ===\n",
      "rifiuti                                   →  FP 3 times\n",
      "centro di raccolta                        →  FP 2 times\n",
      "raccolta differenziata                    →  FP 2 times\n",
      "banda stagnata                            →  FP 2 times\n",
      "centri di raccolta comunali               →  FP 1 times\n",
      "disciplinare                              →  FP 1 times\n",
      "servizio di raccolta dei rifiuti derivanti da sfalci e potature  →  FP 1 times\n",
      "batterie e accumulatori al piombo derivanti dalla manutenzione  →  FP 1 times\n",
      "pile portatili                            →  FP 1 times\n",
      "tari                                      →  FP 1 times\n",
      "sacchetti del non differenziabile         →  FP 1 times\n",
      "utenze domestiche per la raccolta differenziata  →  FP 1 times\n",
      "rifiuti non pericolosi e non ingombranti  →  FP 1 times\n",
      "servizio integrato gestione rifiuti –     →  FP 1 times\n",
      "raccolta differenziata e servizi complementari  →  FP 1 times\n",
      "incendi dei rifiuti                       →  FP 1 times\n",
      "isola ecologica                           →  FP 1 times\n",
      "forme di gestione dei rifiuti             →  FP 1 times\n",
      "carrellati condominiali                   →  FP 1 times\n",
      "costi fissi                               →  FP 1 times\n"
     ]
    }
   ],
   "source": [
    "fp_counter = Counter(\n",
    "    t for row in debug_rows for t in row[\"remaining_false_positives\"]\n",
    ")\n",
    "\n",
    "print(\"\\n=== REMAINING FALSE POSITIVES (still wrong after reranking) ===\")\n",
    "for term, count in fp_counter.most_common(20):\n",
    "    print(f\"{term:40s}  →  FP {count} times\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6596042",
   "metadata": {},
   "source": [
    "### true positives added with reranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "3b69b388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== TRUE POSITIVES ADDED BY RERANKING (good improvements) ===\n"
     ]
    }
   ],
   "source": [
    "tp_gain_counter = Counter(\n",
    "    t for row in debug_rows for t in row[\"new_true_positives\"]\n",
    ")\n",
    "\n",
    "print(\"\\n=== TRUE POSITIVES ADDED BY RERANKING (good improvements) ===\")\n",
    "for term, count in tp_gain_counter.most_common(20):\n",
    "    print(f\"{term:40s}  →  NEW TP {count} times\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94bf5c1",
   "metadata": {},
   "source": [
    "### Hard cases (termini gold mancanti sia da ensemble che reranking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "33c76146",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== HARD MISSED TERMS (neither ensemble nor reranking found them) ===\n",
      "sacchetto trasparente                     →  MISSED 4 times\n",
      "plastica, acciaio e alluminio             →  MISSED 3 times\n",
      "conferiti                                 →  MISSED 3 times\n",
      "raccolta                                  →  MISSED 3 times\n",
      "frazione verde                            →  MISSED 2 times\n",
      "carta, cartone, cartoncino                →  MISSED 2 times\n",
      "conferimento                              →  MISSED 2 times\n",
      "modalità di conferimento                  →  MISSED 2 times\n",
      "rifiuti                                   →  MISSED 2 times\n",
      "busta con legaccio                        →  MISSED 2 times\n",
      "carta - cartone - tetra pak               →  MISSED 2 times\n",
      "misure di gestione ambientale             →  MISSED 2 times\n",
      "r.a.e.e.                                  →  MISSED 2 times\n",
      "tariffe                                   →  MISSED 2 times\n",
      "conferire                                 →  MISSED 2 times\n",
      "plastica                                  →  MISSED 2 times\n",
      "tari                                      →  MISSED 2 times\n",
      "modalità di raccolta                      →  MISSED 2 times\n",
      "latta                                     →  MISSED 2 times\n",
      "disciplina dei centri di raccolta dei rifiuti urbani raccolti in modo differenziato  →  MISSED 1 times\n"
     ]
    }
   ],
   "source": [
    "hard_counter = Counter(\n",
    "    t for row in debug_rows for t in row[\"hard_missed_terms\"]\n",
    ")\n",
    "\n",
    "print(\"\\n=== HARD MISSED TERMS (neither ensemble nor reranking found them) ===\")\n",
    "for term, count in hard_counter.most_common(20):\n",
    "    print(f\"{term:40s}  →  MISSED {count} times\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "0e92c876",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== SENTENCES WHERE RERANKING LOST MOST TERMS ===\n",
      "\n",
      "=== SENTENCES WHERE RERANKING HAS MOST FALSE POSITIVES ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>remaining_false_positives</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>402</th>\n",
       "      <td>402</td>\n",
       "      <td>[oli conferiti, oli esausti, recupero]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>35</td>\n",
       "      <td>[batterie e accumulatori al piombo derivanti d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[centri di raccolta comunali, disciplinare]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>97</td>\n",
       "      <td>[servizio integrato gestione rifiuti –, raccol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>299</td>\n",
       "      <td>[raccolta differenziata, porta a porta spinto]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>371</th>\n",
       "      <td>371</td>\n",
       "      <td>[banda stagnata, lattine]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415</th>\n",
       "      <td>415</td>\n",
       "      <td>[parte fissa, parte variabile]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>538</th>\n",
       "      <td>538</td>\n",
       "      <td>[scarti di produzione, tessuti sporchi ed inqu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>[servizio di raccolta dei rifiuti derivanti da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>70</td>\n",
       "      <td>[centro di raccolta]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     index                          remaining_false_positives\n",
       "402    402             [oli conferiti, oli esausti, recupero]\n",
       "35      35  [batterie e accumulatori al piombo derivanti d...\n",
       "1        1        [centri di raccolta comunali, disciplinare]\n",
       "97      97  [servizio integrato gestione rifiuti –, raccol...\n",
       "299    299     [raccolta differenziata, porta a porta spinto]\n",
       "371    371                          [banda stagnata, lattine]\n",
       "415    415                     [parte fissa, parte variabile]\n",
       "538    538  [scarti di produzione, tessuti sporchi ed inqu...\n",
       "6        6  [servizio di raccolta dei rifiuti derivanti da...\n",
       "70      70                               [centro di raccolta]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sentences where LLM made the worst damage:\n",
    "df_debug[\"lost_count\"] = df_debug[\"lost_terms\"].apply(len)\n",
    "df_debug[\"fp_count\"] = df_debug[\"remaining_false_positives\"].apply(len)\n",
    "\n",
    "worst_lost = df_debug.sort_values(\"lost_count\", ascending=False).head(10)\n",
    "worst_fp = df_debug.sort_values(\"fp_count\", ascending=False).head(10)\n",
    "\n",
    "print(\"\\n=== SENTENCES WHERE RERANKING LOST MOST TERMS ===\")\n",
    "worst_lost[[\"index\", \"lost_terms\"]]\n",
    "\n",
    "print(\"\\n=== SENTENCES WHERE RERANKING HAS MOST FALSE POSITIVES ===\")\n",
    "worst_fp[[\"index\", \"remaining_false_positives\"]]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
