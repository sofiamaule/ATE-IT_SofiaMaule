{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "# LLM-Based Few-Shot Term Extraction\n",
        "\n",
        "This notebook demonstrates a zero-shot term extraction approach using Large Language Models:\n",
        "- Uses Google Gemini API for term extraction\n",
        "- Processes sentences in batches for efficiency\n",
        "- No training required - relies on LLM's general knowledge\n",
        "\n",
        "Dataset: EvalITA 2025 ATE-IT (Automatic Term Extraction - Italian Testbed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup and Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "7ax0pwKnmCS3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Libraries imported\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "import os\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "\n",
        "print(\"✓ Libraries imported\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Loading and Processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Data loading functions defined\n"
          ]
        }
      ],
      "source": [
        "def load_jsonl(path: str):\n",
        "    \"\"\"Load a JSON lines file or JSON array file.\"\"\"\n",
        "    with open(path, 'r', encoding='utf-8') as f:\n",
        "        text = f.read().strip()\n",
        "    if not text:\n",
        "        return []\n",
        "    try:\n",
        "        data = json.loads(text)\n",
        "    except json.JSONDecodeError:\n",
        "        data = []\n",
        "        for line in text.splitlines():\n",
        "            line = line.strip()\n",
        "            if line:\n",
        "                data.append(json.loads(line))\n",
        "    return data\n",
        "\n",
        "\n",
        "def build_sentence_gold_map(records):\n",
        "    \"\"\"Convert dataset rows into list of sentences with aggregated terms.\"\"\"\n",
        "    out = {}\n",
        "    \n",
        "    if isinstance(records, dict) and 'data' in records:\n",
        "        rows = records['data']\n",
        "    else:\n",
        "        rows = records\n",
        "    \n",
        "    for r in rows:\n",
        "        key = (r.get('document_id'), r.get('paragraph_id'), r.get('sentence_id'))\n",
        "        if key not in out:\n",
        "            out[key] = {\n",
        "                'document_id': r.get('document_id'),\n",
        "                'paragraph_id': r.get('paragraph_id'),\n",
        "                'sentence_id': r.get('sentence_id'),\n",
        "                'sentence_text': r.get('sentence_text', ''),\n",
        "                'terms': []\n",
        "            }\n",
        "        \n",
        "        if isinstance(r.get('term_list'), list):\n",
        "            for t in r.get('term_list'):\n",
        "                if t and t not in out[key]['terms']:\n",
        "                    out[key]['terms'].append(t)\n",
        "        else:\n",
        "            term = r.get('term')\n",
        "            if term and term not in out[key]['terms']:\n",
        "                out[key]['terms'].append(term)\n",
        "    \n",
        "    return list(out.values())\n",
        "\n",
        "\n",
        "print(\"✓ Data loading functions defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "jDhknjBdmKoI"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training sentences: 2308\n",
            "Dev sentences: 577\n",
            "\n",
            "Example sentence:\n",
            "  Text: Non Domestica; CAMPEGGI, DISTRIBUTORI CARBURANTI, PARCHEGGI; 1,22; 4,73 \n",
            "  Terms: []\n"
          ]
        }
      ],
      "source": [
        "# Load training and dev data\n",
        "train_data = load_jsonl('../data/subtask_a_train.json')\n",
        "dev_data = load_jsonl('../data/subtask_a_dev.json')\n",
        "\n",
        "train_sentences = build_sentence_gold_map(train_data)\n",
        "dev_sentences = build_sentence_gold_map(dev_data)\n",
        "\n",
        "print(f\"Training sentences: {len(train_sentences)}\")\n",
        "print(f\"Dev sentences: {len(dev_sentences)}\")\n",
        "print(f\"\\nExample sentence:\")\n",
        "print(f\"  Text: {dev_sentences[0]['sentence_text']}\")\n",
        "print(f\"  Terms: {dev_sentences[0]['terms']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluation Metrics\n",
        "\n",
        "Using the official evaluation metrics from the competition."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Evaluation functions defined\n"
          ]
        }
      ],
      "source": [
        "def micro_f1_score(gold_standard, system_output):\n",
        "    \"\"\"\n",
        "    Evaluates performance using Precision, Recall, and F1 score \n",
        "    based on individual term matching (micro-average).\n",
        "    \"\"\"\n",
        "    total_true_positives = 0\n",
        "    total_false_positives = 0\n",
        "    total_false_negatives = 0\n",
        "    \n",
        "    for gold, system in zip(gold_standard, system_output):\n",
        "        gold_set = set(gold)\n",
        "        system_set = set(system)\n",
        "        \n",
        "        true_positives = len(gold_set.intersection(system_set))\n",
        "        false_positives = len(system_set - gold_set)\n",
        "        false_negatives = len(gold_set - system_set)\n",
        "        \n",
        "        total_true_positives += true_positives\n",
        "        total_false_positives += false_positives\n",
        "        total_false_negatives += false_negatives\n",
        "    \n",
        "    precision = total_true_positives / (total_true_positives + total_false_positives) if (total_true_positives + total_false_positives) > 0 else 0\n",
        "    recall = total_true_positives / (total_true_positives + total_false_negatives) if (total_true_positives + total_false_negatives) > 0 else 0\n",
        "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "    \n",
        "    return precision, recall, f1, total_true_positives, total_false_positives, total_false_negatives\n",
        "\n",
        "\n",
        "def type_f1_score(gold_standard, system_output):\n",
        "    \"\"\"\n",
        "    Evaluates performance using Type Precision, Type Recall, and Type F1 score\n",
        "    based on the set of unique terms extracted at least once across the entire dataset.\n",
        "    \"\"\"\n",
        "    all_gold_terms = set()\n",
        "    for item_terms in gold_standard:\n",
        "        all_gold_terms.update(item_terms)\n",
        "    \n",
        "    all_system_terms = set()\n",
        "    for item_terms in system_output:\n",
        "        all_system_terms.update(item_terms)\n",
        "    \n",
        "    type_true_positives = len(all_gold_terms.intersection(all_system_terms))\n",
        "    type_false_positives = len(all_system_terms - all_gold_terms)\n",
        "    type_false_negatives = len(all_gold_terms - all_system_terms)\n",
        "    \n",
        "    type_precision = type_true_positives / (type_true_positives + type_false_positives) if (type_true_positives + type_false_positives) > 0 else 0\n",
        "    type_recall = type_true_positives / (type_true_positives + type_false_negatives) if (type_true_positives + type_false_negatives) > 0 else 0\n",
        "    type_f1 = 2 * (type_precision * type_recall) / (type_precision + type_recall) if (type_precision + type_recall) > 0 else 0\n",
        "    \n",
        "    return type_precision, type_recall, type_f1\n",
        "\n",
        "\n",
        "print(\"✓ Evaluation functions defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Initialize LLM Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "diQC4xoeouBd"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "from dotenv import load_dotenv\n",
        "import google.generativeai as genai\n",
        "ROOT_DIR = Path.cwd()  # se il notebook è nella root, va già bene così\n",
        "dotenv_path = ROOT_DIR / \".env\"\n",
        "if not dotenv_path.exists():\n",
        "    # fallback: prova nella cartella padre\n",
        "    dotenv_path = ROOT_DIR.parent / \".env\"\n",
        "\n",
        "load_dotenv(dotenv_path)\n",
        "\n",
        "api_key = os.getenv(\"GEMINI_API_KEY\")\n",
        "if not api_key:\n",
        "    raise RuntimeError(f\"GEMINI_API_KEY not found in {dotenv_path}\")\n",
        "# rimuovi eventuali virgolette nel .env\n",
        "api_key = api_key.strip().strip('\"').strip(\"'\")\n",
        "# Get the API key from the user\n",
        "#api_key = input(\"Please enter your Gemini API key: \")\n",
        "genai.configure(api_key=api_key)\n",
        "\n",
        "# Define the model\n",
        "model = genai.GenerativeModel('gemini-2.5-flash')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "bLdHG-m1mdrK"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Few-shot prompt configured (batch size: 20)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "batch_size = 20\n",
        "\n",
        "system_prompt = f\"\"\"\n",
        "You are an automatic term extraction agent for Italian municipal waste management texts.\n",
        "You will receive a list of sentences as input.\n",
        "Your role is to extract *all and only* waste management terms from each sentence.\n",
        "Output a list of terms for each sentence.\n",
        "\n",
        "A \"term\" in this task is:\n",
        "- a single- or multi-word expression\n",
        "- that refers to a concept in the waste management domain\n",
        "- often nouns or noun phrases (sometimes adjectives or verbs as part of a phrase)\n",
        "\n",
        "Non-terms are:\n",
        "- generic function words (e.g., \"e\", \"di\", \"per\", \"che\")\n",
        "- pure numbers or dates not part of a waste term\n",
        "- person names, city names, street names (unless part of an official name of a waste service)\n",
        "\n",
        "Strictly adhere to the Example Output Format:\n",
        "\n",
        "Sentence 1: [term1; term2; term3]\n",
        "Sentence 2: [term4]\n",
        "Sentence 3: []\n",
        "\n",
        "--------------------------------\n",
        "ANNOTATED EXAMPLES (FEW-SHOT)\n",
        "--------------------------------\n",
        "\n",
        "Example 1\n",
        "Sentence:\n",
        "\"Il presente disciplinare per la gestione dei centri di raccolta comunali è stato redatto ai sensi del DM 13/05/2009.\"\n",
        "Expected output:\n",
        "Sentence 1: [disciplinare per la gestione dei centri di raccolta comunali; centri di raccolta comunali]\n",
        "\n",
        "Example 2\n",
        "Sentence:\n",
        "\"Il servizio di raccolta differenziata porta a porta dei rifiuti urbani è attivo su tutto il territorio comunale.\"\n",
        "Expected output:\n",
        "Sentence 1: [servizio di raccolta differenziata porta a porta; raccolta differenziata porta a porta; rifiuti urbani]\n",
        "\n",
        "Example 3\n",
        "Sentence:\n",
        "\"Il pagamento della Tassa Rifiuti (TARI) avviene tramite il portale pagoPA.\"\n",
        "Expected output:\n",
        "Sentence 1: [tassa rifiuti; tari]\n",
        "\n",
        "Example 4\n",
        "Sentence:\n",
        "\"Il presente regolamento disciplina le modalità di conferimento dei rifiuti ingombranti presso l'isola ecologica comunale.\"\n",
        "Expected output:\n",
        "Sentence 1: [regolamento; modalità di conferimento; rifiuti ingombranti; isola ecologica comunale]\n",
        "\n",
        "Example 5\n",
        "Sentence:\n",
        "\"In questa frase non sono presenti termini di gestione dei rifiuti.\"\n",
        "Expected output:\n",
        "Sentence 1: []\n",
        "\n",
        "\n",
        "YOUR TASK:\n",
        "Now you will receive {batch_size} sentences in the following format:\n",
        "\n",
        "Sentence k:\n",
        "<sentence_text>\n",
        "\n",
        "For each sentence k, you MUST output exactly one line in this format:\n",
        "\n",
        "Sentence k: [term1; term2; term3]\n",
        "\n",
        "Instructions:\n",
        "* Extract only terms related to waste and waste management (e.g., tassa rifiuti, tari, isola ecologica, raccolta differenziata, impianto di trattamento rifiuti).\n",
        "* Prefer complete multi-word terms (full span) over shorter fragments.\n",
        "* Do NOT output nested terms: if you extract \"impianto di trattamento rifiuti urbani\", do NOT also output \"trattamento rifiuti urbani\", unless it appears as a separate term in the sentence.\n",
        "* Ignore named entities (people, cities, streets) unless they are part of an official waste management term.\n",
        "* If a sentence contains no relevant terms, output an empty list: Sentence k: [].\n",
        "* You must output one line for each of the {batch_size} input sentences, in order (Sentence 1, Sentence 2, ...).\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "print(f\"✓ Few-shot prompt configured (batch size: {batch_size})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configure Few-Shot Prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Save LLM responses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "SAVE_PATH = \"llm_responses.txt\"\n",
        "\n",
        "def save_llm_responses(response_list, path=SAVE_PATH):\n",
        "    \"\"\"\n",
        "    Salva ogni risposta LLM in batch in un file .txt,\n",
        "    separata da un marcatore ---END-OF-BATCH---\n",
        "    \"\"\"\n",
        "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
        "        for resp in response_list:\n",
        "            f.write(resp.strip() + \"\\n---END-OF-BATCH---\\n\")\n",
        "    print(f\"✓ Saved {len(response_list)} batch responses to {path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "lXrKQDeMqHqN"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing 577 sentences in batches of 20...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 577/577 [09:29<00:00,  1.01it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Received 29 batch responses from LLM\n",
            "✓ Saved 29 batch responses to llm_responses.txt\n"
          ]
        }
      ],
      "source": [
        "# Process sentences in batches with LLM\n",
        "print(f\"Processing {len(dev_sentences)} sentences in batches of {batch_size}...\")\n",
        "\n",
        "response_list = []\n",
        "user_prompt = \"\"\n",
        "\n",
        "for i, sent_data in enumerate(tqdm(dev_sentences)):\n",
        "    sent = sent_data['sentence_text']\n",
        "    \n",
        "    # Build prompt until batch size is reached\n",
        "    if (i + 1) % batch_size == 0:\n",
        "        user_prompt += f\"Sentence {i + 1}:\\n {sent}\"\n",
        "        \n",
        "        # Send batch to LLM\n",
        "        response = model.generate_content(\n",
        "            f\"System: {system_prompt}\\nUser: {user_prompt}\"\n",
        "        )\n",
        "        response_list.append(response.text)\n",
        "        \n",
        "        user_prompt = \"\"\n",
        "    else:\n",
        "        user_prompt += f\"Sentence {i + 1}:\\n {sent}\\n\\n\"\n",
        "\n",
        "# Process remaining sentences (last batch)\n",
        "if user_prompt:\n",
        "    user_prompt = user_prompt.rstrip()\n",
        "    response = model.generate_content(\n",
        "        f\"System: {system_prompt}\\nUser: {user_prompt}\"\n",
        "    )\n",
        "    response_list.append(response.text)\n",
        "\n",
        "print(f\"✓ Received {len(response_list)} batch responses from LLM\")\n",
        "save_llm_responses(response_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Parse LLM responses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open(\"llm_responses.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    raw = f.read()\n",
        "\n",
        "responses = raw.split(\"\\n---END-OF-BATCH---\\n\")\n",
        "responses = [r.strip() for r in responses if r.strip()]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "vKzlrjf7onwV"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Parsing LLM responses...\n",
            "✓ Parsed 577 predictions\n"
          ]
        }
      ],
      "source": [
        "# Parse LLM responses to extract term lists\n",
        "print(\"Parsing LLM responses...\")\n",
        "\n",
        "llm_preds = []\n",
        "for response in response_list:\n",
        "    for sent in response.split('\\n'):\n",
        "        if 'Sentence' in sent and '[' in sent:\n",
        "            try:\n",
        "                # Extract sentence ID\n",
        "                id_match = re.search(r'Sentence (\\d+):', sent)\n",
        "                if not id_match:\n",
        "                    continue\n",
        "                \n",
        "                # Extract terms from brackets\n",
        "                terms_match = re.search(r'\\[(.*?)\\]', sent)\n",
        "                if terms_match:\n",
        "                    terms = terms_match.group(1).split(';')\n",
        "                    terms = [term.strip().lower() for term in terms if term.strip()]\n",
        "                else:\n",
        "                    terms = []\n",
        "                \n",
        "                llm_preds.append(terms)\n",
        "            except Exception as e:\n",
        "                print(f\"Warning: Could not parse line: {sent[:50]}...\")\n",
        "                llm_preds.append([])\n",
        "\n",
        "# Verify output length matches input\n",
        "if len(llm_preds) != len(dev_sentences):\n",
        "    print(f\"Warning: Output length ({len(llm_preds)}) doesn't match input ({len(dev_sentences)})\")\n",
        "    # Pad or truncate to match\n",
        "    while len(llm_preds) < len(dev_sentences):\n",
        "        llm_preds.append([])\n",
        "    llm_preds = llm_preds[:len(dev_sentences)]\n",
        "\n",
        "print(f\"✓ Parsed {len(llm_preds)} predictions\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Save LLM predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "i0Dzrbh90J-r"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Saved 577 predictions to predictions/subtask_a_dev_llm_few_shot_preds.json\n"
          ]
        }
      ],
      "source": [
        "# Save predictions in competition format\n",
        "def save_predictions(predictions, sentences, output_path):\n",
        "    \"\"\"Save predictions in competition format.\"\"\"\n",
        "    output = {'data': []}\n",
        "    for pred, sent in zip(predictions, sentences):\n",
        "        output['data'].append({\n",
        "            'document_id': sent['document_id'],\n",
        "            'paragraph_id': sent['paragraph_id'],\n",
        "            'sentence_id': sent['sentence_id'],\n",
        "            'term_list': pred\n",
        "        })\n",
        "    \n",
        "    os.makedirs(os.path.dirname(output_path) or '.', exist_ok=True)\n",
        "    with open(output_path, 'w', encoding='utf-8') as f:\n",
        "        json.dump(output, f, ensure_ascii=False, indent=2)\n",
        "    print(f\"✓ Saved {len(predictions)} predictions to {output_path}\")\n",
        "\n",
        "\n",
        "save_predictions(llm_preds, dev_sentences, 'predictions/subtask_a_dev_llm_few_shot_preds.json')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluate Performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "LLM FEW-SHOT BASELINE RESULTS\n",
            "============================================================\n",
            "\n",
            "Micro-averaged Metrics:\n",
            "  Precision: 0.4630\n",
            "  Recall:    0.6652\n",
            "  F1 Score:  0.5460\n",
            "  TP=300, FP=348, FN=151\n",
            "\n",
            "Type-level Metrics:\n",
            "  Type Precision: 0.4037\n",
            "  Type Recall:    0.7190\n",
            "  Type F1 Score:  0.5171\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# Prepare gold standard and predictions for evaluation\n",
        "dev_gold = [s['terms'] for s in dev_sentences]\n",
        "\n",
        "# Calculate metrics\n",
        "precision, recall, f1, tp, fp, fn = micro_f1_score(dev_gold, llm_preds)\n",
        "type_precision, type_recall, type_f1 = type_f1_score(dev_gold, llm_preds)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"LLM FEW-SHOT BASELINE RESULTS\")\n",
        "print(\"=\"*60)\n",
        "print(\"\\nMicro-averaged Metrics:\")\n",
        "print(f\"  Precision: {precision:.4f}\")\n",
        "print(f\"  Recall:    {recall:.4f}\")\n",
        "print(f\"  F1 Score:  {f1:.4f}\")\n",
        "print(f\"  TP={tp}, FP={fp}, FN={fn}\")\n",
        "\n",
        "print(\"\\nType-level Metrics:\")\n",
        "print(f\"  Type Precision: {type_precision:.4f}\")\n",
        "print(f\"  Type Recall:    {type_recall:.4f}\")\n",
        "print(f\"  Type F1 Score:  {type_f1:.4f}\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Example Predictions:\n",
            "\n",
            "Sentence: Il presente disciplinare per la gestione dei centri di raccolta comunali è stato redatto ai sensi e ...\n",
            "Gold terms: ['disciplina dei centri di raccolta dei rifiuti urbani raccolti in modo differenziato', 'disciplinare per la gestione dei centri di raccolta comunali']\n",
            "LLM predictions: ['disciplinare per la gestione dei centri di raccolta comunali', 'centri di raccolta comunali', 'disciplina dei centri di raccolta dei rifiuti urbani raccolti in modo differenziato', 'centri di raccolta dei rifiuti urbani raccolti in modo differenziato']\n",
            "✓ Correct: 2\n",
            "✗ Missed: 0\n",
            "✗ Wrong: 2\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Sentence: È un Servizio Supplementare di raccolta, rivolto a famiglie con bambini al di sotto dei 3 anni o con...\n",
            "Gold terms: ['raccolta']\n",
            "LLM predictions: ['servizio supplementare di raccolta']\n",
            "✓ Correct: 0\n",
            "✗ Missed: 1\n",
            "✗ Wrong: 1\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Sentence: ll servizio di raccolta dei rifiuti derivanti da sfalci e potature è gestito dalla Buttol Srl con il...\n",
            "Gold terms: ['servizio di raccolta dei rifiuti', 'sfalci e potature']\n",
            "LLM predictions: ['servizio di raccolta dei rifiuti derivanti da sfalci e potature', 'rifiuti derivanti da sfalci e potature', 'sfalci e potature']\n",
            "✓ Correct: 1\n",
            "✗ Missed: 1\n",
            "✗ Wrong: 2\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Sentence: #Differenziati...\n",
            "Gold terms: ['differenziati']\n",
            "LLM predictions: ['differenziati']\n",
            "✓ Correct: 1\n",
            "✗ Missed: 0\n",
            "✗ Wrong: 0\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Sentence: MULTIMATERIALE; Sacchetto blu trasparente; Lunedì Giovedì...\n",
            "Gold terms: ['multimateriale', 'sacchetto trasparente']\n",
            "LLM predictions: ['multimateriale', 'sacchetto blu trasparente']\n",
            "✓ Correct: 1\n",
            "✗ Missed: 1\n",
            "✗ Wrong: 1\n",
            "--------------------------------------------------------------------------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Show example predictions\n",
        "print(\"Example Predictions:\\n\")\n",
        "\n",
        "count = 0\n",
        "for i in range(len(dev_sentences)):\n",
        "    if len(dev_gold[i]) > 0 and count < 5:\n",
        "        print(f\"Sentence: {dev_sentences[i]['sentence_text'][:100]}...\")\n",
        "        print(f\"Gold terms: {dev_gold[i][:5]}\")\n",
        "        print(f\"LLM predictions: {llm_preds[i][:5]}\")\n",
        "        \n",
        "        correct = set(dev_gold[i]) & set(llm_preds[i])\n",
        "        missed = set(dev_gold[i]) - set(llm_preds[i])\n",
        "        wrong = set(llm_preds[i]) - set(dev_gold[i])\n",
        "        \n",
        "        print(f\"✓ Correct: {len(correct)}\")\n",
        "        print(f\"✗ Missed: {len(missed)}\")\n",
        "        print(f\"✗ Wrong: {len(wrong)}\")\n",
        "        print(\"-\"*80)\n",
        "        print()\n",
        "        \n",
        "        count += 1"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyNPasnfXZT1l2LNzTXayrhd",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
